[
  {
    "objectID": "scripts/sim_comp_report.html",
    "href": "scripts/sim_comp_report.html",
    "title": "4  Simulation-based Power Analysis for Two-arm RCT with Compositional Outcomes",
    "section": "",
    "text": "4.1 Introduction\nThis document demonstrates a simulation-based power analysis for a two-arm parallel randomized controlled trial (RCT) with compositional outcomes. The simulation models 24-hour time use data consisting of three components: sleep, sedentary time, and physical activity, which sum to 1440 minutes (24 hours). The analysis includes visualization of power curves and effect sizes to help determine optimal sample sizes and assess the sensitivity of the study design to different parameters.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulation-based Power Analysis for Two-arm RCT with Compositional Outcomes</span>"
    ]
  },
  {
    "objectID": "scripts/sim_comp_report.html#required-packages",
    "href": "scripts/sim_comp_report.html#required-packages",
    "title": "4  Simulation-based Power Analysis for Two-arm RCT with Compositional Outcomes",
    "section": "4.2 Required Packages",
    "text": "4.2 Required Packages\n\n\nCode\n# Install required packages if not already installed\nreq &lt;- c(\"tidyverse\", \"lme4\", \"compositions\", \"ggtern\", \"progress\", \"phyloseq\", \"patchwork\", \"lmerTest\", \"foreach\", \"doSNOW\", \"doParallel\")\nneed &lt;- req[!req %in% installed.packages()[, \"Package\"]]\nif (length(need)) install.packages(need)\n\n\n# Downloading packages -------------------------------------------------------\n- Downloading ggtern from CRAN ...              OK [1.1 Mb in 1.4s]\n- Downloading latex2exp from CRAN ...           OK [1.5 Mb in 1.4s]\n- Downloading proto from CRAN ...               OK [462.2 Kb in 1.2s]\n- Downloading hexbin from CRAN ...              OK [1.5 Mb in 1.4s]\n- Downloading phyloseq from BioCsoft ...        OK [6.1 Mb in 1.6s]\n- Downloading ade4 from CRAN ...                OK [5.2 Mb in 1.9s]\n- Downloading sp from CRAN ...                  OK [5.1 Mb in 1.1s]\n- Downloading ape from CRAN ...                 OK [2.8 Mb in 2.3s]\n- Downloading Biobase from BioCsoft ...         OK [2.5 Mb in 1.4s]\n- Downloading BiocGenerics from BioCsoft ...    OK [613.9 Kb in 0.99s]\n- Downloading biomformat from BioCsoft ...      OK [497.7 Kb in 0.87s]\n- Downloading rhdf5 from BioCsoft ...           OK [3.4 Mb in 1.3s]\n- Downloading Rhdf5lib from BioCsoft ...        OK [10.1 Mb in 2.1s]\n- Downloading rhdf5filters from BioCsoft ...    OK [1 Mb in 0.97s]\n- Downloading Biostrings from BioCsoft ...      OK [13.5 Mb in 2.2s]\n- Downloading S4Vectors from BioCsoft ...       OK [2.4 Mb in 1.1s]\n- Downloading IRanges from BioCsoft ...         OK [2.2 Mb in 1.1s]\n- Downloading XVector from BioCsoft ...         OK [619.4 Kb in 0.93s]\n- Downloading zlibbioc from BioCsoft ...        OK [227.7 Kb in 0.75s]\n- Downloading GenomeInfoDb from BioCsoft ...    OK [4 Mb in 1.4s]\n- Downloading UCSC.utils from BioCsoft ...      OK [282.7 Kb in 0.89s]\n- Downloading GenomeInfoDbData from BioCann ... OK [12.3 Mb in 2.1s]\n- Downloading igraph from CRAN ...              OK [5.5 Mb in 1.8s]\n- Downloading multtest from BioCsoft ...        OK [827.2 Kb in 0.94s]\n- Downloading vegan from CRAN ...               OK [2.9 Mb in 1.7s]\n- Downloading permute from CRAN ...             OK [228.6 Kb in 1.1s]\n- Downloading doSNOW from CRAN ...              OK [26.4 Kb in 0.97s]\n- Downloading snow from CRAN ...                OK [96.4 Kb in 0.78s]\n- Downloading doParallel from CRAN ...          OK [183.3 Kb in 2.0s]\nSuccessfully downloaded 29 packages in 52 seconds.\n\nThe following package(s) will be installed:\n- ade4             [1.7-23]\n- ape              [5.8-1]\n- Biobase          [2.66.0]\n- BiocGenerics     [0.52.0]\n- biomformat       [1.34.0]\n- Biostrings       [2.74.1]\n- doParallel       [1.0.17]\n- doSNOW           [1.0.20]\n- foreach          [1.5.2]\n- GenomeInfoDb     [1.42.3]\n- GenomeInfoDbData [1.2.13]\n- ggtern           [3.5.0]\n- hexbin           [1.28.5]\n- igraph           [2.1.4]\n- IRanges          [2.40.1]\n- iterators        [1.0.14]\n- latex2exp        [0.9.6]\n- multtest         [2.62.0]\n- patchwork        [1.3.0]\n- permute          [0.9-7]\n- phyloseq         [1.50.0]\n- pixmap           [0.4-13]\n- plyr             [1.8.9]\n- proto            [1.0.0]\n- reshape2         [1.4.4]\n- rhdf5            [2.50.2]\n- rhdf5filters     [1.18.1]\n- Rhdf5lib         [1.28.0]\n- S4Vectors        [0.44.0]\n- snow             [0.4-4]\n- sp               [2.2-0]\n- UCSC.utils       [1.2.0]\n- vegan            [2.7-1]\n- XVector          [0.46.0]\n- zlibbioc         [1.52.0]\nThese packages will be installed into \"~/Desktop/Research/1 Active Projects/2025 HFF Gaming Reduction/gaming-reduction-backend/renv/library/macos/R-4.4/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing latex2exp ...                      OK [installed binary and cached in 0.39s]\n- Installing plyr ...                           OK [linked from cache]\n- Installing proto ...                          OK [installed binary and cached in 0.34s]\n- Installing hexbin ...                         OK [installed binary and cached in 0.35s]\n- Installing ggtern ...                         OK [installed binary and cached in 0.37s]\n- Installing pixmap ...                         OK [linked from cache]\n- Installing sp ...                             OK [installed binary and cached in 0.44s]\n- Installing ade4 ...                           OK [installed binary and cached in 0.47s]\n- Installing ape ...                            OK [installed binary and cached in 0.37s]\n- Installing BiocGenerics ...                   OK [installed binary and cached in 0.35s]\n- Installing Biobase ...                        OK [installed binary and cached in 0.42s]\n- Installing Rhdf5lib ...                       OK [installed binary and cached in 0.71s]\n- Installing rhdf5filters ...                   OK [installed binary and cached in 0.37s]\n- Installing rhdf5 ...                          OK [installed binary and cached in 0.42s]\n- Installing biomformat ...                     OK [installed binary and cached in 0.35s]\n- Installing S4Vectors ...                      OK [installed binary and cached in 0.4s]\n- Installing IRanges ...                        OK [installed binary and cached in 0.4s]\n- Installing zlibbioc ...                       OK [installed binary and cached in 0.34s]\n- Installing XVector ...                        OK [installed binary and cached in 0.34s]\n- Installing UCSC.utils ...                     OK [installed binary and cached in 0.33s]\n- Installing GenomeInfoDbData ...               OK [built from source and cached in 0.73s]\n- Installing GenomeInfoDb ...                   OK [installed binary and cached in 0.51s]\n- Installing Biostrings ...                     OK [installed binary and cached in 0.54s]\n- Installing iterators ...                      OK [linked from cache]\n- Installing foreach ...                        OK [linked from cache]\n- Installing igraph ...                         OK [installed binary and cached in 0.44s]\n- Installing multtest ...                       OK [installed binary and cached in 0.34s]\n- Installing reshape2 ...                       OK [linked from cache]\n- Installing permute ...                        OK [installed binary and cached in 0.34s]\n- Installing vegan ...                          OK [installed binary and cached in 0.39s]\n- Installing phyloseq ...                       OK [built from source and cached in 12s]\n- Installing patchwork ...                      OK [linked from cache]\n- Installing snow ...                           OK [installed binary and cached in 0.36s]\n- Installing doSNOW ...                         OK [installed binary and cached in 0.39s]\n- Installing doParallel ...                     OK [installed binary and cached in 0.34s]\nSuccessfully installed 35 packages in 26 seconds.\n\n\nCode\n# Install Bioconductor packages if needed\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\")\nbioc_pkgs &lt;- c(\"phyloseq\", \"microbiome\", \"ComplexHeatmap\")\nbioc_need &lt;- bioc_pkgs[!bioc_pkgs %in% installed.packages()[, \"Package\"]]\nif (length(bioc_need)) BiocManager::install(bioc_need, update = FALSE)\n\n\n\nThe downloaded binary packages are in\n    /var/folders/91/wdzc_7yx0x77b157785fyt6w0000gn/T//RtmpEyIVsp/downloaded_packages\n\n\nCode\n# Install microViz from R Universe only if not already installed\nif (!\"microViz\" %in% installed.packages()[, \"Package\"]) {\n  install.packages(\n    \"microViz\",\n    repos = c(davidbarnett = \"https://david-barnett.r-universe.dev\", getOption(\"repos\"))\n  )\n}\n\n\n# Downloading packages -------------------------------------------------------\n- Downloading microViz from davidbarnett ...    OK [3 Mb in 0.52s]\n- Downloading microbiome from BioCsoft ...      OK [719.5 Kb in 0.85s]\n- Downloading ggiraph from CRAN ...             OK [1.5 Mb in 1.4s]\n- Downloading shiny from CRAN ...               OK [4.8 Mb in 1.5s]\n- Downloading commonmark from CRAN ...          OK [125 Kb in 0.52s]\n- Downloading seriation from CRAN ...           OK [1.3 Mb in 1.5s]\n- Downloading ca from CRAN ...                  OK [237 Kb in 1.2s]\n- Downloading gclus from CRAN ...               OK [409.7 Kb in 1.3s]\n- Downloading qap from CRAN ...                 OK [523.7 Kb in 1.3s]\n- Downloading registry from CRAN ...            OK [190.8 Kb in 1.2s]\n- Downloading TSP from CRAN ...                 OK [667.5 Kb in 1.4s]\nSuccessfully downloaded 11 packages in 30 seconds.\n\nThe following package(s) will be installed:\n- ca          [0.71.1]\n- commonmark  [1.9.5]\n- cowplot     [1.1.3]\n- gclus       [1.3.3]\n- ggiraph     [0.8.13]\n- microbiome  [1.28.0]\n- microViz    [0.12.7]\n- multtest    [2.62.0]\n- qap         [0.1-2]\n- registry    [0.5-1]\n- seriation   [1.5.7]\n- shiny       [1.10.0]\n- sourcetools [0.1.7-1]\n- TSP         [1.2-5]\n- xtable      [1.8-4]\nThese packages will be installed into \"~/Desktop/Research/1 Active Projects/2025 HFF Gaming Reduction/gaming-reduction-backend/renv/library/macos/R-4.4/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing multtest ...                       OK [linked from cache]\n- Installing microbiome ...                     OK [built from source and cached in 8.2s]\n- Installing ggiraph ...                        OK [installed binary and cached in 0.43s]\n- Installing xtable ...                         OK [linked from cache]\n- Installing sourcetools ...                    OK [linked from cache]\n- Installing commonmark ...                     OK [installed binary and cached in 0.32s]\n- Installing shiny ...                          OK [installed binary and cached in 0.59s]\n- Installing ca ...                             OK [installed binary and cached in 0.32s]\n- Installing gclus ...                          OK [installed binary and cached in 0.36s]\n- Installing qap ...                            OK [installed binary and cached in 0.5s]\n- Installing registry ...                       OK [installed binary and cached in 0.33s]\n- Installing TSP ...                            OK [installed binary and cached in 0.37s]\n- Installing seriation ...                      OK [installed binary and cached in 0.35s]\n- Installing cowplot ...                        OK [linked from cache]\n- Installing microViz ...                       OK [installed binary and cached in 0.36s]\nSuccessfully installed 15 packages in 14 seconds.\n\n\nCode\n# Install suggested packages for enhanced microViz functionality\nsuggested &lt;- c(\"ggtext\", \"ggraph\", \"DT\", \"corncob\")\nneed_suggested &lt;- suggested[!suggested %in% installed.packages()[, \"Package\"]]\nif (length(need_suggested)) install.packages(need_suggested)\n\n\n# Downloading packages -------------------------------------------------------\n- Downloading ggtext from CRAN ...              OK [1.2 Mb in 0.68s]\n- Downloading gridtext from CRAN ...            OK [344.8 Kb in 0.57s]\n- Downloading markdown from CRAN ...            OK [62.2 Kb in 0.52s]\n- Downloading litedown from CRAN ...            OK [359.8 Kb in 0.56s]\n- Downloading xfun from CRAN ...                OK [574.8 Kb in 0.91s]\n- Downloading jpeg from CRAN ...                OK [150 Kb in 0.8s]\n- Downloading ggraph from CRAN ...              OK [4.3 Mb in 1.9s]\n- Downloading ggforce from CRAN ...             OK [1.8 Mb in 1.1s]\n- Downloading tweenr from CRAN ...              OK [381.5 Kb in 0.83s]\n- Downloading polyclip from CRAN ...            OK [117.7 Kb in 0.77s]\n- Downloading ggrepel from CRAN ...             OK [272.3 Kb in 1.9s]\n- Downloading tidygraph from CRAN ...           OK [546.3 Kb in 1.3s]\n- Downloading graphlayouts from CRAN ...        OK [2.4 Mb in 1.6s]\n- Downloading corncob from CRAN ...             OK [2.6 Mb in 1.6s]\n- Downloading VGAM from CRAN ...                OK [7.5 Mb in 3.2s]\n- Downloading trust from CRAN ...               OK [209.8 Kb in 1.2s]\n- Downloading detectseparation from CRAN ...    OK [150.2 Kb in 2.1s]\n- Downloading ROI from CRAN ...                 OK [481.9 Kb in 1.3s]\n- Downloading slam from CRAN ...                OK [184.6 Kb in 1.1s]\n- Downloading ROI.plugin.lpsolve from CRAN ...  OK [47.1 Kb in 1.0s]\n- Downloading lpSolveAPI from CRAN ...          OK [573.6 Kb in 1.3s]\n- Downloading pkgload from CRAN ...             OK [214.9 Kb in 0.83s]\n- Downloading desc from CRAN ...                OK [329.7 Kb in 0.56s]\n- Downloading pkgbuild from CRAN ...            OK [203.8 Kb in 0.54s]\nSuccessfully downloaded 24 packages in 33 seconds.\n\nThe following package(s) will be installed:\n- corncob            [0.4.2]\n- desc               [1.4.3]\n- detectseparation   [0.3]\n- ggforce            [0.4.2]\n- ggraph             [2.2.1]\n- ggrepel            [0.9.6]\n- ggtext             [0.1.2]\n- graphlayouts       [1.2.2]\n- gridtext           [0.1.5]\n- jpeg               [0.1-11]\n- litedown           [0.7]\n- lpSolveAPI         [5.5.2.0-17.14]\n- markdown           [2.0]\n- pkgbuild           [1.4.8]\n- pkgload            [1.4.0]\n- polyclip           [1.10-7]\n- ROI                [1.0-1]\n- ROI.plugin.lpsolve [1.0-2]\n- rprojroot          [2.0.4]\n- slam               [0.1-55]\n- tidygraph          [1.3.1]\n- trust              [0.1-8]\n- tweenr             [2.0.3]\n- VGAM               [1.1-13]\n- xfun               [0.52]\nThese packages will be installed into \"~/Desktop/Research/1 Active Projects/2025 HFF Gaming Reduction/gaming-reduction-backend/renv/library/macos/R-4.4/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing xfun ...                           OK [installed binary and cached in 0.34s]\n- Installing litedown ...                       OK [installed binary and cached in 0.33s]\n- Installing markdown ...                       OK [installed binary and cached in 0.32s]\n- Installing jpeg ...                           OK [installed binary and cached in 0.33s]\n- Installing gridtext ...                       OK [installed binary and cached in 0.33s]\n- Installing ggtext ...                         OK [installed binary and cached in 0.35s]\n- Installing tweenr ...                         OK [installed binary and cached in 0.34s]\n- Installing polyclip ...                       OK [installed binary and cached in 0.33s]\n- Installing ggforce ...                        OK [installed binary and cached in 0.36s]\n- Installing ggrepel ...                        OK [installed binary and cached in 0.34s]\n- Installing tidygraph ...                      OK [installed binary and cached in 0.35s]\n- Installing graphlayouts ...                   OK [installed binary and cached in 0.37s]\n- Installing ggraph ...                         OK [installed binary and cached in 0.42s]\n- Installing VGAM ...                           OK [installed binary and cached in 0.41s]\n- Installing trust ...                          OK [installed binary and cached in 0.34s]\n- Installing slam ...                           OK [installed binary and cached in 0.33s]\n- Installing ROI ...                            OK [installed binary and cached in 0.34s]\n- Installing lpSolveAPI ...                     OK [installed binary and cached in 0.36s]\n- Installing ROI.plugin.lpsolve ...             OK [installed binary and cached in 0.33s]\n- Installing desc ...                           OK [installed binary and cached in 0.35s]\n- Installing pkgbuild ...                       OK [installed binary and cached in 0.37s]\n- Installing rprojroot ...                      OK [linked from cache]\n- Installing pkgload ...                        OK [installed binary and cached in 0.38s]\n- Installing detectseparation ...               OK [installed binary and cached in 0.35s]\n- Installing corncob ...                        OK [installed binary and cached in 0.37s]\nSuccessfully installed 25 packages in 9.8 seconds.\n\nThe following loaded package(s) have been updated:\n- xfun\nRestart your R session to use the new versions.\n\n\nCode\n# Load all required packages\ninvisible(lapply(c(req, \"microViz\"), library, character.only = TRUE))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulation-based Power Analysis for Two-arm RCT with Compositional Outcomes</span>"
    ]
  },
  {
    "objectID": "scripts/sim_comp_report.html#helper-functions",
    "href": "scripts/sim_comp_report.html#helper-functions",
    "title": "4  Simulation-based Power Analysis for Two-arm RCT with Compositional Outcomes",
    "section": "4.3 Helper Functions",
    "text": "4.3 Helper Functions\nThese functions convert between compositional data and isometric log-ratio (ilr) coordinates.\n\n\nCode\ncomp_to_ilr &lt;- function(x_min) {\n  stopifnot(is.matrix(x_min), ncol(x_min) == 3)\n  bad_row &lt;- !is.finite(rowSums(x_min)) | rowSums(x_min) &lt;= 0\n  if (any(bad_row)) {\n    x_min[bad_row, ] &lt;- matrix(rep(c(600, 480, 360), each = sum(bad_row)), ncol = 3, byrow = TRUE)\n  }\n  x_min[x_min &lt;= 0 | !is.finite(x_min)] &lt;- 1e-6\n  compositions::ilr(sweep(x_min, 1, rowSums(x_min), \"/\"))\n}\n\nilr_to_minutes &lt;- function(ilr_mat, total = 1440) {\n  stopifnot(is.matrix(ilr_mat), ncol(ilr_mat) == 2)\n  comp_obj &lt;- compositions::ilrInv(ilr_mat)\n  prop &lt;- as.data.frame(comp_obj)\n  prop &lt;- as.matrix(prop)\n  \n  bad &lt;- apply(prop, 1, function(r) any(!is.finite(r) | r &lt;= 0) ||\n                                   !is.finite(sum(r)) || abs(sum(r) - 1) &gt; 1e-8)\n  if (any(bad)) prop[bad, ] &lt;- 1/3\n  round(prop * total, 1)\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulation-based Power Analysis for Two-arm RCT with Compositional Outcomes</span>"
    ]
  },
  {
    "objectID": "scripts/sim_comp_report.html#simulation-engine",
    "href": "scripts/sim_comp_report.html#simulation-engine",
    "title": "4  Simulation-based Power Analysis for Two-arm RCT with Compositional Outcomes",
    "section": "4.4 Simulation Engine",
    "text": "4.4 Simulation Engine\n\n\nCode\n# Power estimation using SimEngine – cleaned version (no play_sd_prop, no corr_noise_sd)\n\n# Install SimEngine if not already installed\nif (!requireNamespace(\"SimEngine\", quietly = TRUE)) {\n  install.packages(\"SimEngine\")\n}\n\n# Load required packages\nlibrary(SimEngine)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(compositions)\nlibrary(MASS)\nlibrary(dplyr)\n\n# --------------------------------------------------------------------------------\n# Command line argument parsing ---------------------------------------------------\n# --------------------------------------------------------------------------------\n# Parse command line arguments for flexible parameter adjustment\nargs &lt;- commandArgs(trailingOnly = TRUE)\n\n# Default values\ndefault_sims &lt;- 10\ndefault_cores &lt;- 16\n\n# Parse arguments: --sims=VALUE --cores=VALUE\nsims_param &lt;- default_sims\ncores_param &lt;- default_cores\n\nif (length(args) &gt; 0) {\n  for (arg in args) {\n    if (grepl(\"^--sims=\", arg)) {\n      sims_param &lt;- as.numeric(sub(\"^--sims=\", \"\", arg))\n      if (is.na(sims_param) || sims_param &lt;= 0) {\n        warning(\"Invalid sims parameter, using default: \", default_sims)\n        sims_param &lt;- default_sims\n      }\n    } else if (grepl(\"^--cores=\", arg)) {\n      cores_param &lt;- as.numeric(sub(\"^--cores=\", \"\", arg))\n      if (is.na(cores_param) || cores_param &lt;= 0) {\n        warning(\"Invalid cores parameter, using default: \", default_cores)\n        cores_param &lt;- default_cores\n      }\n    }\n  }\n}\n\n# Log the parameters being used\nmessage(\"=== SIMULATION PARAMETERS ===\")\nmessage(\"Number of simulations: \", sims_param)\nmessage(\"Number of cores: \", cores_param)\nmessage(\"==============================\")\n\n# --------------------------------------------------------------------------------\n# Main simulation wrapper ---------------------------------------------------------\n# --------------------------------------------------------------------------------\n\nest_power_simengine &lt;- function(n_pg = 50,\n                               effect_min_values = c(30),\n                               s_between_values  = c(0.15),\n                               s_within_values   = c(0.25),\n                               baseline_days     = 7,\n                               intervention_days = 14,\n                               sims              = 500,\n                               cores             = 4) {\n\n  start_time &lt;- Sys.time()\n  message(\"Setting up SimEngine simulation …\")\n\n  # Create simulation object\n  sim &lt;- new_sim()\n\n  # ------------------------------------------------------------------------------\n  # LEVELS (note: no play_sd_prop, no corr_noise_sd) ------------------------------\n  # ------------------------------------------------------------------------------\n  sim %&lt;&gt;% set_levels(\n    n_pg             = n_pg,\n    effect_min       = effect_min_values,\n    s_between        = s_between_values,\n    s_within         = s_within_values,\n    baseline_days    = baseline_days,\n    intervention_days= intervention_days\n  )\n\n  # ------------------------------------------------------------------------------\n  # Helper transformations (INSIDE function for parallel access) ------------------\n  # ------------------------------------------------------------------------------\n  \n  comp_to_ilr &lt;- function(x_min) {\n    stopifnot(is.matrix(x_min), ncol(x_min) == 3)\n    bad_row &lt;- !is.finite(rowSums(x_min)) | rowSums(x_min) &lt;= 0\n    if (any(bad_row)) {\n      x_min[bad_row, ] &lt;- matrix(rep(c(600, 480, 360), each = sum(bad_row)), ncol = 3, byrow = TRUE)\n    }\n    x_min[x_min &lt;= 0 | !is.finite(x_min)] &lt;- 1e-6\n    compositions::ilr(sweep(x_min, 1, rowSums(x_min), \"/\"))\n  }\n\n  ilr_to_minutes &lt;- function(ilr_mat, total = 1440) {\n    stopifnot(is.matrix(ilr_mat), ncol(ilr_mat) == 2)\n    comp_obj &lt;- compositions::ilrInv(ilr_mat)\n    prop &lt;- as.matrix(as.data.frame(comp_obj))\n    bad &lt;- apply(prop, 1, function(r) any(!is.finite(r) | r &lt;= 0) ||\n                   !is.finite(sum(r)) || abs(sum(r) - 1) &gt; 1e-8)\n    if (any(bad)) prop[bad, ] &lt;- 1/3\n    round(prop * total, 1)\n  }\n\n  # ------------------------------------------------------------------------------\n  # Data‑generating function ------------------------------------------------------\n  # ------------------------------------------------------------------------------\n  generate_data &lt;- function(n_pg, effect_min, baseline_days, intervention_days,\n                            s_between, s_within, seed = NULL) {\n\n    if (!is.null(seed)) set.seed(seed)\n\n    N   &lt;- n_pg * 2\n    grp &lt;- rep(0:1, each = n_pg)              # 0 = Control, 1 = Intervention\n\n    # Mean daily compositions: (sedentary, sleep, physical)\n    base_comp   &lt;- c(600, 480, 360)\n    active_comp &lt;- c(600 - effect_min, 480, 360 + effect_min)\n\n    # Person‑level random effects in ILR space\n    b_ilr &lt;- MASS::mvrnorm(N, mu = c(0, 0), Sigma = diag(s_between^2, 2))\n\n    # Person-specific playtime proportion of sedentary time (10-40%)\n    personal_play_prop &lt;- sapply(1:N, function(i) {\n      p &lt;- rbeta(1, 2, 5) * 0.3 + 0.1  # right-skew between 0.1-0.4\n      return(p)\n    })\n\n    # Containers\n    all_ids &lt;- all_periods &lt;- all_days &lt;- NULL\n    all_ilr &lt;- matrix(, 0, 2)\n    all_sedentary &lt;- numeric()  # Store actual sedentary minutes\n\n    for (i in seq_len(N)) {\n      for (period in c(\"baseline\", \"intervention\")) {\n        ndays   &lt;- if (period == \"baseline\") baseline_days else intervention_days\n        comp_mu &lt;- if (period == \"baseline\" || grp[i] == 0) base_comp else active_comp\n\n        comp_ilr &lt;- comp_to_ilr(matrix(rep(comp_mu, ndays), ncol = 3, byrow = TRUE))\n        comp_ilr &lt;- sweep(comp_ilr, 2, b_ilr[i, ], \"+\")               # add person RE\n        day_ilr  &lt;- comp_ilr + MASS::mvrnorm(ndays, mu = c(0, 0),\n                                             Sigma = diag(s_within^2, 2))\n\n        # Index bookkeeping\n        all_ids     &lt;- c(all_ids, rep(i, ndays))\n        all_periods &lt;- c(all_periods, rep(period, ndays))\n        all_days    &lt;- c(all_days,\n                         if (period == \"baseline\") seq_len(baseline_days)\n                         else baseline_days + seq_len(intervention_days))\n        all_ilr     &lt;- rbind(all_ilr, day_ilr)\n        \n        # Store sedentary minutes for this person-period (will be calculated after ILR transformation)\n        # We'll calculate playtime after we have the actual sedentary minutes\n      }\n    }\n\n    # Back‑transform ILR → minutes and calculate playtime based on actual sedentary behavior\n    mins &lt;- ilr_to_minutes(all_ilr)\n    colnames(mins) &lt;- c(\"sedentary\", \"sleep\", \"physical\")\n    \n    # Now generate playtime based on actual sedentary minutes\n    playmin &lt;- numeric(length(all_ids))\n    \n    for (i in seq_along(all_ids)) {\n      person_id &lt;- all_ids[i]\n      period &lt;- all_periods[i]\n      actual_sedentary &lt;- mins[i, \"sedentary\"]\n      \n      # Base playtime as proportion of actual sedentary time\n      base_playtime &lt;- personal_play_prop[person_id] * actual_sedentary\n      \n      # Add small amount of day-to-day noise (2% of base playtime)\n      daily_sd &lt;- 0.02 * base_playtime\n      noisy_playtime &lt;- rnorm(1, base_playtime, daily_sd)\n      \n      # Apply intervention effect for intervention group during intervention period\n      if (period == \"intervention\" && grp[person_id] == 1) {\n        # Reduce playtime by effect_min, but ensure it doesn't go below 0\n        # The reduction is additive (in minutes) to maintain the intended effect size\n        intervention_playtime &lt;- pmax(0, noisy_playtime - effect_min)\n        playmin[i] &lt;- intervention_playtime\n      } else {\n        # Control group or baseline period: just use the playtime based on actual sedentary\n        playmin[i] &lt;- pmax(0, noisy_playtime)  # Ensure non-negative\n      }\n    }\n\n    # Assemble data frame\n    dat &lt;- data.frame(\n      id        = factor(all_ids),\n      group     = factor(grp[all_ids], labels = c(\"Control\", \"Abstinence\")),\n      period    = factor(all_periods, levels = c(\"baseline\", \"intervention\")),\n      day       = all_days,\n      sedentary = mins[, 1],\n      sleep     = mins[, 2],\n      physical  = mins[, 3],\n      playtime  = playmin\n    )\n\n    dat &lt;- dat %&gt;%\n      group_by(id) %&gt;%\n      mutate(\n        base_play_mean      = mean(playtime[period == \"baseline\"]),\n        playtime_reduction  = base_play_mean - playtime,\n        intervention_active = as.integer(group == \"Abstinence\" & period == \"intervention\")\n      ) %&gt;%\n      ungroup()\n\n    return(dat)\n  }\n\n  # ------------------------------------------------------------------------------\n  # Analysis function -------------------------------------------------------------\n  # ------------------------------------------------------------------------------\n  run_analysis &lt;- function(data) {\n    data_ilr &lt;- data\n    comp_matrix &lt;- as.matrix(data[, c(\"sedentary\", \"sleep\", \"physical\")])\n    ilr_coords  &lt;- comp_to_ilr(comp_matrix)\n    data_ilr$ilr1 &lt;- ilr_coords[, 1]\n\n    results &lt;- list()\n\n    ## Between‑group effect during intervention ----------------------------------\n    md &lt;- subset(data_ilr, period == \"intervention\")\n    mb &lt;- try(lmer(ilr1 ~ group + (1 | id), data = md), silent = TRUE)\n    results$p_between &lt;- if (!inherits(mb, \"try-error\")) anova(mb)[\"group\", \"Pr(&gt;F)\"] else NA\n\n    ## Within‑group effects -------------------------------------------------------\n    mc &lt;- try(lmer(ilr1 ~ period + (1 | id), data = subset(data_ilr, group == \"Control\")), silent = TRUE)\n    results$p_control &lt;- if (!inherits(mc, \"try-error\")) anova(mc)[\"period\", \"Pr(&gt;F)\"] else NA\n\n    mi &lt;- try(lmer(ilr1 ~ period + (1 | id), data = subset(data_ilr, group == \"Abstinence\")), silent = TRUE)\n    results$p_intervention &lt;- if (!inherits(mi, \"try-error\")) anova(mi)[\"period\", \"Pr(&gt;F)\"] else NA\n\n    ## Interaction ----------------------------------------------------------------\n    mx &lt;- try(lmer(ilr1 ~ group * period + (1 | id), data = data_ilr), silent = TRUE)\n    results$p_interaction &lt;- if (!inherits(mx, \"try-error\")) anova(mx)[\"group:period\", \"Pr(&gt;F)\"] else NA\n\n    ## Per‑protocol contrast ------------------------------------------------------\n    mp &lt;- try(lmer(ilr1 ~ intervention_active * playtime_reduction + (1 | id), data = data_ilr), silent = TRUE)\n    results$p_protocol &lt;- if (!inherits(mp, \"try-error\")) anova(mp)[\"intervention_active:playtime_reduction\", \"Pr(&gt;F)\"] else NA\n\n    return(results)\n  }\n\n  # ------------------------------------------------------------------------------\n  # Simulation script ------------------------------------------------------------\n  # ------------------------------------------------------------------------------\n  sim %&lt;&gt;% set_script(function() {\n    set.seed(sample.int(1e7, 1))\n    \n    # Access simulation level variables correctly\n    data &lt;- generate_data(\n      n_pg             = L$n_pg,\n      effect_min       = L$effect_min,\n      baseline_days    = L$baseline_days,\n      intervention_days= L$intervention_days,\n      s_between        = L$s_between,\n      s_within         = L$s_within\n    )\n    \n    # Run analysis and ensure proper error handling\n    result &lt;- tryCatch({\n      run_analysis(data)\n    }, error = function(e) {\n      # Return NA values with proper names if analysis fails\n      list(\n        p_between = NA_real_, \n        p_control = NA_real_, \n        p_intervention = NA_real_,\n        p_interaction = NA_real_, \n        p_protocol = NA_real_\n      )\n    })\n    \n    # Ensure result is a proper list with all required elements\n    if (!is.list(result)) {\n      result &lt;- list(\n        p_between = NA_real_, \n        p_control = NA_real_, \n        p_intervention = NA_real_,\n        p_interaction = NA_real_, \n        p_protocol = NA_real_\n      )\n    }\n    \n    # Ensure all required columns exist\n    required_names &lt;- c(\"p_between\", \"p_control\", \"p_intervention\", \"p_interaction\", \"p_protocol\")\n    for (name in required_names) {\n      if (!(name %in% names(result))) {\n        result[[name]] &lt;- NA_real_\n      }\n    }\n    \n    return(result)\n  })\n\n  # ------------------------------------------------------------------------------\n  # Config & run -----------------------------------------------------------------\n  # ------------------------------------------------------------------------------\n  sim %&lt;&gt;% set_config(\n    num_sim      = sims,\n    parallel     = TRUE,   # Enable parallel processing\n    n_cores      = cores,  # Use specified cores\n    packages     = c(\"lme4\", \"lmerTest\", \"compositions\", \"MASS\", \"dplyr\"),\n    progress_bar = TRUE\n  )\n\n  \n  # # Add a test run to debug issues\n  # message(\"Testing data generation and analysis functions...\")\n  # tryCatch({\n  #   test_data &lt;- generate_data(\n  #     n_pg = 10,  # Small test\n  #     effect_min = 30,\n  #     baseline_days = 7,\n  #     intervention_days = 14,\n  #     s_between = 0.15,\n  #     s_within = 0.25\n  #   )\n  #   message(\"✓ Data generation successful\")\n  #   message(\"Test data dimensions: \", nrow(test_data), \" x \", ncol(test_data))\n    \n  #   test_results &lt;- run_analysis(test_data)\n  #   message(\"✓ Analysis function successful\")\n  #   message(\"Test results: \", paste(names(test_results), test_results, sep=\"=\", collapse=\", \"))\n  # }, error = function(e) {\n  #   message(\"❌ Test failed with error: \", e$message)\n  #   stop(\"Stopping due to test failure. Fix the issue before running full simulation.\")\n  # })\n\n  # message(\"Running simulations …\")\n  \n  sim %&lt;&gt;% run()\n\n  # ------------------------------------------------------------------------------\n  # Summarise power --------------------------------------------------------------\n  # ------------------------------------------------------------------------------\n  results &lt;- sim$results\n  \n  # Add debugging information\n  message(\"Debug: Checking simulation results...\")\n  message(\"Results object class: \", class(results))\n  message(\"Results is null: \", is.null(results))\n  if (!is.null(results)) {\n    message(\"Results dimensions: \", nrow(results), \" x \", ncol(results))\n    message(\"Results column names: \", paste(names(results), collapse = \", \"))\n  }\n  \n  # Add error handling for when all simulations fail\n  if (is.null(results) || (is.data.frame(results) && nrow(results) == 0)) {\n    stop(\"All simulations failed. Check your simulation parameters and functions.\")\n  }\n  \n  # Check if required columns exist before processing\n  required_cols &lt;- c(\"p_between\", \"p_control\", \"p_intervention\", \"p_interaction\", \"p_protocol\")\n  missing_cols &lt;- setdiff(required_cols, names(results))\n  if (length(missing_cols) &gt; 0) {\n    stop(paste(\"Missing columns in results:\", paste(missing_cols, collapse = \", \")))\n  }\n  \n  for (col in required_cols) {\n    results[[col]] &lt;- as.numeric(as.character(results[[col]]))\n  }\n\n  power_df &lt;- aggregate(\n    cbind(\n      power_between     = results$p_between     &lt; 0.05,\n      power_control     = results$p_control     &lt; 0.05,\n      power_intervention= results$p_intervention&lt; 0.05,\n      power_interaction = results$p_interaction &lt; 0.05,\n      power_protocol    = results$p_protocol    &lt; 0.05,\n      valid_between     = !is.na(results$p_between),\n      valid_control     = !is.na(results$p_control), \n      valid_intervention= !is.na(results$p_intervention),\n      valid_interaction = !is.na(results$p_interaction),\n      valid_protocol    = !is.na(results$p_protocol)\n    ),\n    by = list(\n      n_pg             = results$n_pg,\n      effect_min       = results$effect_min,\n      s_between        = results$s_between,\n      s_within         = results$s_within,\n      baseline_days    = results$baseline_days,\n      intervention_days= results$intervention_days\n    ),\n    FUN = mean, na.rm = TRUE\n  )\n\n  end_time &lt;- Sys.time()\n  message(sprintf(\"Total elapsed time: %.2f mins\", as.numeric(difftime(end_time, start_time, units = \"mins\"))))\n\n  list(power_summary = power_df, sim_object = sim)\n}\n\n# --------------------------------------------------------------------------------\n# Example call -------------------------------------------------------------------\n# --------------------------------------------------------------------------------\nresult &lt;- est_power_simengine(\n  n_pg               = 50,  # Multiple sample sizes\n  effect_min_values =  c(30, 60, 90, 120),          \n  s_between_values = seq(0.1, 0.3, by = 0.05),\n  s_within_values = seq(0.15, 0.35, by = 0.05),\n  baseline_days      = 7,\n  intervention_days  = 14,\n  sims               = sims_param, \n  cores              = cores_param     \n)\n\n# result &lt;- est_power_simengine(\n#   n_pg               = c(50),  # Multiple sample sizes\n#   effect_min_values =  c(30, 60, 90, 120),          \n#   s_between_values = seq(0.1, 0.3, by = 0.05),\n#   s_within_values = seq(0.15, 0.35, by = 0.05),\n#   baseline_days      = 7,\n#   intervention_days  = 14,\n#   sims               = sims_param, \n#   cores              = cores_param     \n# )\n# print(result$power_summary)\n\n# Save results with descriptive name and timestamp\ntimestamp &lt;- format(Sys.time(), \"%Y%m%d_%H%M%S\")\nfilename &lt;- paste0(\"scripts/sim_comp_debug/power_sim_results_\", timestamp, \".RData\")\nsave(result, file = filename)\n\n# Print power summary\n# print(result$power_summary)\n\n# Print save location\nmessage(\"Results saved to: \", filename)\n\n# POWER SUMMARY ANALYSIS\nmessage(\"\\n\" , paste(rep(\"=\", 60), collapse=\"\"))\nmessage(\"POWER SUMMARY - TOP PERFORMING SETTINGS\")\nmessage(paste(rep(\"=\", 60), collapse=\"\"))\n\npower_data &lt;- result$power_summary\n\n# Summary for power_interaction\nmessage(\"\\n🎯 INTERACTION EFFECT POWER SUMMARY:\")\nmessage(\"-----------------------------------\")\n\n# Find maximum power for interaction\nmax_interaction_power &lt;- max(power_data$power_interaction, na.rm = TRUE)\nbest_interaction &lt;- power_data[which.max(power_data$power_interaction), ]\n\nmessage(sprintf(\"Maximum Interaction Power: %.3f\", max_interaction_power))\nmessage(\"Best settings:\")\nmessage(sprintf(\"  • Sample size per group (n_pg): %d\", best_interaction$n_pg))\nmessage(sprintf(\"  • Effect size (effect_min): %d minutes\", best_interaction$effect_min))\nmessage(sprintf(\"  • Between-subject SD (s_between): %.3f\", best_interaction$s_between))\nmessage(sprintf(\"  • Within-subject SD (s_within): %.3f\", best_interaction$s_within))\n\n# Show top 3 settings for interaction\nmessage(\"\\nTop 3 settings for interaction power:\")\ntop_interaction &lt;- power_data[order(power_data$power_interaction, decreasing = TRUE)[1:min(3, nrow(power_data))], ]\nfor(i in 1:nrow(top_interaction)) {\n  row &lt;- top_interaction[i, ]\n  message(sprintf(\"%d. Power=%.3f | n_pg=%d | effect=%d | s_between=%.3f | s_within=%.3f\", \n                  i, row$power_interaction, row$n_pg, row$effect_min, row$s_between, row$s_within))\n}\n\n# Summary for power_protocol  \nmessage(\"\\n🎯 PROTOCOL EFFECT POWER SUMMARY:\")\nmessage(\"--------------------------------\")\n\n# Find maximum power for protocol\nmax_protocol_power &lt;- max(power_data$power_protocol, na.rm = TRUE)\nbest_protocol &lt;- power_data[which.max(power_data$power_protocol), ]\n\nmessage(sprintf(\"Maximum Protocol Power: %.3f\", max_protocol_power))\nmessage(\"Best settings:\")\nmessage(sprintf(\"  • Sample size per group (n_pg): %d\", best_protocol$n_pg))\nmessage(sprintf(\"  • Effect size (effect_min): %d minutes\", best_protocol$effect_min))\nmessage(sprintf(\"  • Between-subject SD (s_between): %.3f\", best_protocol$s_between))\nmessage(sprintf(\"  • Within-subject SD (s_within): %.3f\", best_protocol$s_within))\n\n# Show top 3 settings for protocol\nmessage(\"\\nTop 3 settings for protocol power:\")\ntop_protocol &lt;- power_data[order(power_data$power_protocol, decreasing = TRUE)[1:min(3, nrow(power_data))], ]\nfor(i in 1:nrow(top_protocol)) {\n  row &lt;- top_protocol[i, ]\n  message(sprintf(\"%d. Power=%.3f | n_pg=%d | effect=%d | s_between=%.3f | s_within=%.3f\", \n                  i, row$power_protocol, row$n_pg, row$effect_min, row$s_between, row$s_within))\n}\n\n# Overall summary statistics\nmessage(\"\\n📊 OVERALL POWER STATISTICS:\")\nmessage(\"---------------------------\")\nmessage(sprintf(\"Interaction Power - Mean: %.3f, Range: %.3f - %.3f\", \n                mean(power_data$power_interaction, na.rm = TRUE),\n                min(power_data$power_interaction, na.rm = TRUE),\n                max(power_data$power_interaction, na.rm = TRUE)))\n                \nmessage(sprintf(\"Protocol Power - Mean: %.3f, Range: %.3f - %.3f\", \n                mean(power_data$power_protocol, na.rm = TRUE),\n                min(power_data$power_protocol, na.rm = TRUE),\n                max(power_data$power_protocol, na.rm = TRUE)))\n\n# DATA QUALITY ANALYSIS\nmessage(\"\\n\" , paste(rep(\"=\", 60), collapse=\"\"))\nmessage(\"DATA QUALITY ANALYSIS - VALIDITY RATES\")\nmessage(paste(rep(\"=\", 60), collapse=\"\"))\n\n# Check validity rates for each contrast\nvalidity_threshold &lt;- 0.95\ntotal_rows &lt;- nrow(power_data)\n\n# Function to analyze validity for each contrast\nanalyze_validity &lt;- function(valid_col, contrast_name) {\n  high_validity_count &lt;- sum(power_data[[valid_col]] &gt; validity_threshold, na.rm = TRUE)\n  perfect_validity_count &lt;- sum(power_data[[valid_col]] == 1.0, na.rm = TRUE)\n  mean_validity &lt;- mean(power_data[[valid_col]], na.rm = TRUE)\n  min_validity &lt;- min(power_data[[valid_col]], na.rm = TRUE)\n  \n  message(sprintf(\"\\n🔍 %s VALIDITY:\", toupper(contrast_name)))\n  message(sprintf(\"  • Rows with validity &gt; %.2f: %d/%d (%.1f%%)\", \n                  validity_threshold, high_validity_count, total_rows, \n                  100 * high_validity_count / total_rows))\n  message(sprintf(\"  • Rows with perfect validity (1.0): %d/%d (%.1f%%)\", \n                  perfect_validity_count, total_rows, \n                  100 * perfect_validity_count / total_rows))\n  message(sprintf(\"  • Mean validity: %.3f\", mean_validity))\n  message(sprintf(\"  • Minimum validity: %.3f\", min_validity))\n  \n  # Identify problematic parameter combinations if any\n  if (high_validity_count &lt; total_rows) {\n    low_validity_rows &lt;- power_data[power_data[[valid_col]] &lt;= validity_threshold, ]\n    message(sprintf(\"  ⚠️  %d rows with validity ≤ %.2f:\", \n                    nrow(low_validity_rows), validity_threshold))\n    for(i in 1:min(3, nrow(low_validity_rows))) {  # Show up to 3 examples\n      row &lt;- low_validity_rows[i, ]\n      message(sprintf(\"     Example %d: validity=%.3f | n_pg=%d | effect=%d | s_between=%.3f | s_within=%.3f\", \n                      i, row[[valid_col]], row$n_pg, row$effect_min, row$s_between, row$s_within))\n    }\n    if (nrow(low_validity_rows) &gt; 3) {\n      message(sprintf(\"     ... and %d more problematic combinations\", nrow(low_validity_rows) - 3))\n    }\n  } else {\n    message(\"  ✅ All parameter combinations produced high-quality results!\")\n  }\n  \n  return(list(\n    high_validity_count = high_validity_count,\n    perfect_validity_count = perfect_validity_count,\n    mean_validity = mean_validity,\n    min_validity = min_validity\n  ))\n}\n\n# Analyze each contrast type\ncontrasts &lt;- list(\n  \"valid_between\" = \"Between-Group\",\n  \"valid_control\" = \"Control Within-Group\", \n  \"valid_intervention\" = \"Intervention Within-Group\",\n  \"valid_interaction\" = \"Group × Period Interaction\",\n  \"valid_protocol\" = \"Per-Protocol\"\n)\n\nvalidity_summary &lt;- list()\nfor(col in names(contrasts)) {\n  validity_summary[[col]] &lt;- analyze_validity(col, contrasts[[col]])\n}\n\n# Overall validity summary\nmessage(\"\\n📋 OVERALL VALIDITY SUMMARY:\")\nmessage(\"---------------------------\")\nall_high_validity &lt;- sapply(validity_summary, function(x) x$high_validity_count)\nall_perfect_validity &lt;- sapply(validity_summary, function(x) x$perfect_validity_count)\nall_mean_validity &lt;- sapply(validity_summary, function(x) x$mean_validity)\n\nmessage(sprintf(\"Contrast with highest reliability: %s (%d/%d rows &gt; %.2f)\", \n                contrasts[[which.max(all_high_validity)]], \n                max(all_high_validity), total_rows, validity_threshold))\nmessage(sprintf(\"Contrast with lowest reliability: %s (%d/%d rows &gt; %.2f)\", \n                contrasts[[which.min(all_high_validity)]], \n                min(all_high_validity), total_rows, validity_threshold))\n\n# Check if all contrasts are highly reliable\nif(all(all_high_validity == total_rows)) {\n  message(\"✅ EXCELLENT: All contrasts have high validity (&gt;95%) across all parameter combinations!\")\n} else {\n  problematic_contrasts &lt;- names(contrasts)[all_high_validity &lt; total_rows]\n  message(sprintf(\"⚠️  WARNING: %d contrast(s) have some parameter combinations with low validity:\", \n                  length(problematic_contrasts)))\n  for(contrast in problematic_contrasts) {\n    message(sprintf(\"   • %s: %d/%d rows with validity ≤ %.2f\", \n                    contrasts[[contrast]], \n                    total_rows - all_high_validity[[contrast]], \n                    total_rows, validity_threshold))\n  }\n}\n\nmessage(\"\\n\" , paste(rep(\"=\", 60), collapse=\"\"))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulation-based Power Analysis for Two-arm RCT with Compositional Outcomes</span>"
    ]
  },
  {
    "objectID": "scripts/sim_comp_report.html#simulate-study",
    "href": "scripts/sim_comp_report.html#simulate-study",
    "title": "4  Simulation-based Power Analysis for Two-arm RCT with Compositional Outcomes",
    "section": "4.5 Simulate study",
    "text": "4.5 Simulate study\n\n\nCode\n# Source the generate_data function (assuming it's in the sim_engine_demo.R file)\ngenerate_data &lt;- function(n_pg, effect_min, baseline_days, intervention_days,\n                          s_between, s_within, seed = NULL) {\n\n  if (!is.null(seed)) set.seed(seed)\n\n  N   &lt;- n_pg * 2\n  grp &lt;- rep(0:1, each = n_pg)              # 0 = Control, 1 = Intervention\n\n  # Mean daily compositions: (sedentary, sleep, physical)\n  base_comp   &lt;- c(600, 480, 360)\n  active_comp &lt;- c(600 - effect_min, 480, 360 + effect_min)\n\n  # Person‑level random effects in ILR space\n  b_ilr &lt;- MASS::mvrnorm(N, mu = c(0, 0), Sigma = diag(s_between^2, 2))\n\n  # Person-specific playtime proportion of sedentary time (10-40%)\n  personal_play_prop &lt;- sapply(1:N, function(i) {\n    p &lt;- rbeta(1, 2, 5) * 0.3 + 0.1  # right-skew between 0.1-0.4\n    return(p)\n  })\n\n  # Containers\n  all_ids &lt;- all_periods &lt;- all_days &lt;- NULL\n  all_ilr &lt;- matrix(, 0, 2)\n  all_sedentary &lt;- numeric()  # Store actual sedentary minutes\n\n  for (i in seq_len(N)) {\n    for (period in c(\"baseline\", \"intervention\")) {\n      ndays   &lt;- if (period == \"baseline\") baseline_days else intervention_days\n      comp_mu &lt;- if (period == \"baseline\" || grp[i] == 0) base_comp else active_comp\n\n      comp_ilr &lt;- comp_to_ilr(matrix(rep(comp_mu, ndays), ncol = 3, byrow = TRUE))\n      comp_ilr &lt;- sweep(comp_ilr, 2, b_ilr[i, ], \"+\")               # add person RE\n      day_ilr  &lt;- comp_ilr + MASS::mvrnorm(ndays, mu = c(0, 0),\n                                            Sigma = diag(s_within^2, 2))\n\n      # Index bookkeeping\n      all_ids     &lt;- c(all_ids, rep(i, ndays))\n      all_periods &lt;- c(all_periods, rep(period, ndays))\n      all_days    &lt;- c(all_days,\n                        if (period == \"baseline\") seq_len(baseline_days)\n                        else baseline_days + seq_len(intervention_days))\n      all_ilr     &lt;- rbind(all_ilr, day_ilr)\n      \n      # Store sedentary minutes for this person-period (will be calculated after ILR transformation)\n      # We'll calculate playtime after we have the actual sedentary minutes\n    }\n  }\n\n  # Back‑transform ILR → minutes and calculate playtime based on actual sedentary behavior\n  mins &lt;- ilr_to_minutes(all_ilr)\n  colnames(mins) &lt;- c(\"sedentary\", \"sleep\", \"physical\")\n  \n  # Now generate playtime based on actual sedentary minutes\n  playmin &lt;- numeric(length(all_ids))\n  \n  for (i in seq_along(all_ids)) {\n    person_id &lt;- all_ids[i]\n    period &lt;- all_periods[i]\n    actual_sedentary &lt;- mins[i, \"sedentary\"]\n    \n    # Base playtime as proportion of actual sedentary time\n    base_playtime &lt;- personal_play_prop[person_id] * actual_sedentary\n    \n    # Add small amount of day-to-day noise (2% of base playtime)\n    daily_sd &lt;- 0.02 * base_playtime\n    noisy_playtime &lt;- rnorm(1, base_playtime, daily_sd)\n    \n    # Apply intervention effect for intervention group during intervention period\n    if (period == \"intervention\" && grp[person_id] == 1) {\n      # Reduce playtime by effect_min, but ensure it doesn't go below 0\n      # The reduction is additive (in minutes) to maintain the intended effect size\n      intervention_playtime &lt;- pmax(0, noisy_playtime - effect_min)\n      playmin[i] &lt;- intervention_playtime\n    } else {\n      # Control group or baseline period: just use the playtime based on actual sedentary\n      playmin[i] &lt;- pmax(0, noisy_playtime)  # Ensure non-negative\n    }\n  }\n\n  # Assemble data frame\n  dat &lt;- data.frame(\n    id        = factor(all_ids),\n    group     = factor(grp[all_ids], labels = c(\"Control\", \"Abstinence\")),\n    period    = factor(all_periods, levels = c(\"baseline\", \"intervention\")),\n    day       = all_days,\n    sedentary = mins[, 1],\n    sleep     = mins[, 2],\n    physical  = mins[, 3],\n    playtime  = playmin\n  )\n\n  dat &lt;- dat %&gt;%\n    group_by(id) %&gt;%\n    mutate(\n      base_play_mean      = mean(playtime[period == \"baseline\"]),\n      playtime_reduction  = base_play_mean - playtime,\n      intervention_active = as.integer(group == \"Abstinence\" & period == \"intervention\")\n    ) %&gt;%\n    ungroup()\n\n  return(dat)\n}\n# Generate sample data with moderate effect size\nset.seed(123)\nsample_data &lt;- generate_data(\n  n_pg = 50,              # 50 participants per group\n  effect_min = 60,         \n  baseline_days = 7,      # 7 days baseline\n  intervention_days = 14, # 14 days intervention\n  s_between = 0.3,        # moderate between-subject variability\n  s_within = 0.2,         # moderate within-subject variability\n  seed = 123\n)\n\n# Check the structure of the generated data\nglimpse(sample_data)\n\n\nRows: 2,100\nColumns: 11\n$ id                  &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ group               &lt;fct&gt; Control, Control, Control, Control, Control, Contr…\n$ period              &lt;fct&gt; baseline, baseline, baseline, baseline, baseline, …\n$ day                 &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ sedentary           &lt;dbl&gt; 424.2, 599.8, 532.8, 539.3, 511.4, 586.7, 606.4, 5…\n$ sleep               &lt;dbl&gt; 670.0, 538.0, 668.0, 480.7, 562.6, 530.5, 449.5, 5…\n$ physical            &lt;dbl&gt; 345.8, 302.2, 239.3, 420.0, 366.1, 322.8, 384.1, 3…\n$ playtime            &lt;dbl&gt; 120.73684, 173.81501, 151.84373, 156.04261, 142.45…\n$ base_play_mean      &lt;dbl&gt; 155.1893, 155.1893, 155.1893, 155.1893, 155.1893, …\n$ playtime_reduction  &lt;dbl&gt; 34.4524824, -18.6256825, 3.3455976, -0.8532876, 12…\n$ intervention_active &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\nCode\n# Calculate daily means for compositions across all participants by group and period\ndaily_composition_means &lt;- sample_data %&gt;%\n  group_by(group, period, day) %&gt;%\n  summarise(\n    sedentary_mean = mean(sedentary),\n    sleep_mean = mean(sleep),\n    physical_mean = mean(physical),\n    .groups = \"drop\"\n  ) %&gt;%\n  pivot_longer(\n    cols = c(sedentary_mean, sleep_mean, physical_mean),\n    names_to = \"component\",\n    values_to = \"minutes\"\n  ) %&gt;%\n  mutate(\n    component = factor(\n      gsub(\"_mean\", \"\", component),\n      levels = c(\"sedentary\", \"sleep\", \"physical\"),\n      labels = c(\"Sedentary\", \"Sleep\", \"Physical Activity\")\n    )\n  )\n\n# Longitudinal compositional barplot\ncomposition_longitudinal &lt;- ggplot(daily_composition_means, \n                                  aes(x = day, y = minutes, fill = component)) +\n  geom_col(position = \"stack\", alpha = 0.8) +\n  geom_vline(xintercept = 7.5, linetype = \"dashed\", color = \"black\", size = 1) +\n  annotate(\"text\", x = 4, y = 1300, label = \"Baseline\", hjust = 0.5, size = 4, fontface = \"bold\") +\n  annotate(\"text\", x = 14, y = 1300, label = \"Intervention\", hjust = 0.5, size = 4, fontface = \"bold\") +\n  facet_wrap(~ group, \n             labeller = labeller(\n               group = c(\"Control\" = \"Control Group\", \"Abstinence\" = \"Abstinence Group\")\n             )) +\n  labs(\n    title = \"Daily Mean Compositions Over Time\",\n    subtitle = \"24-hour time use patterns by study day (stacked bars)\",\n    x = \"Study Day\",\n    y = \"Minutes per Day\",\n    fill = \"Component\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 12, face = \"bold\"),\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.x = element_blank()\n  ) +\n  scale_fill_viridis_d(option = \"plasma\", begin = 0.2, end = 0.8) +\n  scale_y_continuous(breaks = seq(0, 1400, 200)) +\n  scale_x_continuous(breaks = seq(0, 25, 5))\n\nprint(composition_longitudinal)\n\n\n\n\n\n\n\n\n\nCode\n# Calculate daily means for playtime by group and period\ndaily_playtime_means &lt;- sample_data %&gt;%\n  group_by(group, period, day) %&gt;%\n  summarise(\n    playtime_mean = mean(playtime),\n    playtime_se = sd(playtime) / sqrt(n()),\n    .groups = \"drop\"\n  )\n\n# Longitudinal plot for daily mean playtime - now faceted by group like the composition plot\nplaytime_longitudinal &lt;- ggplot(daily_playtime_means, \n                               aes(x = day, y = playtime_mean, color = group)) +\n  geom_line(size = 1.2, alpha = 0.8) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_ribbon(aes(ymin = playtime_mean - playtime_se, \n                  ymax = playtime_mean + playtime_se, \n                  fill = group), \n              alpha = 0.2, color = NA) +\n  geom_vline(xintercept = 7.5, linetype = \"dashed\", color = \"black\", size = 1) +\n  annotate(\"text\", x = 4, y = 175, label = \"Baseline\", hjust = 0.5, size = 4, fontface = \"bold\") +\n  annotate(\"text\", x = 14, y = 175, label = \"Intervention\", hjust = 0.5, size = 4, fontface = \"bold\") +\n  facet_wrap(~ group, \n             labeller = labeller(\n               group = c(\"Control\" = \"Control Group\", \"Abstinence\" = \"Abstinence Group\")\n             )) +\n  labs(\n    title = \"Daily Mean Playtime Over Time\",\n    subtitle = \"Gaming/screen time trends by study day with standard error bands\",\n    x = \"Study Day\",\n    y = \"Mean Playtime (minutes per day)\",\n    color = \"Group\",\n    fill = \"Group\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 12, face = \"bold\"),\n    panel.grid.minor.x = element_blank()\n  ) +\n  scale_color_manual(values = c(\"Control\" = \"#2E8B57\", \"Abstinence\" = \"#FF6347\")) +\n  scale_fill_manual(values = c(\"Control\" = \"#2E8B57\", \"Abstinence\" = \"#FF6347\")) +\n  scale_y_continuous(breaks = seq(0, 200, 25)) +\n  scale_x_continuous(breaks = seq(0, 25, 5))\n\nprint(playtime_longitudinal)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate daily averages for compositions by group and period\ncomposition_summary &lt;- sample_data %&gt;%\n  group_by(group, period, id) %&gt;%\n  summarise(\n    sedentary = mean(sedentary),\n    sleep = mean(sleep), \n    physical = mean(physical),\n    .groups = \"drop\"\n  ) %&gt;%\n  pivot_longer(\n    cols = c(sedentary, sleep, physical),\n    names_to = \"component\",\n    values_to = \"minutes\"\n  ) %&gt;%\n  mutate(\n    component = factor(component, \n                      levels = c(\"sedentary\", \"sleep\", \"physical\"),\n                      labels = c(\"Sedentary\", \"Sleep\", \"Physical Activity\"))\n  )\n\n# Create 2x2 faceted plot for compositions\ncomposition_plot &lt;- ggplot(composition_summary, \n                          aes(x = component, y = minutes, fill = component)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.3, size = 0.8) +\n  facet_grid(period ~ group, \n             labeller = labeller(\n               period = c(\"baseline\" = \"Baseline\", \"intervention\" = \"Intervention\"),\n               group = c(\"Control\" = \"Control Group\", \"Abstinence\" = \"Abstinence Group\")\n             )) +\n  labs(\n    title = \"Daily Average Compositions by Group and Period\",\n    subtitle = \"24-hour time use patterns (minutes per day)\",\n    x = \"Activity Component\",\n    y = \"Minutes per Day\",\n    fill = \"Component\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 12, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  ) +\n  scale_fill_viridis_d(option = \"plasma\", begin = 0.2, end = 0.8) +\n  scale_y_continuous(breaks = seq(0, 800, 100))\n\nprint(composition_plot)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate playtime averages by group and period  \nplaytime_summary &lt;- sample_data %&gt;%\n  group_by(group, period, id) %&gt;%\n  summarise(\n    avg_playtime = mean(playtime),\n    .groups = \"drop\"\n  )\n\n# Create 2x2 faceted plot for playtime\nplaytime_plot &lt;- ggplot(playtime_summary, \n                       aes(x = group, y = avg_playtime, fill = group)) +\n  geom_boxplot(alpha = 0.7, width = 0.6) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5, size = 1.2) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, \n               fill = \"white\", color = \"black\") +\n  facet_wrap(~ period, \n             labeller = labeller(\n               period = c(\"baseline\" = \"Baseline Period\", \n                         \"intervention\" = \"Intervention Period\")\n             )) +\n  labs(\n    title = \"Average Daily Playtime by Group and Period\",\n    subtitle = \"Gaming/screen time reduction intervention effect\",\n    x = \"Study Group\",\n    y = \"Average Playtime (minutes per day)\",\n    fill = \"Group\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 12, face = \"bold\"),\n    panel.grid.minor = element_blank()\n  ) +\n  scale_fill_manual(values = c(\"Control\" = \"#2E8B57\", \"Intervention\" = \"#FF6347\")) +\n  scale_y_continuous(breaks = seq(0, 200, 25))\n\nprint(playtime_plot)\n\n\n\n\n\n\n\n\n\nCode\n# Summary statistics table\nplaytime_stats &lt;- playtime_summary %&gt;%\n  group_by(group, period) %&gt;%\n  summarise(\n    n = n(),\n    mean_playtime = round(mean(avg_playtime), 1),\n    sd_playtime = round(sd(avg_playtime), 1),\n    median_playtime = round(median(avg_playtime), 1),\n    q25 = round(quantile(avg_playtime, 0.25), 1),\n    q75 = round(quantile(avg_playtime, 0.75), 1),\n    .groups = \"drop\"\n  )\n\n# Display summary table\nknitr::kable(\n  playtime_stats,\n  caption = \"Summary Statistics for Daily Playtime by Group and Period\",\n  col.names = c(\"Group\", \"Period\", \"N\", \"Mean\", \"SD\", \"Median\", \"Q25\", \"Q75\")\n)\n\n\n\nSummary Statistics for Daily Playtime by Group and Period\n\n\nGroup\nPeriod\nN\nMean\nSD\nMedian\nQ25\nQ75\n\n\n\n\nControl\nbaseline\n50\n106.1\n36.0\n104.4\n80.4\n120.8\n\n\nControl\nintervention\n50\n106.3\n37.5\n99.0\n78.0\n121.4\n\n\nAbstinence\nbaseline\n50\n118.0\n48.9\n107.7\n80.1\n134.1\n\n\nAbstinence\nintervention\n50\n48.8\n45.6\n35.5\n14.7\n67.5",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulation-based Power Analysis for Two-arm RCT with Compositional Outcomes</span>"
    ]
  },
  {
    "objectID": "scripts/sim_sleepquality_report.html",
    "href": "scripts/sim_sleepquality_report.html",
    "title": "3  Generate Synthetic Data",
    "section": "",
    "text": "3.1 Introduction\nThis document demonstrates a complete workflow for generating synthetic data, introducing dropout, and fitting models (a GAM or MLM) to the data. The main focus is on the sim_study function, which orchestrates the data simulation, dropout process, and model fitting.\nAs an overview, we compare the following possible models for estimating the effect of gaming reduction, for both H2a (the intention-to-treat effect) and H2b (the per-protocol effect; i.e., the effect of actual gaming reduction relative to one’s own baseline). The models we compare are:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "scripts/sim_sleepquality_report.html#introduction",
    "href": "scripts/sim_sleepquality_report.html#introduction",
    "title": "3  Generate Synthetic Data",
    "section": "",
    "text": "ITT = Intention-to-treat; PP = per-protocol\n\n\n\n\n\n\n\n\nModel Name\nSyntax\nTarget Effect\nNotes\n\n\n\n\nGAM\ngam(sleep_quality ~ condition:intervention_period + age + gender + s(id, bs = \"re\") + s(day, by = condition, bs = \"tp\"), correlation = corAR1(form = ~ day | id))\nITT\n\n\n\nGAM with no main effect\ngam(sleep_quality ~         age + gender +         s(id, bs = \"re\") +         s(day, by = condition, bs = \"tp\"),       data = dat,       correlation = corCAR1(form = ~ day | id))\nITT\nHere we do not estimate a parameter for the effect of the intervention directly; rather, we simply fit separate curves to each condition and calculate the average marginal effect using {emmeans}\n\n\nMLM\nlme(     fixed = sleep_quality ~ condition*intervention_period + age + gender,     random = ~ 1|id,     correlation = corCAR1(form = ~ day | id),     method = \"ML\"   )\nITT\nMultiple versions of this model failed when including random slopes; we therefore dropped these\n\n\nMLM Simple\nlme(     fixed = sleep_quality ~ baseline + condition + age + gender,     random = ~ 1 + condition|id,     correlation = corCAR1(form = ~ day | id),     method = \"ML\",   )\nITT\nHere we do not model the baseline (pre-intervention period) itself—we model only the 14-day period when the intervention is active, using average sleep_quality during baseline as a covariate\n\n\nGLS (generalized least squares)\ngls(     sleep_quality ~ condition * intervention_period + age + gender,     correlation = corCAR1(form = ~ day | id),   )\nITT\n\n\n\nGLS Simple\ngls(     sleep_quality ~ condition + baseline + age + gender,     correlation = corAR1(form = ~ day | id),   )\nITT\n\n\n\nGLS Splines\ngls(     sleep_quality ~ ns(day, df = 4) * intervention_period * condition,     correlation = corCAR1(form = ~ day | id),     data = dat   )\nITT\nIn this version, we fit a GLS but allow non-linearity in the trajectory of sleep_quality using splines\n\n\nMLM Reduction\nlme(     fixed = sleep_quality ~ intervention_active*reduction + age + gender,     random = ~ 1 + intervention_active*reduction | id,     correlation = corCAR1(form = ~ day | id)   )\nPP\nHere we test our intended model for the per-protocol effect; reduction is the number of hours played relative to that person’s mean playtime at baseline\n\n\n\n\n3.1.1 Take-aways\nOur simulations show that several models perform well at parameter recovery for the ITT effect, but that the GAM model has the highest power for small effects—the type of effects we believe we are most likely to observed—and for non-linear trajectories over the 14 day period (e.g., an effect that slowly accumulates over a couple of days and then plateaus, or a temporary withdrawal followed by a later improvement). The GAM has approximately 50% power for a standardized effect of .2, and 80% power for a standardized effect of .3, but this varies based the shape of that effect over time.\nThe MLM Reduction model performs very well, and has &gt;95% power for standardized effects of approximately .2 or greater.\n\n\n3.1.2 Load Libraries\nFirst we load packages with pacman, which is fully compatible with renv.\n\n\nShow code (load libraries)\nlibrary(pacman)\n\np_load(tidyverse, qualtRics, lme4, mgcv, marginaleffects, broom, forestplot, broom.mixed, nlme, rms, emmeans, splines, furrr, extraDistr)\n\n# Replace your current plan() line with:\nif (interactive()) {\n  plan(multisession, workers = parallel::detectCores()-8)\n} else {\n  plan(sequential)\n}\n# Diagnostic information about parallel setup\n\nmessage(\"=== PARALLEL PROCESSING SETUP ===\")\nmessage(\"Total CPU cores detected: \", parallel::detectCores())\nmessage(\"Number of workers allocated: \", nbrOfWorkers())\nmessage(\"Future plan: \", paste(class(plan()), collapse = \", \"))\nmessage(\"===================================\")\n\ntheme_set(theme_minimal())\ntheme_update(\n  strip.background = element_rect(fill = \"black\"),\n  strip.text = element_text(color = \"white\", size = 10),\n  panel.grid.minor = element_blank(),\n  panel.border = element_rect(colour = \"black\", fill = NA, linewidth = 1),\n)\n\noptions(scipen = 999)\n\n\n\n\n3.1.3 Simulation, Dropout, and Fitting Functions\nHere we define:\n\nsim_data: Generates synthetic data with random intercepts/slopes and AR(1) errors.\nsim_dropout: Introduces missingness and dropout in the dataset.\nfit_*: Fits a statistical model to the simulated data (see table above)\nsim_study: Ties everything together—generates data, applies dropout, then fits the chosen model and returns a tidy summary.\n\n\n\nShow code (sim functions)\n#' Generate Synthetic Data\n#'\n#' This function simulates synthetic panel data for `n` participants over `n_days` time points,\n#' with an intervention effect, random intercepts and slopes, and AR(1)-correlated residuals.\n#'\n#' @param n Number of participants. Default is 100.\n#' @param n_days Number of time points per participant.\n#' @param b Fixed effect of condition (if mediated == FALSE) or a 1-hour reduction in playtime.\n#' @param phi AR(1) autocorrelation coefficient.\n#' @param sigma AR(1) residual standard deviation.\n\nsim_data &lt;- function(n = 100,\n                     n_days = 28,\n                     \n                     # effect parameters\n                     b = 0.8, # effect in unstandardized units (scaled for 5-25 range)\n                     mu = 15, # grand mean of the outcome (center of 5-25 range)\n                     \n                     \n                     # random effects parameters\n                     tau_int = 2.5,   # Random intercept SD (between-person variance)\n                     tau_slope = .05,  # Random slope SD \n                     within_person_sd = 2.0, # Within-person SD (scaled for 5-25 range)\n                     \n                     # AR(1) parameters\n                     phi = 0.8,     # Autocorrelation coefficient\n                     effect_shape = \"grow\",\n                     k = .5, # affects how quickly the plateau effect plateaus\n                     \n                     mediated = FALSE,\n                     \n                     # playtime parameters\n                     playtime_grand_mean = 1,   # Average baseline playtime in hours\n                     playtime_grand_sd = .5,   # SD for baseline playtime in log units (log-normal distribution)\n                     daily_play_sd = 0.5      # Daily noise in playtime\n                     # compliance_mean = 0.7,    # Average reduction (in hours) for intervention group during intervention period\n)     \n{\n  dat &lt;- tibble(\n    id = 1:n,\n    age = sample(18:36, n, replace = TRUE),\n    gender = sample(c(\"man\",\"woman\",\"non-binary\"), n, prob = c(.45, .45, .1), replace = TRUE),\n    condition = factor(sample(c(\"control\", \"intervention\"), n, replace = TRUE)),\n    experimental_condition = ifelse(condition == \"intervention\", 1, 0),\n    intercept_sq = rnorm(n, 0, tau_int), # Renamed from intercept_wb\n    slope_sq = rnorm(n, 0, tau_slope), # Renamed from slope_wb\n    intercept_play = rlnorm(n, log(playtime_grand_mean), playtime_grand_sd),\n  ) |&gt; \n    # expand to 28 waves per id\n    crossing(\n      day = 1:n_days\n    ) |&gt; \n    mutate(\n      intervention_period = as.numeric(day &gt; 7 & day &lt; 22),\n      intervention_active = intervention_period & condition == \"intervention\",\n      compliance = ifelse(intervention_active, rkumar(n*n_days, a = .05, b = .1), 0),\n      \n      # In the baseline period, play is just the subject's baseline plus some day-to-day noise\n      # During the intervention, experimental subjects reduce play by their compliance amount\n      playtime = (1 - compliance) * rlnorm(n, log(intercept_play), daily_play_sd),\n      effect_time = case_when(\n        effect_shape == \"plateau\" ~ if_else(intervention_period == 1, (b + slope_sq) * (1-exp(-k * (day - 7))), 0), # Renamed from slope_wb\n        effect_shape == \"grow\" ~ if_else(intervention_period == 1, (day - 7) * ((b + slope_sq)/7), 0), # Renamed from slope_wb\n        TRUE ~ NA_real_\n      ),\n    ) |&gt; \n    group_by(id) |&gt; \n    mutate(\n      \n      baseline_playtime = mean(playtime[day &lt;= 7]),\n      reduction = baseline_playtime - playtime, # The mediator: reduction in play relative to the baseline average\n      sigma = within_person_sd * sqrt(1-phi^2),\n      # Generate AR(1) errors for each participant\n      e = as.numeric(arima.sim(n = n_days, \n                               model = list(ar = phi), \n                               sd = sigma)),\n      # Add random effect + fixed effect + AR(1) error\n      sleep_quality_raw = case_when( # Renamed from wellbeing\n        mediated == TRUE ~ mu + \n                            intercept_sq + # Renamed from intercept_wb\n                            effect_time * reduction + \n                            .01*(age-18) +\n                            -.05*(gender %in% c(\"women\",\"non-binary\")) + \n                            e,\n        mediated == FALSE ~ mu + \n                            intercept_sq + # Renamed from intercept_wb\n                            effect_time * experimental_condition * intervention_period + \n                            .01*(age-18) +\n                            -.05*(gender %in% c(\"women\",\"non-binary\")) + \n                            e\n      ),\n      # Constrain to 5-25 range and round to integers\n      sleep_quality = round(pmax(5, pmin(25, sleep_quality_raw)))\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(across(where(is.numeric), ~ round(., 3)))\n  \n  dat\n\n}\n\n\n\n\nShow code (sim data example)\nsim_data(n = 10, n_days = 10, effect_shape = \"grow\")\n\n\n# A tibble: 100 × 20\n      id   age gender     condition experimental_condition intercept_sq slope_sq\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;fct&gt;                      &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1     1    18 non-binary control                        0        -2.17    0.023\n 2     1    18 non-binary control                        0        -2.17    0.023\n 3     1    18 non-binary control                        0        -2.17    0.023\n 4     1    18 non-binary control                        0        -2.17    0.023\n 5     1    18 non-binary control                        0        -2.17    0.023\n 6     1    18 non-binary control                        0        -2.17    0.023\n 7     1    18 non-binary control                        0        -2.17    0.023\n 8     1    18 non-binary control                        0        -2.17    0.023\n 9     1    18 non-binary control                        0        -2.17    0.023\n10     1    18 non-binary control                        0        -2.17    0.023\n# ℹ 90 more rows\n# ℹ 13 more variables: intercept_play &lt;dbl&gt;, day &lt;dbl&gt;,\n#   intervention_period &lt;dbl&gt;, intervention_active &lt;lgl&gt;, compliance &lt;dbl&gt;,\n#   playtime &lt;dbl&gt;, effect_time &lt;dbl&gt;, baseline_playtime &lt;dbl&gt;,\n#   reduction &lt;dbl&gt;, sigma &lt;dbl&gt;, e &lt;dbl&gt;, sleep_quality_raw &lt;dbl&gt;,\n#   sleep_quality &lt;dbl&gt;\n\n\nShow code (sim data example)\n#' Simulate Dropout\n#'\n#' Introduces missingness and dropout into a dataset by randomly assigning records as missing\n#' or dropped out. Once a participant is dropped out, all subsequent records become missing.\n#'\n#' @param dat A tibble generated by \\code{sim_data()}.\n#'\n#' @return A tibble of the same structure as \\code{dat}, but with some \\code{sleep_quality} values set to NA. # Renamed from wellbeing\n#'\nsim_dropout &lt;- function(dat) {\n  \n  dropout &lt;- dat |&gt; \n    mutate(\n      missing = sample(c(TRUE, FALSE), n(), replace = TRUE, prob = c(.10, .90)),\n      dropout = sample(c(TRUE, FALSE), n(), replace = TRUE, prob = c(.01, .99))\n    ) |&gt;\n    mutate(\n      missing = ifelse(cumsum(dropout) &gt; 0, TRUE, missing),\n      .by = id\n    ) |&gt;\n    arrange(as.integer(id), day) |&gt; \n    mutate(sleep_quality = ifelse(missing, NA, sleep_quality)) # Renamed from wellbeing\n  dropout\n}\n\n\n\n\nShow code (fit functions)\n#' Fit a Generalized Additive Model (GAM)\n#'\n#' Fits a GAM model to the provided dataset using \\code{mgcv::gam}, including an AR(1)\n#' correlation structure and random intercept for each ID.\n#'\n#' @param dat A tibble of repeated-measures data (e.g., from \\code{sim_data()} and \\code{sim_dropout()}).\n#'\n#' @return An object of class \\code{gam}, which is the fitted GAM model.\n#'\n#' \nfit_gam &lt;- function(dat) {\n  \n  gam(sleep_quality ~ # Renamed from wellbeing\n        condition:intervention_period + age + gender +\n        s(id, bs = \"re\") + \n        s(day, by = condition, bs = \"tp\"), \n      data = dat,\n      correlation = corAR1(form = ~ day | id))\n}\n\nfit_gam_no_main &lt;- function(dat) {\n  \n  gam(sleep_quality ~ # Renamed from wellbeing\n        age + gender +\n        s(id, bs = \"re\") + \n        s(day, by = condition, bs = \"tp\"), \n      data = dat,\n      correlation = corCAR1(form = ~ day | id))\n}\n\n#' Fit a Multi-Level Model (MLM)\n#'\n#' Fits a linear mixed-effects model (LME) with random intercept for each ID using \\code{lme4::lmer}.\n#'\n#' @param dat A tibble of repeated-measures data (e.g., from \\code{sim_data()} and \\code{sim_dropout()}).\n#'\n#' @return An object of class \\code{lmerMod}, which is the fitted MLM model.\n#'\nfit_mlm &lt;- function(dat) {\n  # lmer(sleep_quality ~ condition*intervention_period + age + gender + (1|id), data = dat) # Renamed from wellbeing\n  lme(\n    fixed = sleep_quality ~ condition*intervention_period + age + gender, # Renamed from wellbeing\n    random = ~ 1|id,  # or use a more flexible structure if needed\n    correlation = corCAR1(form = ~ day | id),\n    method = \"ML\",\n    data = dat |&gt; filter(!is.na(sleep_quality)) # Renamed from wellbeing\n  )\n}\n\nfit_mlm_simple &lt;- function(dat) {\n  \n  tmp &lt;- dat |&gt; \n    group_by(id) |&gt; \n    # take the mean of days 1-7 \n    mutate(baseline = mean(sleep_quality[day &lt; 8], na.rm = TRUE)) |&gt; # Renamed from wellbeing\n    filter(intervention_period == 1) |&gt; \n    filter(!is.na(sleep_quality)) # Renamed from wellbeing\n  \n  lme(\n    fixed = sleep_quality ~ baseline + condition + age + gender, # Renamed from wellbeing\n    random = ~ 1|id,  # Only random intercept since condition is between-subjects\n    correlation = corCAR1(form = ~ day | id),\n    method = \"ML\",\n    data = tmp |&gt; filter(!is.na(sleep_quality)) # Renamed from wellbeing\n  )\n}\n\nfit_gls &lt;- function(dat) {\n  \n  gls(\n    sleep_quality ~ condition * intervention_period + age + gender, # Renamed from wellbeing\n    correlation = corCAR1(form = ~ day | id),\n    data = dat |&gt; filter(!is.na(sleep_quality)) # Renamed from wellbeing\n  )\n}\n\nfit_gls_simple &lt;- function(dat) {\n  \n  tmp &lt;- dat |&gt; \n    group_by(id) |&gt; \n    # take the mean of days 1-7 \n    mutate(baseline = mean(sleep_quality[day &lt; 8], na.rm = TRUE)) |&gt; # Renamed from wellbeing\n    filter(intervention_period == 1) |&gt; \n    filter(!is.na(sleep_quality)) # Renamed from wellbeing\n  \n  gls(\n    sleep_quality ~ condition + baseline + age + gender, # Renamed from wellbeing\n    correlation = corAR1(form = ~ day | id),\n    data = tmp\n  )\n}\n\nfit_gls_spline &lt;- function(dat) {\n  gls(\n    sleep_quality ~ ns(day, df = 4) * intervention_period * condition, # Renamed from wellbeing, corrected comma\n    correlation = corCAR1(form = ~ day | id),\n    data = dat\n  )\n}\n\nfit_mlm_reduction &lt;- function(dat) {\n  lme(\n    fixed = sleep_quality ~ intervention_active*reduction + age + gender, # Renamed from wellbeing\n    random = ~ 1 | id,  # Simplified to random intercept only\n    correlation = corCAR1(form = ~ day | id),\n    data = dat\n  )\n}\n\n# Helper function to extract the focal effect for GLS models\nextract_marginal_effect &lt;- function(mod, dat, focal_term = \"conditionintervention\") {\n  # Here we assume your GLS model is specified with condition*intervention_period\n  # and you want the effect of condition (e.g., intervention vs. control) during intervention.\n  # We create a reference grid that fixes intervention_period at 1.\n  rg &lt;- ref_grid(mod, data = dat, at = list(intervention_period = 1))\n  \n  # Obtain estimated marginal means for each condition.\n  emm &lt;- emmeans(rg, ~ condition)\n  \n  # Compute the pairwise contrast (e.g., intervention - control)\n  # Adjust names as needed. The contrast below returns a one-row summary.\n  contr &lt;- emmeans::contrast(emm, method = list(\"intervention - control\" = c(-1, 1)), adjust = \"none\")\n  contr_sum &lt;- summary(contr, infer = TRUE)\n  \n  # Construct a one-row data frame with consistent column names.\n  # If you have more than one contrast, you might need to filter for the one of interest.\n  df &lt;- data.frame(\n    term = focal_term,\n    estimate = contr_sum$estimate,\n    std.error = contr_sum$SE,\n    conf.low = contr_sum$lower.CL,\n    conf.high = contr_sum$upper.CL,\n    row.names = NULL\n  )\n  \n  return(df)\n}\n\n\n\n\nCode\n#' Simulation Study Orchestrator\n#'\n#' A higher-level function that ties together data simulation, dropout, and model fitting,\n#' returning a tidy summary of the fitted model parameters.\n#'\n#' @param model_function A function to fit the model. Defaults to \\code{fit_gam}.\n#' @param n Number of participants passed to \\code{sim_data()}. Default is 1000.\n#' @param n_days Number of time points per participant passed to \\code{sim_data()}. Default is 28.\n#' @param b Fixed effect for the intervention slope passed to \\code{sim_data()}. Default is 0.01.\n#' @param phi AR(1) autocorrelation coefficient passed to \\code{sim_data()}. Default is 0.7.\n#' @param sigma AR(1) residual standard deviation passed to \\code{sim_data()}. Default is 0.6.\n#'\n#' @return A data frame (tibble) of model estimates from \\code{broom::tidy(parametric = TRUE)}.\n#'\n\n# Updated simulation orchestrator that handles GLS models separately.\nsim_study &lt;- function(model = \"fit_gam\", focal_term = \"intervention_activeTRUE:reduction\", ...) {\n  args &lt;- list(...)\n  dat &lt;- do.call(sim_data, args)\n  model_function &lt;- get(model)\n  \n  mod &lt;- model_function(dat)\n  \n  if (model %in% c(\"fit_gam_no_main\",\"fit_gls_spline\")) {\n    # Extract the effect using our helper function.\n    result &lt;- suppressMessages(extract_marginal_effect(mod, \n                                                       dat, \n                                                       focal_term = focal_term))\n  } else {\n    # For models that work with broom, extract the focal parameter.\n    # Adjust the filtering term as needed.\n    result &lt;- broom::tidy(mod, parametric = TRUE) |&gt;\n      filter(term == focal_term) |&gt; \n      # filter(\n      #   term == \"conditionintervention:intervention_period\" | \n      #     (model %in% c(\"fit_mlm_simple\",\"fit_gls_simple\") & term == \"conditionintervention\")\n      # ) |&gt; \n      mutate(\n        conf.low = estimate - 1.96 * std.error,\n        conf.high = estimate + 1.96 * std.error\n      )\n  }\n  result\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "scripts/sim_sleepquality_report.html#test-and-plot-one-simulated-study",
    "href": "scripts/sim_sleepquality_report.html#test-and-plot-one-simulated-study",
    "title": "3  Generate Synthetic Data",
    "section": "3.2 Test and Plot One Simulated Study",
    "text": "3.2 Test and Plot One Simulated Study\nBelow, we create a sample dataset using sim_data() and examine it with a line plots by day. We can also see whether the simulated SDs for wellbeing align with the target values in the simulation—luckily, they do.\n\n\nShow code (descriptive plotting)\ndat &lt;- sim_data(effect_shape = \"plateau\", mediated = TRUE)\n\nsds &lt;- dat |&gt; \n  group_by(id) |&gt; \n  summarise(mean_value = mean(sleep_quality, na.rm = TRUE), # Renamed from wellbeing\n            sd_within  = sd(sleep_quality, na.rm = TRUE)) |&gt; # Renamed from wellbeing\n  summarise(between_sd   = sd(mean_value, na.rm = TRUE),\n            avg_within_sd = mean(sd_within, na.rm = TRUE))\n\n# plot sleep_quality by group\ndat |&gt; \n  group_by(condition, day) |&gt; \n  summarise(sleep_quality = mean(sleep_quality)) |&gt; # Renamed from wellbeing\n  ggplot(aes(y = sleep_quality, x = day, color = condition)) + # Renamed from wellbeing\n  geom_line() + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3.2.1 Test Fit (h2a - intention to treat)\nWe fit various models to the newly simulated data to make sure each appears to be working properly, and also test the full sim_study pipeline.\n\n\nShow code (test fit h2a)\ndat &lt;- sim_data(mediated = FALSE)\n\nfit_mlm(dat) |&gt; summary()\nfit_mlm_simple(dat) |&gt; summary()\nfit_gls(dat) |&gt; summary()\nfit_gls_simple(dat) |&gt; summary()\nfit_gls_spline(dat) |&gt; extract_marginal_effect()\nfit_gam(dat) |&gt; summary()\nfit_gam_no_main(dat) |&gt; extract_marginal_effect()\n\nsim_study(model = \"fit_gam_no_main\")\nsim_study(model = \"fit_gls_spline\")\n\n\nSince some models (e.g., fit_gam_no_main) do not have a parameter that represents the average difference-in-difference between groups during the intervention period, we need to calculate this ourselves by marginalize across the 14-day intervention period.\n\n\nShow code (test emmeans)\nemm_day &lt;- emmeans(\n  fit_gls_spline(dat), \n  pairwise ~ condition | day, \n  at = list(day = 8:21), \n  condition = c(\"control\", \"intervention\"), \n  data = dat |&gt; mutate(condition = factor(condition, levels = c(\"intervention\", \"control\")))\n)\n\nsummary(emm_day$contrasts, infer = TRUE, level = .95, by = NULL, adjust = \"none\")\n\n# and then integrated over the 14 day intervention period\nrg &lt;- ref_grid(fit_gls_spline(dat),\n               at = list(intervention_period = 1),\n               cov.reduce = list(day = mean),\n               data = dat |&gt; mutate(condition = factor(condition, levels = c(\"control\",\"intervention\"))))\n\nemm &lt;- emmeans(rg, ~ condition)\n(contrast_result &lt;- contrast(emm, method = list(\"intervention - control\" = c(-1, 1)), adjust = \"none\"))\n\n\nmeans &lt;- summary(emm)$emmean\nnames(means) &lt;- summary(emm)$condition\n(diff_manual &lt;- means[\"intervention\"] - means[\"control\"])\n\n\n\n\n3.2.2 Test Fit (h2b - per-protocol)\nAnother quick test of our fit_mlm_reduction model, to make sure the alternative simulation whereby the effect of the intervention is mediated by a reduction in playtime is also functioning properly.\n\n\nShow code (test fit h2b)\ndat &lt;- sim_data(mediated = TRUE)\n\nfit_mlm_reduction(dat) |&gt; summary()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "scripts/sim_sleepquality_report.html#simulated-h2a-power-analysis",
    "href": "scripts/sim_sleepquality_report.html#simulated-h2a-power-analysis",
    "title": "3  Generate Synthetic Data",
    "section": "3.3 Simulated h2a power analysis",
    "text": "3.3 Simulated h2a power analysis\nTo assess power/sensitivity, we run multiple simulations (controlled by n_sims) and gather the parameter estimates for a particular term (e.g., conditionintervention:intervention_periodTRUE, or for our marginalized effect conditionintervention). Each iteration calls sim_study, which does the data generation, dropout, and fitting.\nAs this is quite slow, we both use parallel processing with furrr cache the results.\n\n\nShow code (simulate power h2a)\n# Load tictoc for timing if not already loaded\nif (!require(tictoc, quietly = TRUE)) {\n  install.packages(\"tictoc\")\n  library(tictoc)\n}\n\n# Start timing\ntic(\"H2a simulation total time\")\n\nn_sims &lt;- 500\n\nspecs_h2a &lt;- expand_grid(\n  model = c(\"fit_gam\", \"fit_gam_no_main\", \"fit_mlm\", \"fit_mlm_simple\", \"fit_gls\", \"fit_gls_simple\", \"fit_gls_spline\"),  # model names as strings\n  b = c(0.3, 0.6, 0.9, 1.2, 1.5),  # scaled for 5-25 range\n  effect_shape = c(\"grow\", \"plateau\")\n) |&gt; \n  mutate(\n    focal_term = case_when(\n      model %in% c(\"fit_mlm_simple\",\"fit_gls_simple\", \"fit_gam_no_main\") ~ \"conditionintervention\",\n      model %in% c(\"fit_gam\", \"fit_mlm\", \"fit_gls\", \"fit_gls_spline\") ~ \"conditionintervention:intervention_period\",\n      TRUE ~ \"conditionintervention:intervention_period\"\n    )\n  ) |&gt; \n  (function(d) { d$row_id &lt;- pmap_chr(d, ~ paste0(names(list(...)), \"=\", c(...), collapse = \"_\")); d })() |&gt; \n  mutate(i = row_number())\n\n# Create all spec-simulation combinations for better parallelization\nall_jobs_h2a &lt;- specs_h2a |&gt; \n  crossing(sim = 1:n_sims) |&gt; \n  mutate(job_id = row_number())\n\nmessage(\"Running \", nrow(all_jobs_h2a), \" simulations across \", nbrOfWorkers(), \" cores...\")\n\n# Run all simulations in parallel\nresults_h2a &lt;- future_map_dfr(1:nrow(all_jobs_h2a), function(job_idx) {\n  # Load required libraries in each worker\n  library(tidyverse)\n  library(lme4)\n  library(mgcv)\n  library(broom)\n  library(nlme)\n  library(emmeans)\n  library(splines)\n  library(broom.mixed)\n  library(extraDistr)\n  \n  # Get job parameters\n  job &lt;- all_jobs_h2a[job_idx, ]\n  \n  tryCatch({\n    result &lt;- sim_study(\n      model = job$model,\n      focal_term = job$focal_term,\n      n = 100,\n      n_days = 28,\n      # effect parameters\n      b = job$b,\n      mu = 15,\n      effect_shape = job$effect_shape,\n      k = .5,\n      # random effects parameters\n      tau_int = 2.5,\n      tau_slope = .8,\n      within_person_sd = 2.5,\n      # AR(1) parameters\n      phi = 0.7\n    ) |&gt; \n      mutate(\n        sim = job$sim,\n        row_id = job$row_id,\n        model = job$model,\n        b = job$b,\n        effect_shape = job$effect_shape\n      )\n    \n    return(result)\n  }, error = function(e) {\n    message(\"Job \", job_idx, \" (spec row: \", job$i, \", sim: \", job$sim, \") failed: \", e$message)\n    tibble(\n      term = NA_character_,\n      estimate = NA_real_,\n      std.error = NA_real_,\n      conf.low = NA_real_,\n      conf.high = NA_real_,\n      sim = job$sim,\n      row_id = job$row_id,\n      model = job$model,\n      b = job$b,\n      effect_shape = job$effect_shape\n    )\n  })\n}, .progress = TRUE,\n.options = furrr_options(\n  globals = c(\"all_jobs_h2a\", \"sim_study\", \"sim_data\", \n              \"fit_gam\", \"fit_gam_no_main\",\n              \"fit_mlm\", \"fit_mlm_simple\",\n              \"fit_gls\", \"fit_gls_simple\",\n              \"fit_gls_spline\", \n              \"extract_marginal_effect\"),\n  seed = TRUE\n))\n\nsim_summary_h2a &lt;- results_h2a |&gt; \n  group_by(row_id) |&gt; \n  summarise(\n    model = first(model),\n    b = first(b),\n    effect_shape = first(effect_shape),\n    mean_effect = mean(estimate, na.rm = TRUE),\n    mean_se = mean(std.error, na.rm = TRUE),\n    mean_conf.low = mean(conf.low, na.rm = TRUE),\n    mean_conf.high = mean(conf.high, na.rm = TRUE),\n    power = sum(conf.low &gt; 0, na.rm = TRUE) / sum(!is.na(conf.low))\n  )\n\n# Stop timing and display results\ntoc()\n\n\nH2a simulation total time: 8105.799 sec elapsed\n\n\nShow code (simulate power h2a)\n# Also print some summary statistics about the simulation\nmessage(\"=== SIMULATION SUMMARY ===\")\nmessage(\"Total number of jobs: \", nrow(all_jobs_h2a))\nmessage(\"Number of successful results: \", sum(!is.na(results_h2a$estimate)))\nmessage(\"Number of failed jobs: \", sum(is.na(results_h2a$estimate)))\nmessage(\"Success rate: \", round(100 * sum(!is.na(results_h2a$estimate)) / nrow(results_h2a), 1), \"%\")\nmessage(\"============================\")\n\n\n\n\nShow code (visualize power h2a)\n# Estimated effect vs. true effect (b)\nggplot(sim_summary_h2a, aes(x = b, y = mean_effect, color = model, alpha = model == \"fit_gam\")) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = mean_conf.low, ymax = mean_conf.high), width = 0.1) +\n  facet_wrap(~ effect_shape) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray\") +\n  scale_alpha_manual(values = c(\"TRUE\" = 1.0, \"FALSE\" = 0.2), guide = \"none\") +\n  labs(x = \"True Effect (unstandardized b)\", y = \"Estimated Effect\",\n       title = \"Estimated vs. True Effects by Model and Effect Shape\") +\n  scale_x_continuous(breaks = c(0.3, 0.6, 0.9, 1.2, 1.5),\n                     sec.axis = sec_axis(~ . / 3, name = \"Standardized Effect (b/3)\"))\n\n\n\n\n\n\n\n\n\nShow code (visualize power h2a)\n# ggsave(\"figures/estimated_vs_true_effects.png\", width = 10, height = 6, dpi = 300)\n\n# Power vs. true effect (b)\nggplot(sim_summary_h2a, aes(x = b, y = power, color = model, alpha = model == \"fit_gam\")) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  facet_wrap(~ effect_shape) +\n  scale_alpha_manual(values = c(\"TRUE\" = 1.0, \"FALSE\" = 0.2), guide = \"none\") +\n  labs(x = \"True Effect (unstandardized b)\", y = \"Power\",\n       title = \"Power by Model and Effect Shape\") +\n  scale_x_continuous(breaks = c(0.3, 0.6, 0.9, 1.2, 1.5),\n                     sec.axis = sec_axis(~ . / 3, name = \"Standardized Effect (b/3)\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, 1), breaks = seq(0, 1, .1))\n\n\n\n\n\n\n\n\n\nShow code (visualize power h2a)\n# ggsave(\"figures/power_by_model_and_effect_shape.png\", width = 10, height = 6, dpi = 300)\n\n# results |&gt;\n#   forestplot(mean = estimate,\n#              lower = conf.low,\n#              upper = conf.high,\n#              labeltext = term)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "scripts/sim_sleepquality_report.html#simulated-h2b-power-analysis",
    "href": "scripts/sim_sleepquality_report.html#simulated-h2b-power-analysis",
    "title": "3  Generate Synthetic Data",
    "section": "3.4 Simulated h2b power analysis",
    "text": "3.4 Simulated h2b power analysis\nSame thing as above, but now looking at power for our per-protocol model.\n\n\nShow code (sim study h2b)\n# Start timing\ntic(\"H2b simulation total time\")\n\nn_sims &lt;- 500\n\nspecs_h2b &lt;- expand_grid(\n  model = c(\"fit_mlm_reduction\"),  # model names as strings\n  b = c(0.5, 1.0, 1.5, 2.0, 2.5),  # these seem reasonable for the mediated model\n  effect_shape = c(\"grow\", \"plateau\")\n) |&gt; \n  # calculate the mean of the Kumaraswamy distribution - the expected effect size of the mediated version is b * average compliance\n  mutate(\n    focal_term = case_when(\n      model == \"fit_mlm_reduction\" ~ \"intervention_activeTRUE:reduction\"\n    ),\n    expected_effect = b * .1*beta(1 + 1/.05, .1),\n  ) |&gt; \n  (\\(d) { d$row_id &lt;- pmap_chr(d, ~ paste0(names(list(...)), \"=\", c(...), collapse = \"_\")); d })() |&gt; \n  mutate(i = row_number())\n\n# Create all spec-simulation combinations for better parallelization\nall_jobs_h2b &lt;- specs_h2b |&gt; \n  crossing(sim = 1:n_sims) |&gt; \n  mutate(job_id = row_number())\n\nmessage(\"Running \", nrow(all_jobs_h2b), \" simulations across \", nbrOfWorkers(), \" cores...\")\n\n# Run all simulations in parallel\nresults_h2b &lt;- future_map_dfr(1:nrow(all_jobs_h2b), function(job_idx) {\n  # Load required libraries in each worker\n  library(tidyverse)\n  library(nlme)\n  library(broom.mixed)\n  library(extraDistr)\n  library(rms)\n  \n  # Get job parameters\n  job &lt;- all_jobs_h2b[job_idx, ]\n  \n  tryCatch({\n    result &lt;- sim_study(\n      model = job$model,\n      focal_term = job$focal_term,\n      n = 100,\n      n_days = 28,\n      # effect parameters\n      b = job$b,\n      mu = 15,\n      effect_shape = job$effect_shape,\n      k = .5,\n      # random effects parameters\n      tau_int = 2.5,\n      tau_slope = .8,\n      within_person_sd = 2.0,\n      # AR(1) parameters\n      phi = 0.7,\n      mediated = TRUE\n    ) |&gt; \n      mutate(\n        sim = job$sim,\n        row_id = job$row_id,\n        model = job$model,\n        b = job$b,\n        expected_effect = job$expected_effect,\n        effect_shape = job$effect_shape\n      )\n    \n    return(result)\n  }, error = function(e) {\n    message(\"Job \", job_idx, \" (spec row: \", job$i, \", sim: \", job$sim, \") failed: \", e$message)\n    tibble(\n      term = NA_character_,\n      estimate = NA_real_,\n      std.error = NA_real_,\n      conf.low = NA_real_,\n      conf.high = NA_real_,\n      sim = job$sim,\n      row_id = job$row_id,\n      model = job$model,\n      b = job$b,\n      expected_effect = job$expected_effect,\n      effect_shape = job$effect_shape\n    )\n  })\n}, .progress = TRUE,\n.options = furrr_options(\n  globals = c(\"all_jobs_h2b\", \"sim_study\", \"sim_data\", \"fit_mlm_reduction\"),\n  seed = TRUE\n))\n\nsim_summary_h2b &lt;- results_h2b |&gt; \n  group_by(row_id) |&gt; \n  summarise(\n    model = first(model),\n    b = first(b),\n    expected_effect = first(expected_effect),\n    effect_shape = first(effect_shape),\n    mean_effect = mean(estimate, na.rm = TRUE),\n    mean_se = mean(std.error, na.rm = TRUE),\n    mean_conf.low = mean(conf.low, na.rm = TRUE),\n    mean_conf.high = mean(conf.high, na.rm = TRUE),\n    power = sum(conf.low &gt; 0, na.rm = TRUE) / sum(!is.na(conf.low))\n  )\n\n# Stop timing and display results\ntoc()\n\n\nH2b simulation total time: 2027.535 sec elapsed\n\n\nShow code (sim study h2b)\n# Also print some summary statistics about the simulation\nmessage(\"=== H2B SIMULATION SUMMARY ===\")\nmessage(\"Total number of jobs: \", nrow(all_jobs_h2b))\nmessage(\"Number of successful results: \", sum(!is.na(results_h2b$estimate)))\nmessage(\"Number of failed jobs: \", sum(is.na(results_h2b$estimate)))\nmessage(\"Success rate: \", round(100 * sum(!is.na(results_h2b$estimate)) / nrow(results_h2b), 1), \"%\")\nmessage(\"===============================\")\n\n\n\n\nShow code (visualize power h2b)\n# Estimated effect vs. true effect (b)\nggplot(sim_summary_h2b, aes(x = expected_effect, y = mean_effect, color = model)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = mean_conf.low, ymax = mean_conf.high), width = 0.1) +\n  facet_wrap(~ effect_shape) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray\") +\n  labs(x = \"True Effect (expected effect)\", y = \"Estimated Effect\",\n       title = \"Estimated vs. True Effects by Model and Effect Shape\") +\n  scale_x_continuous(breaks = c(0.05, 0.1, 0.15, 0.2, 0.25),\n                     sec.axis = sec_axis(~ . / 3, name = \"Standardized Effect\"))\n\n\n\n\n\n\n\n\n\nShow code (visualize power h2b)\n# ggsave(\"figures/estimated_vs_true_effects_h2b.png\", width = 10, height = 6, dpi = 300)\n\n# Power vs. true effect (b)\nggplot(sim_summary_h2b, aes(x = expected_effect, y = power, color = model)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  facet_wrap(~ effect_shape) +\n  labs(x = \"True Effect (expected effect)\", y = \"Power\",\n       title = \"Power by Model and Effect Shape\") +\n  scale_x_continuous(breaks = c(0.05, 0.1, 0.15, 0.2, 0.25),\n                     sec.axis = sec_axis(~ . / 3, name = \"Standardized Effect\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, 1), breaks = seq(0, 1, .1))\n\n\n\n\n\n\n\n\n\nShow code (visualize power h2b)\n# ggsave(\"figures/power_by_model_and_effect_shape_h2b.png\", width = 10, height = 6, dpi = 300)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "scripts/sim_sleepquality_report.html#planned-sensitivity-analyses",
    "href": "scripts/sim_sleepquality_report.html#planned-sensitivity-analyses",
    "title": "3  Generate Synthetic Data",
    "section": "3.5 Planned Sensitivity Analyses",
    "text": "3.5 Planned Sensitivity Analyses\nWe have preregistered several sensitivity analyses to test the robustness of any effects we find. These include:\n\nDay 21 only: We will estimate the effect of the intervention on day 21 only, to see what the difference between groups is at the end of the intervention period\nMarginal means: In h2a, we will use the emmeans package to calculate marginal means for each condition and produce a single parameter by integrating across the 14-day period\nMultilevel model: We will fit the MLM as defined in Table 1 above, as the second-highest performing model in the simulations (having higher power for linear effects, but lower for non-linear)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "scripts/sim_self_report.html",
    "href": "scripts/sim_self_report.html",
    "title": "2  Sleep Quality Analysis",
    "section": "",
    "text": "2.1 Introduction\nThis document demonstrates a complete workflow for generating synthetic data, introducing dropout, and fitting models (a GAM or MLM) to the data. The main focus is on the sim_study function, which orchestrates the data simulation, dropout process, and model fitting.\nAs an overview, we compare the following possible models for estimating the effect of gaming reduction, for both H3a (the intention-to-treat effect) and H3b (the per-protocol effect; i.e., the effect of actual gaming reduction relative to one’s own baseline). The models we compare are:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sleep Quality Analysis</span>"
    ]
  },
  {
    "objectID": "scripts/sim_self_report.html#introduction",
    "href": "scripts/sim_self_report.html#introduction",
    "title": "2  Sleep Quality Analysis",
    "section": "",
    "text": "ITT = Intention-to-treat; PP = per-protocol\n\n\n\n\n\n\n\n\nModel Name\nSyntax\nTarget Effect\nNotes\n\n\n\n\nGAM\ngam(wellbeing ~ condition:intervention_period + age + gender + s(id, bs = \"re\") + s(day, by = condition, bs = \"tp\"), correlation = corAR1(form = ~ day | id))\nITT\n\n\n\nGAM with no main effect)\ngam(wellbeing ~         age + gender +         s(id, bs = \"re\") +         s(day, by = condition, bs = \"tp\"),       data = dat,       correlation = corCAR1(form = ~ day | id))\nITT\nHere we do not estimate a parameter for the effect of the intervention directly; rather, we simply fit separate curves to each condition and calculate the average marginal effect using {emmeans}\n\n\nMLM\nlme(     fixed = wellbeing ~ condition*intervention_period + age + gender,     random = ~ 1|id,     correlation = corCAR1(form = ~ day | id),     method = \"ML\"   )\nITT\nMultiple versions of this model failed when including random slopes; we therefore dropped these\n\n\nMLM Simple\nlme(     fixed = wellbeing ~ baseline + condition + age + gender,     random = ~ 1 + condition|id,     correlation = corCAR1(form = ~ day | id),     method = \"ML\",   )\nITT\nHere we do not model the baseline (pre-intervention period) itself—we model only the 14-day period when the intervention is active, using average wellbeing during baseline as a covariate\n\n\nGLS (generalized least squares)\ngls(     wellbeing ~ condition * intervention_period + age + gender,     correlation = corCAR1(form = ~ day | id),   )\nITT\n\n\n\nGLS Simple\ngls(     wellbeing ~ condition + baseline + age + gender,     correlation = corAR1(form = ~ day | id),   )\nITT\n\n\n\nGLS Splines\ngls(     wellbeing ~ ns(day, df = 4) * intervention_period * condition,,     correlation = corCAR1(form = ~ day | id),     data = dat   )\nITT\nIn this version, we fit a GLS but allow non-linearity in the trajectory of wellbeing using splines\n\n\nMLM Reduction\nlme(     fixed = wellbeing ~ intervention_active*reduction + age + gender,     random = ~ 1 + intervention_active*reduction | id,     correlation = corCAR1(form = ~ day | id)   )\nPP\nHere we test our intended model for the per-protocol effect; reduction is the number of hours played relative to that person’s mean playtime at baseline\n\n\n\n\n2.1.1 Take-aways\nOur simulations show that several models perform well at parameter recovery for the ITT effect, but that the GAM model has the highest power for small effects—the type of effects we believe we are most likely to observed—and for non-linear trajectories over the 14 day period (e.g., an effect that slowly accumulates over a couple of days and then plateaus, or a temporary withdrawal followed by a later improvement). The GAM has approximately 50% power for a standardized effect of .2, and 80% power for a standardized effect of .3, but this varies based the shape of that effect over time.\nThe MLM Reduction model performs very well, and has &gt;95% power for standardized effects of approximately .2 or greater.\n\n\n2.1.2 Load Libraries\nFirst we load packages with pacman, which is fully compatible with renv.\n\n\nShow code (load libraries)\nlibrary(pacman)\n\np_load(tidyverse, qualtRics, lme4, mgcv, marginaleffects, broom, forestplot, broom.mixed, nlme, rms, emmeans, splines, furrr, extraDistr)\n\nplan(multisession, workers = parallel::detectCores()-1)\n\ntheme_set(theme_minimal())\ntheme_update(\n  strip.background = element_rect(fill = \"black\"),\n  strip.text = element_text(color = \"white\", size = 10),\n  panel.grid.minor = element_blank(),\n  panel.border = element_rect(colour = \"black\", fill = NA, linewidth = 1),\n)\n\noptions(scipen = 999)\n\n\n\n\n2.1.3 Simulation, Dropout, and Fitting Functions\nHere we define:\n\nsim_data: Generates synthetic data with random intercepts/slopes and AR(1) errors.\nsim_dropout: Introduces missingness and dropout in the dataset.\nfit_*: Fits a statistical model to the simulated data (see table above)\nsim_study: Ties everything together—generates data, applies dropout, then fits the chosen model and returns a tidy summary.\n\n\n\nShow code (sim functions)\n#' Generate Synthetic Data\n#'\n#' This function simulates synthetic panel data for `n` participants over `n_days` time points,\n#' with an intervention effect, random intercepts and slopes, and AR(1)-correlated residuals.\n#'\n#' @param n Number of participants. Default is 100.\n#' @param n_days Number of time points per participant.\n#' @param b Fixed effect of condition (if mediated == FALSE) or a 1-hour reduction in playtime.\n#' @param phi AR(1) autocorrelation coefficient.\n#' @param sigma AR(1) residual standard deviation.\nsim_data &lt;- function(n = 100,\n                     n_days = 28,\n                     \n                     # effect parameters\n                     b = 3.7, # effect in unstandardized units\n                     mu = 78.5, # grand mean of the outcome\n                     \n                     \n                     # random effects parameters\n                     tau_int = 9.7,   # Random intercept SD (between-person variance)\n                     tau_slope = .05,  # Random slope SD \n                     within_person_sd = 11.8, # Within-person SD\n                     \n                     # AR(1) parameters\n                     phi = 0.8,     # Autocorrelation coefficient\n                     effect_shape = \"grow\",\n                     k = .5, # affects how quickly the plateau effect plateaus\n                     \n                     mediated = FALSE,\n                     \n                     # playtime parameters\n                     playtime_grand_mean = 1,   # Average baseline playtime in hours\n                     playtime_grand_sd = .5,   # SD for baseline playtime in log units (log-normal distribution)\n                     daily_play_sd = 0.5      # Daily noise in playtime\n                     # compliance_mean = 0.7,    # Average reduction (in hours) for intervention group during intervention period\n)     \n{\n  dat &lt;- tibble(\n    id = 1:n,\n    age = sample(18:36, n, replace = TRUE),\n    gender = sample(c(\"man\",\"woman\",\"non-binary\"), n, prob = c(.45, .45, .1), replace = TRUE),\n    condition = factor(sample(c(\"control\", \"intervention\"), n, replace = TRUE)),\n    experimental_condition = ifelse(condition == \"intervention\", 1, 0),\n    intercept_wb = rnorm(n, 0, tau_int),\n    slope_wb = rnorm(n, 0, tau_slope),\n    intercept_play = rlnorm(n, log(playtime_grand_mean), playtime_grand_sd),\n  ) |&gt; \n    # expand to 28 waves per id\n    crossing(\n      day = 1:n_days\n    ) |&gt; \n    mutate(\n      intervention_period = as.numeric(day &gt; 7 & day &lt; 22),\n      intervention_active = intervention_period & condition == \"intervention\",\n      compliance = ifelse(intervention_active, rkumar(n*n_days, a = .05, b = .1), 0),\n      \n      # In the baseline period, play is just the subject’s baseline plus some day-to-day noise\n      # During the intervention, experimental subjects reduce play by their compliance amount\n      playtime = (1 - compliance) * rlnorm(n, log(intercept_play), daily_play_sd),\n      effect_time = case_when(\n        effect_shape == \"plateau\" ~ if_else(intervention_period == 1, (b + slope_wb) * (1-exp(-k * (day - 7))), 0),\n        effect_shape == \"grow\" ~ if_else(intervention_period == 1, (day - 7) * ((b + slope_wb)/7), 0),\n        TRUE ~ NA_real_\n      ),\n    ) |&gt; \n    group_by(id) |&gt; \n    mutate(\n      \n      baseline_playtime = mean(playtime[day &lt;= 7]),\n      reduction = baseline_playtime - playtime, # The mediator: reduction in play relative to the baseline average\n      sigma = within_person_sd * sqrt(1-phi^2),\n      # Generate AR(1) errors for each participant\n      e = as.numeric(arima.sim(n = n_days, \n                               model = list(ar = phi), \n                               sd = sigma)),\n      # Add random effect + fixed effect + AR(1) error\n      wellbeing = case_when(\n        mediated == TRUE ~ mu + \n                            intercept_wb + \n                            effect_time * reduction + \n                            .01*(age-18) +\n                            -.05*gender %in% c(\"women\",\"non-binary\") +\n                            e,\n        mediated == FALSE ~ mu + \n                            intercept_wb + \n                            effect_time * experimental_condition * intervention_period + \n                            .01*(age-18) +\n                            -.05*gender %in% c(\"women\",\"non-binary\") +\n                            e\n      )\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(across(where(is.numeric), ~ round(., 3)))\n  \n  dat\n\n}\n\n\n#' Simulate Dropout\n#'\n#' Introduces missingness and dropout into a dataset by randomly assigning records as missing\n#' or dropped out. Once a participant is dropped out, all subsequent records become missing.\n#'\n#' @param dat A tibble generated by \\code{sim_data()}.\n#'\n#' @return A tibble of the same structure as \\code{dat}, but with some \\code{wellbeing} values set to NA.\n#'\nsim_dropout &lt;- function(dat) {\n  \n  dropout &lt;- dat |&gt; \n    mutate(\n      missing = sample(c(TRUE, FALSE), n(), replace = TRUE, prob = c(.10, .90)),\n      dropout = sample(c(TRUE, FALSE), n(), replace = TRUE, prob = c(.01, .99))\n    ) |&gt;\n    mutate(\n      missing = ifelse(cumsum(dropout) &gt; 0, TRUE, missing),\n      .by = id\n    ) |&gt;\n    arrange(as.integer(id), day) |&gt; \n    mutate(wellbeing = ifelse(missing, NA, wellbeing))\n  dropout\n}\n\n#' Fit a Generalized Additive Model (GAM)\n#'\n#' Fits a GAM model to the provided dataset using \\code{mgcv::gam}, including an AR(1)\n#' correlation structure and random intercept for each ID.\n#'\n#' @param dat A tibble of repeated-measures data (e.g., from \\code{sim_data()} and \\code{sim_dropout()}).\n#'\n#' @return An object of class \\code{gam}, which is the fitted GAM model.\n#'\nfit_gam &lt;- function(dat) {\n  \n  gam(wellbeing ~ \n        condition:intervention_period + age + gender +\n        s(id, bs = \"re\") + \n        s(day, by = condition, bs = \"tp\"), \n      data = dat,\n      correlation = corAR1(form = ~ day | id))\n}\n\nfit_gam_no_main &lt;- function(dat) {\n  \n  gam(wellbeing ~ \n        age + gender +\n        s(id, bs = \"re\") + \n        s(day, by = condition, bs = \"tp\"), \n      data = dat,\n      correlation = corCAR1(form = ~ day | id))\n}\n\n#' Fit a Multi-Level Model (MLM)\n#'\n#' Fits a linear mixed-effects model (LME) with random intercept for each ID using \\code{lme4::lmer}.\n#'\n#' @param dat A tibble of repeated-measures data (e.g., from \\code{sim_data()} and \\code{sim_dropout()}).\n#'\n#' @return An object of class \\code{lmerMod}, which is the fitted MLM model.\n#'\nfit_mlm &lt;- function(dat) {\n  # lmer(wellbeing ~ condition*intervention_period + age + gender + (1|id), data = dat)\n  lme(\n    fixed = wellbeing ~ condition*intervention_period + age + gender,\n    random = ~ 1|id,  # or use a more flexible structure if needed\n    correlation = corCAR1(form = ~ day | id),\n    method = \"ML\",\n    data = dat |&gt; filter(!is.na(wellbeing))\n  )\n}\n\nfit_mlm_simple &lt;- function(dat) {\n  \n  tmp &lt;- dat |&gt; \n    group_by(id) |&gt; \n    # take the mean of days 1-7 \n    mutate(baseline = mean(wellbeing[day &lt; 8], na.rm = TRUE)) |&gt; \n    filter(intervention_period == 1) |&gt; \n    filter(!is.na(wellbeing))\n  \n  lme(\n    fixed = wellbeing ~ baseline + condition + age + gender,\n    random = ~ 1 + condition|id,\n    correlation = corCAR1(form = ~ day | id),\n    method = \"ML\",\n    data = tmp |&gt; filter(!is.na(wellbeing))\n  )\n}\n\nfit_gls &lt;- function(dat) {\n  \n  gls(\n    wellbeing ~ condition * intervention_period + age + gender, \n    correlation = corCAR1(form = ~ day | id),\n    data = dat |&gt; filter(!is.na(wellbeing))\n  )\n}\n\nfit_gls_simple &lt;- function(dat) {\n  \n  tmp &lt;- dat |&gt; \n    group_by(id) |&gt; \n    # take the mean of days 1-7 \n    mutate(baseline = mean(wellbeing[day &lt; 8], na.rm = TRUE)) |&gt; \n    filter(intervention_period == 1) |&gt; \n    filter(!is.na(wellbeing))\n  \n  gls(\n    wellbeing ~ condition + baseline + age + gender, \n    correlation = corAR1(form = ~ day | id),\n    data = tmp\n  )\n}\n\nfit_gls_spline &lt;- function(dat) {\n  gls(\n    wellbeing ~ ns(day, df = 4) * intervention_period * condition,,  \n    correlation = corCAR1(form = ~ day | id),\n    data = dat\n  )\n}\n\nfit_mlm_reduction &lt;- function(dat) {\n  lme(\n    fixed = wellbeing ~ intervention_active*reduction + age + gender, \n    random = ~ 1 + intervention_active*reduction | id,\n    correlation = corCAR1(form = ~ day | id),\n    data = dat\n  )\n}\n\n# Helper function to extract the focal effect for GLS models\nextract_marginal_effect &lt;- function(mod, dat, focal_term = \"conditionintervention\") {\n  # Here we assume your GLS model is specified with condition*intervention_period\n  # and you want the effect of condition (e.g., intervention vs. control) during intervention.\n  # We create a reference grid that fixes intervention_period at 1.\n  rg &lt;- ref_grid(mod, data = dat, at = list(intervention_period = 1))\n  \n  # Obtain estimated marginal means for each condition.\n  emm &lt;- emmeans(rg, ~ condition)\n  \n  # Compute the pairwise contrast (e.g., intervention - control)\n  # Adjust names as needed. The contrast below returns a one-row summary.\n  contr &lt;- emmeans::contrast(emm, method = list(\"intervention - control\" = c(-1, 1)), adjust = \"none\")\n  contr_sum &lt;- summary(contr, infer = TRUE)\n  \n  # Construct a one-row data frame with consistent column names.\n  # If you have more than one contrast, you might need to filter for the one of interest.\n  df &lt;- data.frame(\n    term = focal_term,\n    estimate = contr_sum$estimate,\n    std.error = contr_sum$SE,\n    conf.low = contr_sum$lower.CL,\n    conf.high = contr_sum$upper.CL,\n    row.names = NULL\n  )\n  \n  return(df)\n}\n\n#' Simulation Study Orchestrator\n#'\n#' A higher-level function that ties together data simulation, dropout, and model fitting,\n#' returning a tidy summary of the fitted model parameters.\n#'\n#' @param model_function A function to fit the model. Defaults to \\code{fit_gam}.\n#' @param n Number of participants passed to \\code{sim_data()}. Default is 1000.\n#' @param n_days Number of time points per participant passed to \\code{sim_data()}. Default is 28.\n#' @param b Fixed effect for the intervention slope passed to \\code{sim_data()}. Default is 0.01.\n#' @param phi AR(1) autocorrelation coefficient passed to \\code{sim_data()}. Default is 0.7.\n#' @param sigma AR(1) residual standard deviation passed to \\code{sim_data()}. Default is 0.6.\n#'\n#' @return A data frame (tibble) of model estimates from \\code{broom::tidy(parametric = TRUE)}.\n#'\n\n# Updated simulation orchestrator that handles GLS models separately.\nsim_study &lt;- function(model = \"fit_gam\", focal_term = \"intervention_activeTRUE:reduction\", ...) {\n  args &lt;- list(...)\n  dat &lt;- do.call(sim_data, args)\n  model_function &lt;- get(model)\n  \n  mod &lt;- model_function(dat)\n  \n  if (model %in% c(\"fit_gam_no_main\",\"fit_gls_spline\")) {\n    # Extract the effect using our helper function.\n    result &lt;- suppressMessages(extract_marginal_effect(mod, \n                                                       dat, \n                                                       focal_term = focal_term))\n  } else {\n    # For models that work with broom, extract the focal parameter.\n    # Adjust the filtering term as needed.\n    result &lt;- broom::tidy(mod, parametric = TRUE) %&gt;%\n      filter(term == focal_term) |&gt; \n      # filter(\n      #   term == \"conditionintervention:intervention_period\" | \n      #     (model %in% c(\"fit_mlm_simple\",\"fit_gls_simple\") & term == \"conditionintervention\")\n      # ) |&gt; \n      mutate(\n        conf.low = estimate - 1.96 * std.error,\n        conf.high = estimate + 1.96 * std.error\n      )\n  }\n  result\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sleep Quality Analysis</span>"
    ]
  },
  {
    "objectID": "scripts/sim_self_report.html#test-and-plot-one-simulated-study",
    "href": "scripts/sim_self_report.html#test-and-plot-one-simulated-study",
    "title": "2  Sleep Quality Analysis",
    "section": "2.2 Test and Plot One Simulated Study",
    "text": "2.2 Test and Plot One Simulated Study\nBelow, we create a sample dataset using sim_data() and examine it with a line plots by day. We can also see whether the simulated SDs for wellbeing align with the target values in the simulation—luckily, they do.\n\n\nShow code (descriptive plotting)\ndat &lt;- sim_data(effect_shape = \"plateau\", mediated = TRUE)\n\nsds &lt;- dat |&gt; \n  group_by(id) |&gt; \n  summarise(mean_value = mean(wellbeing, na.rm = TRUE),\n            sd_within  = sd(wellbeing, na.rm = TRUE)) |&gt;\n  summarise(between_sd   = sd(mean_value, na.rm = TRUE),\n            avg_within_sd = mean(sd_within, na.rm = TRUE))\n\n# plot wellbeing by group\ndat |&gt; \n  group_by(condition, day) |&gt; \n  summarise(wellbeing = mean(wellbeing)) |&gt; \n  ggplot(aes(y = wellbeing, x = day, color = condition)) +\n  geom_line() + \n  theme_minimal() +\n  scale_y_continuous(limits = c(60, 100))\n\n\n\n\n\n\n\n\n\n\n2.2.1 Test Fit (H3a - intention to treat)\nWe fit various models to the newly simulated data to make sure each appears to be working properly, and also test the full sim_study pipeline.\n\n\nShow code (test fit h3a)\ndat &lt;- sim_data(mediated = FALSE)\n\nfit_mlm(dat) |&gt; summary()\nfit_mlm_simple(dat) |&gt; summary()\nfit_gls(dat) |&gt; summary()\nfit_gls_simple(dat) |&gt; summary()\nfit_gls_spline(dat) |&gt; extract_marginal_effect()\nfit_gam(dat) |&gt; summary()\nfit_gam_no_main(dat) |&gt; extract_marginal_effect()\n\nsim_study(model = \"fit_gam_no_main\")\nsim_study(model = \"fit_gls_spline\")\n\n\nSince some models (e.g., fit_gam_no_main) do not have a parameter that represents the average difference-in-difference between groups during the intervention period, we need to calculate this ourselves by marginalize across the 14-day intervention period.\n\n\nShow code (test emmeans)\nemm_day &lt;- emmeans(\n  fit_gls_spline(dat), \n  pairwise ~ condition | day, \n  at = list(day = 8:21), \n  condition = c(\"control\", \"intervention\"), \n  data = dat |&gt; mutate(condition = factor(condition, levels = c(\"intervention\", \"control\")))\n)\n\nsummary(emm_day$contrasts, infer = TRUE, level = .95, by = NULL, adjust = \"none\")\n\n# and then integrated over the 14 day intervention period\nrg &lt;- ref_grid(fit_gls_spline(dat),\n               at = list(intervention_period = 1),\n               cov.reduce = list(day = mean),\n               data = dat |&gt; mutate(condition = factor(condition, levels = c(\"control\",\"intervention\"))))\n\nemm &lt;- emmeans(rg, ~ condition)\n(contrast_result &lt;- contrast(emm, method = list(\"intervention - control\" = c(-1, 1)), adjust = \"none\"))\n\n\nmeans &lt;- summary(emm)$emmean\nnames(means) &lt;- summary(emm)$condition\n(diff_manual &lt;- means[\"intervention\"] - means[\"control\"])\n\n\n\n\n2.2.2 Test Fit (H3b - per-protocol)\nAnother quick test of our fit_mlm_reduction model, to make sure the alternative simulation whereby the effect of the intervention is mediated by a reduction in playtime is also functioning properly.\n\n\nShow code (test fit h3b)\ndat &lt;- sim_data(mediated = TRUE)\n\nfit_mlm_reduction(dat) |&gt; summary()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sleep Quality Analysis</span>"
    ]
  },
  {
    "objectID": "scripts/sim_self_report.html#simulated-h3a-power-analysis",
    "href": "scripts/sim_self_report.html#simulated-h3a-power-analysis",
    "title": "2  Sleep Quality Analysis",
    "section": "2.3 Simulated H3a power analysis",
    "text": "2.3 Simulated H3a power analysis\nTo assess power/sensitivity, we run multiple simulations (controlled by n_sims) and gather the parameter estimates for a particular term (e.g., conditionintervention:intervention_periodTRUE, or for our marginalized effect conditionintervention). Each iteration calls sim_study, which does the data generation, dropout, and fitting.\nAs this is quite slow, we both use parallel processing with furrr cache the results.\n\n\nShow code (simulate power h3a)\nn_sims &lt;- 500\n\nspecs_h3a &lt;- expand_grid(\n  model = c(\"fit_gam\", \"fit_gam_no_main\", \"fit_mlm\", \"fit_mlm_simple\", \"fit_gls\", \"fit_gls_simple\", \"fit_gls_spline\"),  # model names as strings\n  b = c(1.2, 2.4, 3.6, 4.8, 6),\n  effect_shape = c(\"grow\", \"plateau\")\n) |&gt; \n  mutate(\n    focal_term = case_when(\n      model %in% c(\"fit_mlm_simple\",\"fit_gls_simple\", \"fit_gam_no_main\") ~ \"conditionintervention\",\n      model %in% c(\"fit_gam\", \"fit_mlm\", \"fit_gls\", \"fit_gls_spline\") ~ \"conditionintervention:intervention_period\",\n      TRUE ~ \"conditionintervention:intervention_period\"\n    )\n  ) |&gt; \n  (\\(d) { d$row_id &lt;- pmap_chr(d, ~ paste0(names(list(...)), \"=\", c(...), collapse = \"_\")); d })() |&gt; \n  mutate(i = row_number())\n\nresults_h3a &lt;- specs_h3a |&gt; \n  rowwise() |&gt; \n  mutate(sim_results = list({\n    message(\"Processing spec row: \", i)\n    future_map_dfr(1:n_sims, function(sim) {\n      library(tidyverse)\n      library(lme4)\n      library(mgcv)\n      library(broom)\n      library(nlme)\n      library(emmeans)\n      library(splines)\n      library(broom.mixed)\n      library(extraDistr)\n\n      tryCatch({\n        sim_study(\n          model = model,  # retrieve the function from its name\n          focal_term = focal_term,\n          n = 100,\n          n_days = 28,\n          # effect parameters\n          b = b, # effect in unstandardized units\n          mu = 78.5, # grand mean of the outcome\n          effect_shape = effect_shape,\n          k = .5,\n          # random effects parameters\n          tau_int = 9.7,   # Random intercept SD (between-person variance)\n          tau_slope = .8,  # Random slope SD \n          within_person_sd = 11.8, # Within-person SD\n          # AR(1) parameters\n          phi = 0.7     # Autocorrelation coefficient\n        ) |&gt; \n          mutate(sim = sim)\n      }, error = function(e) {\n        message(\"Simulation \", sim, \" failed: \", e$message)\n        tibble(\n          term = NA_character_,\n          estimate = NA_real_,\n          std.error = NA_real_,\n          conf.low = NA_real_,\n          conf.high = NA_real_,\n          sim = sim\n        )\n      })\n    }, \n    .progress = TRUE,\n    .options = furrr_options(globals = c(\"specs\",\"sim_study\", \"sim_data\", \n                                         \"fit_gam\", \"fit_gam_no_main\",\n                                         \"fit_mlm\", \"fit_mlm_simple\",\n                                         \"fit_gls\", \"fit_gls_simple\",\n                                         \"fit_gls_spline\", \n                                         \"extract_marginal_effect\"),\n                             seed = TRUE)\n    )\n  })) |&gt; \n  ungroup() |&gt; \n  unnest(sim_results)\n\nsim_summary_h3a &lt;- results_h3a |&gt; \n  group_by(row_id) |&gt; \n  summarise(\n    model = first(model),\n    b = first(b),\n    effect_shape = first(effect_shape),\n    mean_effect = mean(estimate, na.rm = TRUE),\n    mean_se = mean(std.error, na.rm = TRUE),\n    mean_conf.low = mean(conf.low, na.rm = TRUE),\n    mean_conf.high = mean(conf.high, na.rm = TRUE),\n    power = sum(conf.low &gt; 0, na.rm = TRUE) / sum(!is.na(conf.low))\n  )\n\n\n\n\nShow code (visualize power h3a)\n# Estimated effect vs. true effect (b)\nggplot(sim_summary_h3a, aes(x = b, y = mean_effect, color = model)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = mean_conf.low, ymax = mean_conf.high), width = 0.1) +\n  facet_wrap(~ effect_shape) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray\") +\n  labs(x = \"True Effect (unstandardized b)\", y = \"Estimated Effect\",\n       title = \"Estimated vs. True Effects by Model and Effect Shape\") +\n  scale_x_continuous(breaks = c(1.2, 2.4, 3.6, 4.8, 6),\n                     sec.axis = sec_axis(~ . / 12, name = \"Standardized Effect (b/12)\"))\n\n\n\n\n\n\n\n\n\nShow code (visualize power h3a)\n# Power vs. true effect (b)\nggplot(sim_summary_h3a, aes(x = b, y = power, color = model)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  facet_wrap(~ effect_shape) +\n  labs(x = \"True Effect (unstandardized b)\", y = \"Power\",\n       title = \"Power by Model and Effect Shape\") +\n  scale_x_continuous(breaks = c(1.2, 2.4, 3.6, 4.8, 6),\n                     sec.axis = sec_axis(~ . / 12, name = \"Standardized Effect (b/12)\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, 1), breaks = seq(0, 1, .1))\n\n\n\n\n\n\n\n\n\nShow code (visualize power h3a)\n# results |&gt;\n#   forestplot(mean = estimate,\n#              lower = conf.low,\n#              upper = conf.high,\n#              labeltext = term)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sleep Quality Analysis</span>"
    ]
  },
  {
    "objectID": "scripts/sim_self_report.html#simulated-h3b-power-analysis",
    "href": "scripts/sim_self_report.html#simulated-h3b-power-analysis",
    "title": "2  Sleep Quality Analysis",
    "section": "2.4 Simulated H3b power analysis",
    "text": "2.4 Simulated H3b power analysis\nSame thing as above, but now looking at power for our per-protocol model.\n\n\nShow code (sim study h3b)\nn_sims &lt;- 500\n\nspecs_h3b &lt;- expand_grid(\n  model = c(\"fit_mlm_reduction\"),  # model names as strings\n  b = c(1.2, 2.4, 3.6, 4.8),\n  effect_shape = c(\"grow\", \"plateau\")\n) |&gt; \n  # calculate the mean of the Kumaraswamy distribution - the expected effect size of the mediated version is b * average compliance\n  mutate(\n    focal_term = case_when(\n      model == \"fit_mlm_reduction\" ~ \"intervention_activeTRUE:reduction\"\n    ),\n    expected_effect = b * .1*beta(1 + 1/.05, .1),\n  ) |&gt; \n  (\\(d) { d$row_id &lt;- pmap_chr(d, ~ paste0(names(list(...)), \"=\", c(...), collapse = \"_\")); d })() |&gt; \n  mutate(i = row_number())\n\nresults_h3b &lt;- specs_h3b |&gt; \n  rowwise() |&gt; \n  mutate(sim_results = list({\n    message(\"Processing spec row: \", i)\n    future_map_dfr(1:n_sims, function(sim) {\n      library(tidyverse)\n      library(nlme)\n      library(broom.mixed)\n      library(extraDistr)\n      library(rms)\n\n      tryCatch({\n        sim_study(\n          model = model,  # retrieve the function from its name\n          focal_term = focal_term,\n          n = 100,\n          n_days = 28,\n          # effect parameters\n          b = b, # effect in unstandardized units\n          mu = 78.5, # grand mean of the outcome\n          effect_shape = effect_shape,\n          k = .5,\n          # random effects parameters\n          tau_int = 9.7,   # Random intercept SD (between-person variance)\n          tau_slope = .8,  # Random slope SD \n          within_person_sd = 11.8, # Within-person SD\n          # AR(1) parameters\n          phi = 0.7,     # Autocorrelation coefficient\n          mediated = TRUE\n        ) |&gt; \n          mutate(sim = sim)\n      }, error = function(e) {\n        message(\"Simulation \", sim, \" failed: \", e$message)\n        tibble(\n          term = NA_character_,\n          estimate = NA_real_,\n          std.error = NA_real_,\n          conf.low = NA_real_,\n          conf.high = NA_real_,\n          sim = sim\n        )\n      })\n    }, \n    .progress = TRUE,\n    .options = furrr_options(globals = c(\"specs\",\"sim_study\", \"sim_data\", \n                                         \"fit_mlm_reduction\"),\n                             seed = TRUE)\n    )\n  })) |&gt; \n  ungroup() |&gt; \n  unnest(sim_results)\n\nsim_summary_h3b &lt;- results_h3b |&gt; \n  group_by(row_id) |&gt; \n  summarise(\n    model = first(model),\n    b = first(b),\n    expected_effect = first(expected_effect),\n    effect_shape = first(effect_shape),\n    mean_effect = mean(estimate, na.rm = TRUE),\n    mean_se = mean(std.error, na.rm = TRUE),\n    mean_conf.low = mean(conf.low, na.rm = TRUE),\n    mean_conf.high = mean(conf.high, na.rm = TRUE),\n    power = sum(conf.low &gt; 0, na.rm = TRUE) / sum(!is.na(conf.low))\n  )\n\n\n\n\nShow code (visualize power h3b)\n# Estimated effect vs. true effect (b)\nggplot(sim_summary_h3b, aes(x = expected_effect, y = mean_effect, color = model)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = mean_conf.low, ymax = mean_conf.high), width = 0.1) +\n  facet_wrap(~ effect_shape) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray\") +\n  labs(x = \"True Effect (unstandardized b)\", y = \"Estimated Effect\",\n       title = \"Estimated vs. True Effects by Model and Effect Shape\") +\n  scale_x_continuous(breaks = c(1.2, 2.4, 3.6, 4.8, 6),\n                     sec.axis = sec_axis(~ . / 12, name = \"Standardized Effect (b/12)\"))\n\n\n\n\n\n\n\n\n\nShow code (visualize power h3b)\n# Power vs. true effect (b)\nggplot(sim_summary_h3b, aes(x = expected_effect, y = power, color = model)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  facet_wrap(~ effect_shape) +\n  labs(x = \"True Effect (unstandardized b)\", y = \"Power\",\n       title = \"Power by Model and Effect Shape\") +\n  scale_x_continuous(breaks = c(1.2, 2.4, 3.6, 4.8, 6),\n                     sec.axis = sec_axis(~ . / 12, name = \"Standardized Effect (b/12)\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, 1), breaks = seq(0, 1, .1))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sleep Quality Analysis</span>"
    ]
  },
  {
    "objectID": "scripts/sim_self_report.html#planned-sensitivity-analyses",
    "href": "scripts/sim_self_report.html#planned-sensitivity-analyses",
    "title": "2  Sleep Quality Analysis",
    "section": "2.5 Planned Sensitivity Analyses",
    "text": "2.5 Planned Sensitivity Analyses\nWe have preregistered several sensitivity analyses to test the robustness of any effects we find. These include:\n\nDay 21 only: We will estimate the effect of the intervention on day 21 only, to see what the difference between groups is at the end of the intervention period\nMarginal means: In H3a, we will use the emmeans package to calculate marginal means for each condition and produce a single parameter by integrating across the 14-day period\nMultilevel model: We will fit the MLM as defined in Table 1 above, as the second-highest performing model in the simulations (having higher power for linear effects, but lower for non-linear)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sleep Quality Analysis</span>"
    ]
  }
]