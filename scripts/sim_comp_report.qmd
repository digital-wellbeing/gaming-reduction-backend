---
title: "H1 - Effect of Abstention on Sendentary Activity"
format: 
  html:
    toc: true
    code-fold: true
author: "Tamas Földes"
date: today
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Introduction

This document demonstrates a simulation-based power analysis for a two-arm parallel randomized controlled trial (RCT) with compositional outcomes. The simulation models 24-hour time use data consisting of three components: sleep, sedentary time, and physical activity, which sum to 1440 minutes (24 hours). The analysis includes visualization of power curves and effect sizes to help determine optimal sample sizes and assess the sensitivity of the study design to different parameters.

### Required Packages

```{r packages}
#| label: packages
#| code-summary: "Show code (libraries)"
#| output: false

# Load required packages with error handling
load_package_safely <- function(pkg) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    message(paste("Package", pkg, "not available, attempting to install..."))
    tryCatch({
      if (pkg %in% c("microbiome", "ComplexHeatmap")) {
        # Bioconductor packages
        if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager")
        BiocManager::install(pkg, update = FALSE)
      } else if (pkg == "microViz") {
        # Special repository
        install.packages(pkg, repos = c(davidbarnett = "https://david-barnett.r-universe.dev", getOption("repos")))
      } else {
        # Regular CRAN packages
        install.packages(pkg)
      }
    }, error = function(e) {
      message(paste("Failed to install", pkg, ":", e$message))
    })
  }
  
  # Try to load the package
  success <- suppressMessages(suppressWarnings(
    tryCatch({
      library(pkg, character.only = TRUE)
      TRUE
    }, error = function(e) {
      message(paste("Package", pkg, "not available, skipping"))
      FALSE
    })
  ))
  return(success)
}

# Load core packages (required for analysis)
core_packages <- c("tidyverse", "lme4", "compositions", "lmerTest", "MASS")
message("Loading core packages...")
loaded_core <- sapply(core_packages, load_package_safely)
if (!all(loaded_core)) {
  stop("Core packages failed to load: ", paste(core_packages[!loaded_core], collapse = ", "))
}

# Load optional packages (nice to have but not critical)
optional_packages <- c("ggtern", "progress", "patchwork", "foreach", "doSNOW", "doParallel", "DT", "ggtext", "ggraph", "kableExtra")
message("Loading optional packages...")
loaded_optional <- sapply(optional_packages, load_package_safely)
message("Optional packages loaded: ", paste(optional_packages[loaded_optional], collapse = ", "))
if (any(!loaded_optional)) {
  message("Optional packages not loaded: ", paste(optional_packages[!loaded_optional], collapse = ", "))
}

# Load specialized packages (for microbiome analysis, if needed)
# Note: phyloseq temporarily commented out due to installation issues
specialized_packages <- c("microbiome", "ComplexHeatmap", "microViz", "corncob")
message("Loading specialized packages...")
loaded_specialized <- sapply(specialized_packages, load_package_safely)
message("Specialized packages loaded: ", paste(specialized_packages[loaded_specialized], collapse = ", "))
if (any(!loaded_specialized)) {
  message("Specialized packages not loaded: ", paste(specialized_packages[!loaded_specialized], collapse = ", "))
}
```

### Helper Functions

These functions convert between compositional data and isometric log-ratio (ilr) coordinates.

```{r helper_functions}
comp_to_ilr <- function(x_min) {
  stopifnot(is.matrix(x_min), ncol(x_min) == 3)
  bad_row <- !is.finite(rowSums(x_min)) | rowSums(x_min) <= 0
  if (any(bad_row)) {
    x_min[bad_row, ] <- matrix(rep(c(600, 480, 360), each = sum(bad_row)), ncol = 3, byrow = TRUE)
  }
  x_min[x_min <= 0 | !is.finite(x_min)] <- 1e-6
  compositions::ilr(sweep(x_min, 1, rowSums(x_min), "/"))
}

ilr_to_minutes <- function(ilr_mat, total = 1440) {
  stopifnot(is.matrix(ilr_mat), ncol(ilr_mat) == 2)
  comp_obj <- compositions::ilrInv(ilr_mat)
  prop <- as.data.frame(comp_obj)
  prop <- as.matrix(prop)
  
  bad <- apply(prop, 1, function(r) any(!is.finite(r) | r <= 0) ||
                                   !is.finite(sum(r)) || abs(sum(r) - 1) > 1e-8)
  if (any(bad)) prop[bad, ] <- 1/3
  round(prop * total, 1)
}
```

## Understanding Within-Subject Variability

The **within-subject SD** (`s_within`) parameter controls how much a person's sedentary behavior varies from day to day around their personal average. This captures the natural day-to-day fluctuations in behavior that everyone experiences.

### What Different SD Values Mean in Practice

Let's illustrate what different within-subject SD values mean for typical sedentary behavior patterns:

```{r within_subject_interpretation}
#| label: within-subject-interpretation
#| code-summary: "Show code (within-subject variability examples)"

# Function to demonstrate within-subject variability in practical terms
demonstrate_within_subject_sd <- function(baseline_sedentary = 600, s_within_values = c(0.1, 0.2, 0.3, 0.4), n_days = 14) {
  
  set.seed(42)  # For reproducible examples
  
  results <- tibble()
  
  for (sd_val in s_within_values) {
    # Generate ILR coordinates with this SD level
    base_comp <- c(baseline_sedentary, 480, 360)  # baseline composition
    base_ilr <- comp_to_ilr(matrix(base_comp, nrow = 1))
    
    # Add within-subject variation in ILR space
    daily_ilr <- base_ilr[1,] + MASS::mvrnorm(n_days, mu = c(0, 0), Sigma = diag(sd_val^2, 2))
    
    # Convert back to minutes
    daily_mins <- ilr_to_minutes(daily_ilr)
    sedentary_mins <- daily_mins[, 1]
    
    # Calculate practical metrics
    daily_range <- max(sedentary_mins) - min(sedentary_mins)
    typical_variation <- sd(sedentary_mins)
    
    # Store results
    day_data <- tibble(
      sd_level = paste0("SD = ", sd_val),
      day = 1:n_days,
      sedentary = sedentary_mins,
      deviation_from_mean = sedentary_mins - mean(sedentary_mins)
    )
    
    results <- bind_rows(results, day_data)
  }
  
  return(results)
}

# Generate example data
variability_examples <- demonstrate_within_subject_sd()

# Calculate summary statistics for each SD level
variability_summary <- variability_examples %>%
  group_by(sd_level) %>%
  summarise(
    mean_sedentary = round(mean(sedentary), 1),
    sd_sedentary = round(sd(sedentary), 1),
    min_sedentary = round(min(sedentary), 1),
    max_sedentary = round(max(sedentary), 1),
    daily_range = round(max(sedentary) - min(sedentary), 1),
    .groups = "drop"
  ) %>%
  mutate(
    interpretation = case_when(
      sd_level == "SD = 0.1" ~ "Very consistent (±15-20 min/day)",
      sd_level == "SD = 0.2" ~ "Moderately consistent (±30-40 min/day)", 
      sd_level == "SD = 0.3" ~ "Moderately variable (±45-60 min/day)",
      sd_level == "SD = 0.4" ~ "Highly variable (±60-80 min/day)"
    )
  )

# Display summary table
knitr::kable(
  variability_summary,
  col.names = c("Within-Subject SD", "Mean Sedentary", "SD (minutes)", "Min", "Max", "Daily Range", "Behavioral Interpretation"),
  caption = "How Within-Subject SD Translates to Daily Behavioral Variation",
  digits = 1
)
```

```{r plot_within_subject_variability}
#| label: plot-within-subject-variability
#| fig-height: 8
#| fig-width: 10

# Create visualization of different variability levels
variability_plot <- ggplot(variability_examples, aes(x = day, y = sedentary, color = sd_level)) +
  geom_line(size = 1, alpha = 0.8) +
  geom_point(size = 2, alpha = 0.7) +
  geom_hline(data = variability_examples %>% group_by(sd_level) %>% summarise(mean_sed = mean(sedentary)), 
             aes(yintercept = mean_sed, color = sd_level), 
             linetype = "dashed", alpha = 0.6) +
  facet_wrap(~ sd_level, ncol = 2) +
  labs(
    title = "Day-to-Day Sedentary Behavior Patterns by Within-Subject Variability",
    subtitle = "Each panel shows 14 days of simulated sedentary time for different SD levels\nDashed lines show individual means",
    x = "Day",
    y = "Sedentary Time (minutes)",
    color = "Variability Level"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",  # Remove legend since facets show the levels
    strip.text = element_text(size = 12, face = "bold")
  ) +
  scale_color_viridis_d(option = "plasma") +
  scale_y_continuous(breaks = seq(500, 700, 25))

print(variability_plot)

# Create a deviation plot to show how much each day deviates from personal mean
deviation_plot <- ggplot(variability_examples, aes(x = day, y = deviation_from_mean, fill = sd_level)) +
  geom_col(alpha = 0.7) +
  geom_hline(yintercept = 0, color = "black", size = 1) +
  facet_wrap(~ sd_level, ncol = 2) +
  labs(
    title = "Daily Deviations from Personal Average",
    subtitle = "Positive values = more sedentary than usual; Negative values = less sedentary than usual",
    x = "Day",
    y = "Deviation from Personal Mean (minutes)",
    fill = "Variability Level"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    strip.text = element_text(size = 12, face = "bold")
  ) +
  scale_fill_viridis_d(option = "plasma") +
  scale_y_continuous(breaks = seq(-80, 80, 20))

print(deviation_plot)
```

### Real-World Context

These variability levels correspond to different types of lifestyle patterns:

- **SD = 0.1 (Very Consistent)**: Someone with a highly regular routine - same commute, same work schedule, consistent evening activities. Day-to-day sedentary time varies by only ±15-20 minutes.

- **SD = 0.2 (Moderately Consistent)**: Typical working adult with some routine variation - occasional work-from-home days, some weekend differences, minor schedule changes. Varies by ±30-40 minutes daily.

- **SD = 0.3 (Moderately Variable)**: More lifestyle variation - mix of office/remote work, irregular weekend activities, some social commitments affecting routine. Varies by ±45-60 minutes daily.

- **SD = 0.4 (Highly Variable)**: Irregular schedule or lifestyle - shift work, freelancing, frequent travel, or major day-to-day routine differences. Can vary by ±60-80 minutes daily.


## Simulation Engine

```{r sim_engine, eval=FALSE}
# Power estimation using SimEngine – cleaned version (no play_sd_prop, no corr_noise_sd)

# Install SimEngine if not already installed
if (!requireNamespace("SimEngine", quietly = TRUE)) {
  install.packages("SimEngine")
}

# Load required packages
library(SimEngine)
library(lme4)
library(lmerTest)
library(compositions)
library(MASS)
library(dplyr)

# --------------------------------------------------------------------------------
# Command line argument parsing ---------------------------------------------------
# --------------------------------------------------------------------------------
# Parse command line arguments for flexible parameter adjustment
args <- commandArgs(trailingOnly = TRUE)

# Default values
default_sims <- 1000
default_cores <- 16

# Parse arguments: --sims=VALUE --cores=VALUE
sims_param <- default_sims
cores_param <- default_cores

if (length(args) > 0) {
  for (arg in args) {
    if (grepl("^--sims=", arg)) {
      sims_param <- as.numeric(sub("^--sims=", "", arg))
      if (is.na(sims_param) || sims_param <= 0) {
        warning("Invalid sims parameter, using default: ", default_sims)
        sims_param <- default_sims
      }
    } else if (grepl("^--cores=", arg)) {
      cores_param <- as.numeric(sub("^--cores=", "", arg))
      if (is.na(cores_param) || cores_param <= 0) {
        warning("Invalid cores parameter, using default: ", default_cores)
        cores_param <- default_cores
      }
    }
  }
}

# Log the parameters being used
message("=== SIMULATION PARAMETERS ===")
message("Number of simulations: ", sims_param)
message("Number of cores: ", cores_param)
message("==============================")

# --------------------------------------------------------------------------------
# Main simulation wrapper ---------------------------------------------------------
# --------------------------------------------------------------------------------

est_power_simengine <- function(n_pg = 40,
                               effect_min_values = c(30),
                               s_between_values  = c(0.15),
                               s_within_values   = c(0.25),
                               baseline_days     = 7,
                               intervention_days = 14,
                               sims              = 500,
                               cores             = 4) {

  start_time <- Sys.time()
  message("Setting up SimEngine simulation …")

  # Create simulation object
  sim <- new_sim()

  # ------------------------------------------------------------------------------
  # LEVELS (note: no play_sd_prop, no corr_noise_sd) ------------------------------
  # ------------------------------------------------------------------------------
  sim %<>% set_levels(
    n_pg             = n_pg,
    effect_min       = effect_min_values,
    s_between        = s_between_values,
    s_within         = s_within_values,
    baseline_days    = baseline_days,
    intervention_days= intervention_days
  )

  # ------------------------------------------------------------------------------
  # Helper transformations (INSIDE function for parallel access) ------------------
  # ------------------------------------------------------------------------------
  
  comp_to_ilr <- function(x_min) {
    stopifnot(is.matrix(x_min), ncol(x_min) == 3)
    bad_row <- !is.finite(rowSums(x_min)) | rowSums(x_min) <= 0
    if (any(bad_row)) {
      x_min[bad_row, ] <- matrix(rep(c(600, 480, 360), each = sum(bad_row)), ncol = 3, byrow = TRUE)
    }
    x_min[x_min <= 0 | !is.finite(x_min)] <- 1e-6
    compositions::ilr(sweep(x_min, 1, rowSums(x_min), "/"))
  }

  ilr_to_minutes <- function(ilr_mat, total = 1440) {
    stopifnot(is.matrix(ilr_mat), ncol(ilr_mat) == 2)
    comp_obj <- compositions::ilrInv(ilr_mat)
    prop <- as.matrix(as.data.frame(comp_obj))
    bad <- apply(prop, 1, function(r) any(!is.finite(r) | r <= 0) ||
                   !is.finite(sum(r)) || abs(sum(r) - 1) > 1e-8)
    if (any(bad)) prop[bad, ] <- 1/3
    round(prop * total, 1)
  }

  # ------------------------------------------------------------------------------
  # Data‑generating function ------------------------------------------------------
  # ------------------------------------------------------------------------------
  generate_data <- function(n_pg, effect_min, baseline_days, intervention_days,
                            s_between, s_within, seed = NULL) {

    if (!is.null(seed)) set.seed(seed)

    N   <- n_pg * 2
    grp <- rep(0:1, each = n_pg)              # 0 = Control, 1 = Intervention

    # Mean daily compositions: (sedentary, sleep, physical)
    base_comp   <- c(600, 480, 360)
    active_comp <- c(600 - effect_min, 480, 360 + effect_min)

    # Person‑level random effects in ILR space
    b_ilr <- MASS::mvrnorm(N, mu = c(0, 0), Sigma = diag(s_between^2, 2))

    # Person-specific playtime proportion of sedentary time (10-40%)
    personal_play_prop <- sapply(1:N, function(i) {
      p <- rbeta(1, 2, 5) * 0.3 + 0.1  # right-skew between 0.1-0.4
      return(p)
    })

    # Person-specific compliance rates for intervention group (60-95%)
    # Control group gets compliance = 1 (no intervention to comply with)
    personal_compliance <- sapply(1:N, function(i) {
      if (grp[i] == 0) {
        return(1.0)  # Control group - no intervention
      } else {
        # Intervention group: Beta distribution shifted to 60-95% range
        # Beta(2,2) gives symmetric distribution, shifted to [0.6, 0.95]
        compliance <- rbeta(1, 2, 2) * 0.35 + 0.6
        return(compliance)
      }
    })
    
    # Daily compliance variation using Kumaraswamy distribution
    # This adds day-to-day variation in compliance within each person
    daily_compliance_variation <- function(n_days, base_compliance) {
      # Use Kumaraswamy distribution to add daily variation
      # Parameters chosen to create realistic daily fluctuations around base compliance
      daily_factors <- extraDistr::rkumar(n_days, a = 0.05, b = 0.1)
      # Scale the factors to create variation around base compliance
      # This creates realistic day-to-day variation in compliance behavior
      pmax(0, pmin(1, base_compliance * (0.8 + 0.4 * daily_factors)))
    }

    # Containers
    all_ids <- all_periods <- all_days <- NULL
    all_ilr <- matrix(, 0, 2)
    all_sedentary <- numeric()  # Store actual sedentary minutes

    for (i in seq_len(N)) {
      for (period in c("baseline", "intervention")) {
        ndays   <- if (period == "baseline") baseline_days else intervention_days
        comp_mu <- if (period == "baseline" || grp[i] == 0) base_comp else active_comp

        comp_ilr <- comp_to_ilr(matrix(rep(comp_mu, ndays), ncol = 3, byrow = TRUE))
        comp_ilr <- sweep(comp_ilr, 2, b_ilr[i, ], "+")               # add person RE
        day_ilr  <- comp_ilr + MASS::mvrnorm(ndays, mu = c(0, 0),
                                             Sigma = diag(s_within^2, 2))

        # Index bookkeeping
        all_ids     <- c(all_ids, rep(i, ndays))
        all_periods <- c(all_periods, rep(period, ndays))
        all_days    <- c(all_days,
                         if (period == "baseline") seq_len(baseline_days)
                         else baseline_days + seq_len(intervention_days))
        all_ilr     <- rbind(all_ilr, day_ilr)
        
        # Store sedentary minutes for this person-period (will be calculated after ILR transformation)
        # We'll calculate playtime after we have the actual sedentary minutes
      }
    }

    # Back‑transform ILR → minutes and calculate playtime based on actual sedentary behavior
    mins <- ilr_to_minutes(all_ilr)
    colnames(mins) <- c("sedentary", "sleep", "physical")
    
    # Now generate playtime based on actual sedentary minutes
    playmin <- numeric(length(all_ids))
    
    # Pre-generate daily compliance for each person during intervention period
    daily_compliance <- list()
    for (person_id in 1:N) {
      if (grp[person_id] == 1) {  # Intervention group
        daily_compliance[[person_id]] <- daily_compliance_variation(
          intervention_days, 
          personal_compliance[person_id]
        )
      } else {
        daily_compliance[[person_id]] <- rep(1.0, intervention_days)  # Control group
      }
    }
    
    # Track intervention day counter for each person
    intervention_day_counter <- rep(0, N)
    
    for (i in seq_along(all_ids)) {
      person_id <- all_ids[i]
      period <- all_periods[i]
      actual_sedentary <- mins[i, "sedentary"]
      
      # Base playtime as proportion of actual sedentary time
      base_playtime <- personal_play_prop[person_id] * actual_sedentary
      
      # Add small amount of day-to-day noise (2% of base playtime)
      daily_sd <- 0.02 * base_playtime
      noisy_playtime <- rnorm(1, base_playtime, daily_sd)
      
      # Apply intervention effect for intervention group during intervention period
      if (period == "intervention" && grp[person_id] == 1) {
        # Increment intervention day counter for this person
        intervention_day_counter[person_id] <- intervention_day_counter[person_id] + 1
        
        # Get daily compliance for this person and day
        daily_compliance_rate <- daily_compliance[[person_id]][intervention_day_counter[person_id]]
        
        # Reduce playtime by effect_min * daily compliance rate
        # Perfect compliance = full effect_min reduction
        # Partial compliance = proportional reduction
        actual_reduction <- effect_min * daily_compliance_rate
        intervention_playtime <- pmax(0, noisy_playtime - actual_reduction)
        playmin[i] <- intervention_playtime
      } else {
        # Control group or baseline period: just use the playtime based on actual sedentary
        playmin[i] <- pmax(0, noisy_playtime)  # Ensure non-negative
      }
    }

    # Create daily compliance values for the dataset
    daily_compliance_values <- numeric(length(all_ids))
    intervention_day_counter <- rep(0, N)
    
    for (i in seq_along(all_ids)) {
      person_id <- all_ids[i]
      period <- all_periods[i]
      
      if (period == "intervention" && grp[person_id] == 1) {
        intervention_day_counter[person_id] <- intervention_day_counter[person_id] + 1
        daily_compliance_values[i] <- daily_compliance[[person_id]][intervention_day_counter[person_id]]
      } else {
        daily_compliance_values[i] <- personal_compliance[person_id]
      }
    }
    
    # Assemble data frame
    dat <- data.frame(
      id        = factor(all_ids),
      group     = factor(grp[all_ids], labels = c("Control", "Abstinence")),
      period    = factor(all_periods, levels = c("baseline", "intervention")),
      day       = all_days,
      sedentary = mins[, 1],
      sleep     = mins[, 2],
      physical  = mins[, 3],
      playtime  = playmin,
      compliance = daily_compliance_values,  # Add daily compliance to dataset
      base_compliance = personal_compliance[all_ids]  # Add base person-level compliance
    )

    dat <- dat %>%
      group_by(id) %>%
      mutate(
        base_play_mean      = mean(playtime[period == "baseline"]),
        playtime_reduction  = base_play_mean - playtime,
        intervention_active = as.integer(group == "Abstinence" & period == "intervention"),
        # Calculate actual compliance as proportion of intended reduction achieved
        intended_reduction  = ifelse(group == "Abstinence" & period == "intervention", effect_min, 0),
        actual_compliance   = ifelse(intended_reduction > 0, 
                                   pmin(1, playtime_reduction / intended_reduction), 
                                   compliance)
      ) %>%
      ungroup()

    return(dat)
  }

  # ------------------------------------------------------------------------------
  # Analysis function -------------------------------------------------------------
  # ------------------------------------------------------------------------------
  run_analysis <- function(data) {
    data_ilr <- data
    comp_matrix <- as.matrix(data[, c("sedentary", "sleep", "physical")])
    ilr_coords  <- comp_to_ilr(comp_matrix)
    data_ilr$ilr1 <- ilr_coords[, 1]

    results <- list()

    ## Between‑group effect during intervention ----------------------------------
    md <- subset(data_ilr, period == "intervention")
    mb <- try(lmer(ilr1 ~ group + (1 | id), data = md), silent = TRUE)
    results$p_between <- if (!inherits(mb, "try-error")) anova(mb)["group", "Pr(>F)"] else NA

    ## Within‑group effects -------------------------------------------------------
    mc <- try(lmer(ilr1 ~ period + (1 | id), data = subset(data_ilr, group == "Control")), silent = TRUE)
    results$p_control <- if (!inherits(mc, "try-error")) anova(mc)["period", "Pr(>F)"] else NA

    mi <- try(lmer(ilr1 ~ period + (1 | id), data = subset(data_ilr, group == "Abstinence")), silent = TRUE)
    results$p_intervention <- if (!inherits(mi, "try-error")) anova(mi)["period", "Pr(>F)"] else NA

    ## Interaction ----------------------------------------------------------------
    mx <- try(lmer(ilr1 ~ group * period + (1 | id), data = data_ilr), silent = TRUE)
    results$p_interaction <- if (!inherits(mx, "try-error")) anova(mx)["group:period", "Pr(>F)"] else NA

    ## Per‑protocol contrast (original change score approach) ---------------------
    mp_change <- try(lmer(ilr1 ~ intervention_active * playtime_reduction + (1 | id), data = data_ilr), silent = TRUE)
    results$p_protocol_change <- if (!inherits(mp_change, "try-error")) anova(mp_change)["intervention_active:playtime_reduction", "Pr(>F)"] else NA
    
    ## Per‑protocol contrast (robust approach without change scores) --------------
    # This model tests if the intervention effect on sedentary behavior (ilr1) varies 
    # as a function of actual playtime levels, controlling for baseline playtime.
    # More robust than change scores as it directly models the relationship between
    # current playtime and outcomes while adjusting for baseline differences.
    mp_robust <- try(lmer(ilr1 ~ intervention_active * playtime + base_play_mean + (1 | id), data = data_ilr), silent = TRUE)
    results$p_protocol_robust <- if (!inherits(mp_robust, "try-error")) anova(mp_robust)["intervention_active:playtime", "Pr(>F)"] else NA

    return(results)
  }

  # ------------------------------------------------------------------------------
  # Simulation script ------------------------------------------------------------
  # ------------------------------------------------------------------------------
  sim %<>% set_script(function() {
    set.seed(sample.int(1e7, 1))
    
    # Access simulation level variables correctly
    data <- generate_data(
      n_pg             = L$n_pg,
      effect_min       = L$effect_min,
      baseline_days    = L$baseline_days,
      intervention_days= L$intervention_days,
      s_between        = L$s_between,
      s_within         = L$s_within
    )
    
    # Run analysis and ensure proper error handling
    result <- tryCatch({
      run_analysis(data)
    }, error = function(e) {
      # Return NA values with proper names if analysis fails
      list(
        p_between = NA_real_, 
        p_control = NA_real_, 
        p_intervention = NA_real_,
        p_interaction = NA_real_, 
        p_protocol_change = NA_real_,
        p_protocol_robust = NA_real_
      )
    })
    
    # Ensure result is a proper list with all required elements
    if (!is.list(result)) {
      result <- list(
        p_between = NA_real_, 
        p_control = NA_real_, 
        p_intervention = NA_real_,
        p_interaction = NA_real_, 
        p_protocol_change = NA_real_,
        p_protocol_robust = NA_real_
      )
    }
    
    # Ensure all required columns exist
    required_names <- c("p_between", "p_control", "p_intervention", "p_interaction", "p_protocol_change", "p_protocol_robust")
    for (name in required_names) {
      if (!(name %in% names(result))) {
        result[[name]] <- NA_real_
      }
    }
    
    return(result)
  })

  # ------------------------------------------------------------------------------
  # Config & run -----------------------------------------------------------------
  # ------------------------------------------------------------------------------
  sim %<>% set_config(
    num_sim      = sims,
    parallel     = TRUE,   # Enable parallel processing
    n_cores      = cores,  # Use specified cores
    packages     = c("lme4", "lmerTest", "compositions", "MASS", "dplyr"),
    progress_bar = TRUE
  )

  
  # # Add a test run to debug issues
  # message("Testing data generation and analysis functions...")
  # tryCatch({
  #   test_data <- generate_data(
  #     n_pg = 10,  # Small test
  #     effect_min = 30,
  #     baseline_days = 7,
  #     intervention_days = 14,
  #     s_between = 0.15,
  #     s_within = 0.25
  #   )
  #   message("✓ Data generation successful")
  #   message("Test data dimensions: ", nrow(test_data), " x ", ncol(test_data))
    
  #   test_results <- run_analysis(test_data)
  #   message("✓ Analysis function successful")
  #   message("Test results: ", paste(names(test_results), test_results, sep="=", collapse=", "))
  # }, error = function(e) {
  #   message("❌ Test failed with error: ", e$message)
  #   stop("Stopping due to test failure. Fix the issue before running full simulation.")
  # })

  # message("Running simulations …")
  
  sim %<>% run()

  # ------------------------------------------------------------------------------
  # Summarise power --------------------------------------------------------------
  # ------------------------------------------------------------------------------
  results <- sim$results
  
  # Add debugging information
  message("Debug: Checking simulation results...")
  message("Results object class: ", class(results))
  message("Results is null: ", is.null(results))
  if (!is.null(results)) {
    message("Results dimensions: ", nrow(results), " x ", ncol(results))
    message("Results column names: ", paste(names(results), collapse = ", "))
  }
  
  # Add error handling for when all simulations fail
  if (is.null(results) || (is.data.frame(results) && nrow(results) == 0)) {
    stop("All simulations failed. Check your simulation parameters and functions.")
  }
  
  # Check if required columns exist before processing
  required_cols <- c("p_between", "p_control", "p_intervention", "p_interaction", "p_protocol_change", "p_protocol_robust")
  missing_cols <- setdiff(required_cols, names(results))
  if (length(missing_cols) > 0) {
    stop(paste("Missing columns in results:", paste(missing_cols, collapse = ", ")))
  }
  
  for (col in required_cols) {
    results[[col]] <- as.numeric(as.character(results[[col]]))
  }

  power_df <- aggregate(
    cbind(
      power_between        = results$p_between        < 0.05,
      power_control        = results$p_control        < 0.05,
      power_intervention   = results$p_intervention   < 0.05,
      power_interaction    = results$p_interaction    < 0.05,
      power_protocol_change= results$p_protocol_change< 0.05,
      power_protocol_robust= results$p_protocol_robust< 0.05,
      valid_between        = !is.na(results$p_between),
      valid_control        = !is.na(results$p_control), 
      valid_intervention   = !is.na(results$p_intervention),
      valid_interaction    = !is.na(results$p_interaction),
      valid_protocol_change= !is.na(results$p_protocol_change),
      valid_protocol_robust= !is.na(results$p_protocol_robust)
    ),
    by = list(
      n_pg             = results$n_pg,
      effect_min       = results$effect_min,
      s_between        = results$s_between,
      s_within         = results$s_within,
      baseline_days    = results$baseline_days,
      intervention_days= results$intervention_days
    ),
    FUN = mean, na.rm = TRUE
  )

  end_time <- Sys.time()
  message(sprintf("Total elapsed time: %.2f mins", as.numeric(difftime(end_time, start_time, units = "mins"))))

  list(power_summary = power_df, sim_object = sim)
}

# --------------------------------------------------------------------------------
# Example call -------------------------------------------------------------------
# --------------------------------------------------------------------------------
result <- est_power_simengine(
  n_pg               = 40,  # 40 participants per group
  effect_min_values =  c(30, 60, 90, 120),          
  s_between_values = seq(0.1, 0.3, by = 0.05),
  s_within_values = seq(0.15, 0.35, by = 0.05),
  baseline_days      = 7,
  intervention_days  = 14,
  sims               = sims_param, 
  cores              = cores_param     
)

# result <- est_power_simengine(
#   n_pg               = c(50),  # Multiple sample sizes
#   effect_min_values =  c(30, 60, 90, 120),          
#   s_between_values = seq(0.1, 0.3, by = 0.05),
#   s_within_values = seq(0.15, 0.35, by = 0.05),
#   baseline_days      = 7,
#   intervention_days  = 14,
#   sims               = sims_param, 
#   cores              = cores_param     
# )
# print(result$power_summary)

# Save results with descriptive name and timestamp
timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
filename <- paste0("scripts/sim_comp_debug/power_sim_results_", timestamp, ".RData")
save(result, file = filename)

# Print power summary
# print(result$power_summary)

# Print save location
message("Results saved to: ", filename)

# POWER SUMMARY ANALYSIS
message("\n" , paste(rep("=", 60), collapse=""))
message("POWER SUMMARY - TOP PERFORMING SETTINGS")
message(paste(rep("=", 60), collapse=""))

power_data <- result$power_summary

# Summary for power_interaction
message("\n🎯 INTERACTION EFFECT POWER SUMMARY:")
message("-----------------------------------")

# Find maximum power for interaction
max_interaction_power <- max(power_data$power_interaction, na.rm = TRUE)
best_interaction <- power_data[which.max(power_data$power_interaction), ]

message(sprintf("Maximum Interaction Power: %.3f", max_interaction_power))
message("Best settings:")
message(sprintf("  • Sample size per group (n_pg): %d", best_interaction$n_pg))
message(sprintf("  • Effect size (effect_min): %d minutes", best_interaction$effect_min))
message(sprintf("  • Between-subject SD (s_between): %.3f", best_interaction$s_between))
message(sprintf("  • Within-subject SD (s_within): %.3f", best_interaction$s_within))

# Show top 3 settings for interaction
message("\nTop 3 settings for interaction power:")
top_interaction <- power_data[order(power_data$power_interaction, decreasing = TRUE)[1:min(3, nrow(power_data))], ]
for(i in 1:nrow(top_interaction)) {
  row <- top_interaction[i, ]
  message(sprintf("%d. Power=%.3f | n_pg=%d | effect=%d | s_between=%.3f | s_within=%.3f", 
                  i, row$power_interaction, row$n_pg, row$effect_min, row$s_between, row$s_within))
}

# Summary for power_protocol (both approaches)
message("\n🎯 PROTOCOL EFFECT POWER SUMMARY:")
message("--------------------------------")

# Change score approach
max_protocol_change_power <- max(power_data$power_protocol_change, na.rm = TRUE)
best_protocol_change <- power_data[which.max(power_data$power_protocol_change), ]

message("\n📊 CHANGE SCORE APPROACH:")
message(sprintf("Maximum Protocol Power (Change): %.3f", max_protocol_change_power))
message("Best settings:")
message(sprintf("  • Sample size per group (n_pg): %d", best_protocol_change$n_pg))
message(sprintf("  • Effect size (effect_min): %d minutes", best_protocol_change$effect_min))
message(sprintf("  • Between-subject SD (s_between): %.3f", best_protocol_change$s_between))
message(sprintf("  • Within-subject SD (s_within): %.3f", best_protocol_change$s_within))

# Robust approach
max_protocol_robust_power <- max(power_data$power_protocol_robust, na.rm = TRUE)
best_protocol_robust <- power_data[which.max(power_data$power_protocol_robust), ]

message("\n📊 ROBUST APPROACH (no change scores):")
message(sprintf("Maximum Protocol Power (Robust): %.3f", max_protocol_robust_power))
message("Best settings:")
message(sprintf("  • Sample size per group (n_pg): %d", best_protocol_robust$n_pg))
message(sprintf("  • Effect size (effect_min): %d minutes", best_protocol_robust$effect_min))
message(sprintf("  • Between-subject SD (s_between): %.3f", best_protocol_robust$s_between))
message(sprintf("  • Within-subject SD (s_within): %.3f", best_protocol_robust$s_within))

# Comparison
message("\n🔍 PROTOCOL APPROACH COMPARISON:")
message(sprintf("Change Score Approach - Mean: %.3f, Max: %.3f", 
                mean(power_data$power_protocol_change, na.rm = TRUE),
                max_protocol_change_power))
message(sprintf("Robust Approach - Mean: %.3f, Max: %.3f", 
                mean(power_data$power_protocol_robust, na.rm = TRUE),
                max_protocol_robust_power))
                
power_diff <- mean(power_data$power_protocol_robust, na.rm = TRUE) - mean(power_data$power_protocol_change, na.rm = TRUE)
message(sprintf("Robust approach is %.3f points %s on average", 
                abs(power_diff), 
                ifelse(power_diff > 0, "higher", "lower")))

# Overall summary statistics
message("\n📊 OVERALL POWER STATISTICS:")
message("---------------------------")
message(sprintf("Interaction Power - Mean: %.3f, Range: %.3f - %.3f", 
                mean(power_data$power_interaction, na.rm = TRUE),
                min(power_data$power_interaction, na.rm = TRUE),
                max(power_data$power_interaction, na.rm = TRUE)))
                
message(sprintf("Protocol Power (Change) - Mean: %.3f, Range: %.3f - %.3f", 
                mean(power_data$power_protocol_change, na.rm = TRUE),
                min(power_data$power_protocol_change, na.rm = TRUE),
                max(power_data$power_protocol_change, na.rm = TRUE)))
                
message(sprintf("Protocol Power (Robust) - Mean: %.3f, Range: %.3f - %.3f", 
                mean(power_data$power_protocol_robust, na.rm = TRUE),
                min(power_data$power_protocol_robust, na.rm = TRUE),
                max(power_data$power_protocol_robust, na.rm = TRUE)))

# DATA QUALITY ANALYSIS
message("\n" , paste(rep("=", 60), collapse=""))
message("DATA QUALITY ANALYSIS - VALIDITY RATES")
message(paste(rep("=", 60), collapse=""))

# Check validity rates for each contrast
validity_threshold <- 0.95
total_rows <- nrow(power_data)

# Function to analyze validity for each contrast
analyze_validity <- function(valid_col, contrast_name) {
  high_validity_count <- sum(power_data[[valid_col]] > validity_threshold, na.rm = TRUE)
  perfect_validity_count <- sum(power_data[[valid_col]] == 1.0, na.rm = TRUE)
  mean_validity <- mean(power_data[[valid_col]], na.rm = TRUE)
  min_validity <- min(power_data[[valid_col]], na.rm = TRUE)
  
  message(sprintf("\n🔍 %s VALIDITY:", toupper(contrast_name)))
  message(sprintf("  • Rows with validity > %.2f: %d/%d (%.1f%%)", 
                  validity_threshold, high_validity_count, total_rows, 
                  100 * high_validity_count / total_rows))
  message(sprintf("  • Rows with perfect validity (1.0): %d/%d (%.1f%%)", 
                  perfect_validity_count, total_rows, 
                  100 * perfect_validity_count / total_rows))
  message(sprintf("  • Mean validity: %.3f", mean_validity))
  message(sprintf("  • Minimum validity: %.3f", min_validity))
  
  # Identify problematic parameter combinations if any
  if (high_validity_count < total_rows) {
    low_validity_rows <- power_data[power_data[[valid_col]] <= validity_threshold, ]
    message(sprintf("  ⚠️  %d rows with validity ≤ %.2f:", 
                    nrow(low_validity_rows), validity_threshold))
    for(i in 1:min(3, nrow(low_validity_rows))) {  # Show up to 3 examples
      row <- low_validity_rows[i, ]
      message(sprintf("     Example %d: validity=%.3f | n_pg=%d | effect=%d | s_between=%.3f | s_within=%.3f", 
                      i, row[[valid_col]], row$n_pg, row$effect_min, row$s_between, row$s_within))
    }
    if (nrow(low_validity_rows) > 3) {
      message(sprintf("     ... and %d more problematic combinations", nrow(low_validity_rows) - 3))
    }
  } else {
    message("  ✅ All parameter combinations produced high-quality results!")
  }
  
  return(list(
    high_validity_count = high_validity_count,
    perfect_validity_count = perfect_validity_count,
    mean_validity = mean_validity,
    min_validity = min_validity
  ))
}

# Analyze each contrast type
contrasts <- list(
  "valid_between" = "Between-Group",
  "valid_control" = "Control Within-Group", 
  "valid_intervention" = "Intervention Within-Group",
  "valid_interaction" = "Group × Period Interaction",
  "valid_protocol_change" = "Per-Protocol (Change Score)",
  "valid_protocol_robust" = "Per-Protocol (Robust)"
)

validity_summary <- list()
for(col in names(contrasts)) {
  validity_summary[[col]] <- analyze_validity(col, contrasts[[col]])
}

# Overall validity summary
message("\n📋 OVERALL VALIDITY SUMMARY:")
message("---------------------------")
all_high_validity <- sapply(validity_summary, function(x) x$high_validity_count)
all_perfect_validity <- sapply(validity_summary, function(x) x$perfect_validity_count)
all_mean_validity <- sapply(validity_summary, function(x) x$mean_validity)

message(sprintf("Contrast with highest reliability: %s (%d/%d rows > %.2f)", 
                contrasts[[which.max(all_high_validity)]], 
                max(all_high_validity), total_rows, validity_threshold))
message(sprintf("Contrast with lowest reliability: %s (%d/%d rows > %.2f)", 
                contrasts[[which.min(all_high_validity)]], 
                min(all_high_validity), total_rows, validity_threshold))

# Check if all contrasts are highly reliable
if(all(all_high_validity == total_rows)) {
  message("✅ EXCELLENT: All contrasts have high validity (>95%) across all parameter combinations!")
} else {
  problematic_contrasts <- names(contrasts)[all_high_validity < total_rows]
  message(sprintf("⚠️  WARNING: %d contrast(s) have some parameter combinations with low validity:", 
                  length(problematic_contrasts)))
  for(contrast in problematic_contrasts) {
    message(sprintf("   • %s: %d/%d rows with validity ≤ %.2f", 
                    contrasts[[contrast]], 
                    total_rows - all_high_validity[[contrast]], 
                    total_rows, validity_threshold))
  }
}

message("\n" , paste(rep("=", 60), collapse=""))
```

### Simulate study

```{r sample_study, message=FALSE, warning=FALSE}
# ---- 1. Load / install required packages ----
required_pkgs <- c("tidyverse", "MASS", "compositions", "extraDistr")
for (p in required_pkgs) {
  if (!requireNamespace(p, quietly = TRUE)) {
    install.packages(p, repos = "https://cloud.r-project.org")
  }
  library(p, character.only = TRUE)
}

# ---- 2. Helper conversion functions (define if absent) ----
if (!exists("comp_to_ilr", mode = "function")) {
  comp_to_ilr <- function(x_min) {
    stopifnot(is.matrix(x_min), ncol(x_min) == 3)
    bad_row <- !is.finite(rowSums(x_min)) | rowSums(x_min) <= 0
    if (any(bad_row)) {
      x_min[bad_row, ] <- matrix(rep(c(600, 480, 360), each = sum(bad_row)),
                                 ncol = 3, byrow = TRUE)
    }
    x_min[x_min <= 0 | !is.finite(x_min)] <- 1e-6
    compositions::ilr(sweep(x_min, 1, rowSums(x_min), "/"))
  }
}

if (!exists("ilr_to_minutes", mode = "function")) {
  ilr_to_minutes <- function(ilr_mat, total = 1440) {
    stopifnot(is.matrix(ilr_mat), ncol(ilr_mat) == 2)
    prop <- as.matrix(compositions::ilrInv(ilr_mat))
    bad <- apply(prop, 1, function(r) any(!is.finite(r) | r <= 0) ||
                                   !is.finite(sum(r)) || abs(sum(r) - 1) > 1e-8)
    if (any(bad)) prop[bad, ] <- 1/3
    round(prop * total, 1)
  }
}

# ---- 3. Data-generation function with compliance modelling ----
generate_data <- function(n_pg, effect_min, baseline_days, intervention_days,
                          s_between, s_within, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  N   <- n_pg * 2
  grp <- rep(0:1, each = n_pg)          # 0 = Control, 1 = Intervention

  # Mean daily compositions (sedentary, sleep, physical)
  base_comp   <- c(600, 480, 360)
  active_comp <- c(600 - effect_min, 480, 360 + effect_min)

  # Person-level random effects in ILR space
  b_ilr <- MASS::mvrnorm(N, mu = c(0, 0), Sigma = diag(s_between^2, 2))

  # Person-specific playtime proportion of sedentary time (10–40%)
  personal_play_prop <- rbeta(N, 2, 5) * 0.3 + 0.1

  # Person-specific compliance (intervention only)
  personal_compliance <- ifelse(
    grp == 0, 1,
    rbeta(N, 2, 2) * 0.35 + 0.6
  )

  # Helper: daily compliance variation
  daily_compliance_variation <- function(n_days, base_compliance) {
    pmax(0, pmin(1, base_compliance * (0.8 + 0.4 *
           extraDistr::rkumar(n_days, a = 0.05, b = 0.1))))
  }

  # Storage
  all_ids <- all_periods <- all_days <- NULL
  all_ilr <- matrix(0, 0, 2)
  playmin <- numeric()

  # Loop over people & periods
  for (i in seq_len(N)) {
    for (period in c("baseline", "intervention")) {
      ndays   <- if (period == "baseline") baseline_days else intervention_days
      comp_mu <- if (period == "baseline" || grp[i] == 0) base_comp else active_comp

      comp_ilr <- comp_to_ilr(matrix(rep(comp_mu, ndays), ncol = 3, byrow = TRUE))
      comp_ilr <- sweep(comp_ilr, 2, b_ilr[i, ], "+")
      day_ilr  <- comp_ilr + MASS::mvrnorm(ndays, mu = c(0, 0),
                                           Sigma = diag(s_within^2, 2))
      all_ids     <- c(all_ids, rep(i, ndays))
      all_periods <- c(all_periods, rep(period, ndays))
      all_days    <- c(all_days,
                       if (period == "baseline") seq_len(baseline_days)
                       else baseline_days + seq_len(intervention_days))
      all_ilr     <- rbind(all_ilr, day_ilr)
    }
  }

  # Convert ILR → minutes
  mins <- ilr_to_minutes(all_ilr)
  colnames(mins) <- c("sedentary", "sleep", "physical")

  # Prepare daily compliance per participant
  daily_compliance <- lapply(seq_len(N), function(pid) {
    if (grp[pid] == 0) rep(1, intervention_days)
    else daily_compliance_variation(intervention_days, personal_compliance[pid])
  })

  intervention_day_counter <- rep(0, N)
  for (row in seq_along(all_ids)) {
    pid   <- all_ids[row]
    period <- all_periods[row]

    base_playtime   <- personal_play_prop[pid] * mins[row, "sedentary"]
    noisy_playtime  <- rnorm(1, base_playtime, 0.02 * base_playtime)

    if (period == "intervention" && grp[pid] == 1) {
      intervention_day_counter[pid] <- intervention_day_counter[pid] + 1
      daily_c <- daily_compliance[[pid]][intervention_day_counter[pid]]
      playmin[row] <- pmax(0, noisy_playtime - effect_min * daily_c)
    } else {
      playmin[row] <- pmax(0, noisy_playtime)
    }
  }

  # Align daily compliance to rows
  intervention_day_counter <- rep(0, N)
  daily_comp_rows <- mapply(function(pid, per) {
    if (per == "intervention" && grp[pid] == 1) {
      intervention_day_counter[pid] <<- intervention_day_counter[pid] + 1
      daily_compliance[[pid]][intervention_day_counter[pid]]
    } else personal_compliance[pid]
  }, all_ids, all_periods)

  tibble(
    id         = factor(all_ids),
    group      = factor(grp[all_ids], labels = c("Control", "Abstinence")),
    period     = factor(all_periods, levels = c("baseline", "intervention")),
    day        = all_days,
    sedentary  = mins[, 1],
    sleep      = mins[, 2],
    physical   = mins[, 3],
    playtime   = playmin,
    compliance = daily_comp_rows
  ) %>%
    group_by(id) %>%
    mutate(
      base_play_mean     = mean(playtime[period == "baseline"]),
      playtime_reduction = base_play_mean - playtime,
      intended_reduction = if_else(group == "Abstinence" & period == "intervention",
                                   effect_min, 0),
      actual_compliance  = if_else(intended_reduction > 0,
                                   pmin(1, playtime_reduction / intended_reduction),
                                   compliance)
    ) %>%
    ungroup()
}

# ---- 4. Run a demo simulation ----
set.seed(123)
sample_data <- generate_data(
  n_pg             = 40,
  effect_min       = 60,
  baseline_days    = 7,
  intervention_days = 14,
  s_between        = 0.3,
  s_within         = 0.2
)

glimpse(sample_data)

# Quick compliance summary
sample_data %>%
  filter(group == "Abstinence", period == "intervention") %>%
  distinct(id, compliance) %>%
  summarise(
    n               = n(),
    mean_compliance = mean(compliance),
    sd_compliance   = sd(compliance),
    min_compliance  = min(compliance),
    max_compliance  = max(compliance)
  )
```

```{r longitudinal_plots}
# Calculate daily means for compositions across all participants by group and period
daily_composition_means <- sample_data %>%
  group_by(group, period, day) %>%
  summarise(
    sedentary_mean = mean(sedentary, na.rm = TRUE),
    sleep_mean = mean(sleep, na.rm = TRUE),
    physical_mean = mean(physical, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_longer(
    cols = c(sedentary_mean, sleep_mean, physical_mean),
    names_to = "component",
    values_to = "minutes"
  ) %>%
  mutate(
    component = factor(
      gsub("_mean", "", component),
      levels = c("sedentary", "sleep", "physical"),
      labels = c("Sedentary", "Sleep", "Physical Activity")
    )
  )

# Longitudinal compositional barplot
composition_longitudinal <- ggplot(daily_composition_means, 
                                  aes(x = day, y = minutes, fill = component)) +
  geom_col(position = "stack", alpha = 0.8) +
  geom_vline(xintercept = 7.5, linetype = "dashed", color = "black", size = 1) +
  annotate("text", x = 4, y = 1300, label = "Baseline", hjust = 0.5, size = 4, fontface = "bold") +
  annotate("text", x = 14, y = 1300, label = "Intervention", hjust = 0.5, size = 4, fontface = "bold") +
  facet_wrap(~ group, 
             labeller = labeller(
               group = c("Control" = "Control Group", "Abstinence" = "Abstinence Group")
             )) +
  labs(
    title = "Daily Mean Compositions Over Time",
    subtitle = "24-hour time use patterns by study day (stacked bars)",
    x = "Study Day",
    y = "Minutes per Day",
    fill = "Component"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    strip.text = element_text(size = 12, face = "bold"),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.x = element_blank()
  ) +
  scale_fill_viridis_d(option = "plasma", begin = 0.2, end = 0.8) +
  scale_y_continuous(breaks = seq(0, 1400, 200)) +
  scale_x_continuous(breaks = seq(0, 25, 5))

print(composition_longitudinal)

# Calculate daily means for playtime by group and period
daily_playtime_means <- sample_data %>%
  group_by(group, period, day) %>%
  summarise(
    playtime_mean = mean(playtime, na.rm = TRUE),
    playtime_se = sd(playtime, na.rm = TRUE) / sqrt(sum(!is.na(playtime))),
    .groups = "drop"
  )

# Longitudinal plot for daily mean playtime - now faceted by group like the composition plot
playtime_longitudinal <- ggplot(daily_playtime_means, 
                               aes(x = day, y = playtime_mean, color = group)) +
  geom_line(size = 1.2, alpha = 0.8) +
  geom_point(size = 2, alpha = 0.7) +
  geom_ribbon(aes(ymin = playtime_mean - playtime_se, 
                  ymax = playtime_mean + playtime_se, 
                  fill = group), 
              alpha = 0.2, color = NA) +
  geom_vline(xintercept = 7.5, linetype = "dashed", color = "black", size = 1) +
  annotate("text", x = 4, y = 175, label = "Baseline", hjust = 0.5, size = 4, fontface = "bold") +
  annotate("text", x = 14, y = 175, label = "Intervention", hjust = 0.5, size = 4, fontface = "bold") +
  facet_wrap(~ group, 
             labeller = labeller(
               group = c("Control" = "Control Group", "Abstinence" = "Abstinence Group")
             )) +
  labs(
    title = "Daily Mean Playtime Over Time",
    subtitle = "Gaming/screen time trends by study day with standard error bands",
    x = "Study Day",
    y = "Mean Playtime (minutes per day)",
    color = "Group",
    fill = "Group"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    strip.text = element_text(size = 12, face = "bold"),
    panel.grid.minor.x = element_blank()
  ) +
  scale_color_manual(values = c("Control" = "#2E8B57", "Abstinence" = "#FF6347")) +
  scale_fill_manual(values = c("Control" = "#2E8B57", "Abstinence" = "#FF6347")) +
  scale_y_continuous(breaks = seq(0, 200, 25)) +
  scale_x_continuous(breaks = seq(0, 25, 5))

print(playtime_longitudinal)
```

```{r composition_plots}
# Calculate daily averages for compositions by group and period
composition_summary <- sample_data %>%
  group_by(group, period, id) %>%
  summarise(
    sedentary = mean(sedentary),
    sleep = mean(sleep), 
    physical = mean(physical),
    .groups = "drop"
  ) %>%
  pivot_longer(
    cols = c(sedentary, sleep, physical),
    names_to = "component",
    values_to = "minutes"
  ) %>%
  mutate(
    component = factor(component, 
                      levels = c("sedentary", "sleep", "physical"),
                      labels = c("Sedentary", "Sleep", "Physical Activity"))
  )

# Create 2x2 faceted plot for compositions
composition_plot <- ggplot(composition_summary, 
                          aes(x = component, y = minutes, fill = component)) +
  geom_boxplot(alpha = 0.7) +
  geom_point(position = position_jitter(width = 0.2), alpha = 0.3, size = 0.8) +
  facet_grid(period ~ group, 
             labeller = labeller(
               period = c("baseline" = "Baseline", "intervention" = "Intervention"),
               group = c("Control" = "Control Group", "Abstinence" = "Abstinence Group")
             )) +
  labs(
    title = "Daily Average Compositions by Group and Period",
    subtitle = "24-hour time use patterns (minutes per day)",
    x = "Activity Component",
    y = "Minutes per Day",
    fill = "Component"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    strip.text = element_text(size = 12, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  scale_fill_viridis_d(option = "plasma", begin = 0.2, end = 0.8) +
  scale_y_continuous(breaks = seq(0, 800, 100))

print(composition_plot)
```

```{r playtime_plots}
# Calculate playtime averages by group and period  
playtime_summary <- sample_data %>%
  group_by(group, period, id) %>%
  summarise(
    avg_playtime = mean(playtime),
    .groups = "drop"
  )

# Create 2x2 faceted plot for playtime
playtime_plot <- ggplot(playtime_summary, 
                       aes(x = group, y = avg_playtime, fill = group)) +
  geom_boxplot(alpha = 0.7, width = 0.6) +
  geom_point(position = position_jitter(width = 0.2), alpha = 0.5, size = 1.2) +
  stat_summary(fun = mean, geom = "point", shape = 23, size = 3, 
               fill = "white", color = "black") +
  facet_wrap(~ period, 
             labeller = labeller(
               period = c("baseline" = "Baseline Period", 
                         "intervention" = "Intervention Period")
             )) +
  labs(
    title = "Average Daily Playtime by Group and Period",
    subtitle = "Gaming/screen time reduction intervention effect",
    x = "Study Group",
    y = "Average Playtime (minutes per day)",
    fill = "Group"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    strip.text = element_text(size = 12, face = "bold"),
    panel.grid.minor = element_blank()
  ) +
  scale_fill_manual(values = c("Control" = "#2E8B57", "Intervention" = "#FF6347")) +
  scale_y_continuous(breaks = seq(0, 200, 25))

print(playtime_plot)

# Summary statistics table
playtime_stats <- playtime_summary %>%
  group_by(group, period) %>%
  summarise(
    n = n(),
    mean_playtime = round(mean(avg_playtime), 1),
    sd_playtime = round(sd(avg_playtime), 1),
    median_playtime = round(median(avg_playtime), 1),
    q25 = round(quantile(avg_playtime, 0.25), 1),
    q75 = round(quantile(avg_playtime, 0.75), 1),
    .groups = "drop"
  )

# Display summary table
knitr::kable(
  playtime_stats,
  caption = "Summary Statistics for Daily Playtime by Group and Period",
  col.names = c("Group", "Period", "N", "Mean", "SD", "Median", "Q25", "Q75")
)
```

## Power analysis 

```{r power_analysis_extended, eval=TRUE}
# Load the results from the SimEngine power analysis
load("scripts/sim_comp_debug/power_sim_results_20250906_224427.RData")

# Get the power summary data frame
power_results_multi <- result$power_summary

# Filter rows where all columns starting with "valid" have values >= 0.95
valid_cols <- grep("^valid", names(power_results_multi), value = TRUE)
power_results_multi <- power_results_multi %>%
  filter(if_all(all_of(valid_cols), ~ .x >= 0.95))

# Add a factor for effect size for better plotting
power_results_multi$effect_min_factor <- factor(
  power_results_multi$effect_min,
  levels = unique(power_results_multi$effect_min),
  labels = paste0(unique(power_results_multi$effect_min), " min")
)

# Convert from wide to long format to plot all power metrics
power_results_long <- power_results_multi %>%
  pivot_longer(
    cols = c("power_interaction", "power_protocol_robust"),
    names_to = "power_type",
    values_to = "power"
  ) %>%
  mutate(
    power_type = factor(
      power_type,
      levels = c("power_interaction", "power_protocol_robust"),
      labels = c("Intention to Treat Effect", "Per-Protocol (Robust)")
    )
  )

# Create a faceted plot showing all power curves
ggplot(power_results_long, 
       aes(x = s_within, y = power, 
           color = effect_min_factor,
           alpha = factor(s_between),
           group = interaction(effect_min_factor, s_between))) +
  geom_line(size = 1.2) +
  geom_point(size = 1.5) +
  facet_wrap(~ power_type, ncol = 2) +
  labs(title = "Power Curves by Effect Size and Variability Parameters",
       subtitle = "Each line represents a unique Effect Size × Between-subject SD combination",
       x = "Within-subject SD", 
       y = "Statistical Power",
       color = "Effect Size",
       alpha = "Between-subject SD") +
  theme_minimal() +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "red", alpha = 0.7) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_reverse() +
  scale_alpha_discrete(range = c(0.4, 1.0)) +
  theme(legend.position = "bottom") +
  guides(
    color = guide_legend(title = "Effect Size", nrow = 1, order = 1),
    alpha = guide_legend(title = "Between-subject SD", nrow = 1, order = 2)
  )

# Create a condensed comparison plot focusing on highest within-subject SD
highest_s_within <- max(power_results_long$s_within)
power_results_highest_sd <- power_results_long %>%
  filter(s_within == highest_s_within)

# Calculate summary statistics across between-subject SD values for ribbon
power_summary_ribbon <- power_results_highest_sd %>%
  group_by(effect_min, power_type) %>%
  summarise(
    mean_power = mean(power, na.rm = TRUE),
    min_power = min(power, na.rm = TRUE),
    max_power = max(power, na.rm = TRUE),
    .groups = "drop"
  )

ggplot(power_summary_ribbon, 
       aes(x = effect_min, y = mean_power, 
           color = power_type,
           fill = power_type,
           linetype = power_type)) +
  geom_ribbon(aes(ymin = min_power, ymax = max_power), alpha = 0.2, color = NA) +
  geom_line(size = 1.2) +
  geom_point(size = 2) +
  labs(title = paste("Comparison of Power by Effect Type (Highest Within-Subject SD =", highest_s_within, ")"),
       subtitle = "Lines show mean power; ribbons show range across Between-subject SD values",
       x = "Effect Size (minutes)", 
       y = "Statistical Power",
       color = "Effect Type",
       fill = "Effect Type",
       linetype = "Effect Type") +
  theme_minimal() +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "black") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2))



power_protocol_robust_heatmap <- power_results_multi %>%
  ggplot(aes(x = s_within, y = s_between, fill = power_protocol_robust)) +
  geom_tile() +
  facet_wrap(~ effect_min_factor) +
  scale_fill_viridis_c(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  labs(title = "Power Heatmap for Per-Protocol Effect (Robust)",
       x = "Within-subject SD",
       y = "Between-subject SD",
       fill = "Power") +
  theme_minimal() +
  theme(legend.position = "right")

# Display the heatmap
power_protocol_robust_heatmap

# Create a heatmap visualization showing power for different parameter combinations
# Focus on the interaction effect as it's typically the primary outcome
power_interaction_heatmap <- power_results_multi %>%
  ggplot(aes(x = s_within, y = s_between, fill = power_interaction)) +
  geom_tile() +
  facet_wrap(~ effect_min_factor) +
  scale_fill_viridis_c(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  labs(title = "Power Heatmap for Intention to Treat Effect",
       x = "Within-subject SD",
       y = "Between-subject SD",
       fill = "Power") +
  theme_minimal() +
  theme(legend.position = "right")

# Display the heatmap
power_interaction_heatmap

# Create a table summarizing the minimum effect size needed for 80% power
# under different variability conditions
power_threshold <- 0.8
power_summary_table <- power_results_multi %>%
  group_by(s_between, s_within) %>%
  summarize(
    min_effect_for_interaction = min(effect_min[power_interaction >= power_threshold], na.rm = TRUE),
    min_effect_for_protocol_change = min(effect_min[power_protocol_change >= power_threshold], na.rm = TRUE),
    min_effect_for_protocol_robust = min(effect_min[power_protocol_robust >= power_threshold], na.rm = TRUE),
    .groups = "drop"
  )

# Display the summary table
library(DT)

datatable(
  power_summary_table,
  colnames = c(
    "Between-subject SD", "Within-subject SD", 
    "Min Effect for 80% Int to Treat Power (min)", 
    "Min Effect for 80% Protocol Power (Change) (min)",
    "Min Effect for 80% Protocol Power (Robust) (min)"
  ),
  caption = "Minimum effect size needed for 80% power under different variability conditions",
  options = list(
    pageLength = 10,
    autoWidth = TRUE,
    searching = TRUE,
    ordering = TRUE
  )
)
```


## Minimum Detectable Effect Sizes at 80% Power

This section analyzes the minimum detectable effect sizes (MDEs) at 80% power for different models and variability parameters, helping identify optimal study design configurations.

### ITT Effects - Minimum Detectable Effect Sizes

```{r}
#| label: itt-power-table
#| code-summary: "Show code (ITT minimum detectable effects table)"

# Function to interpolate minimum detectable effect at 80% power
find_mde_80 <- function(power_data) {
  # If we already have 80% power or higher at the smallest effect, return that
  if (min(power_data$power_interaction, na.rm = TRUE) >= 0.8) {
    return(min(power_data$effect_min, na.rm = TRUE))
  }
  
  # If we never reach 80% power, return NA
  if (max(power_data$power_interaction, na.rm = TRUE) < 0.8) {
    return(NA_real_)
  }
  
  # Linear interpolation to find effect size at 80% power
  approx(x = power_data$power_interaction, y = power_data$effect_min, xout = 0.8)$y
}

# Calculate minimum detectable effects for ITT across different variability settings
mde_table_itt <- power_results_multi %>%
  group_by(s_between, s_within) %>%
  summarise(
    mde_80_itt = find_mde_80(cur_data()),
    max_power_itt = max(power_interaction, na.rm = TRUE),
    mean_power_itt = mean(power_interaction, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(s_between, s_within) %>%
  mutate(
    # Format effect sizes for display
    mde_80_itt_fmt = ifelse(is.na(mde_80_itt), 
                           paste0(">", max(power_results_multi$effect_min)), 
                           sprintf("%.0f", mde_80_itt)),
    max_power_itt_fmt = sprintf("%.1f%%", max_power_itt * 100),
    mean_power_itt_fmt = sprintf("%.1f%%", mean_power_itt * 100),
    # Create combined variability label for easier interpretation
    variability_profile = case_when(
      s_between <= 0.15 & s_within <= 0.20 ~ "Low variability (consistent participants)",
      s_between <= 0.15 & s_within > 0.20 ~ "Low between, high within (stable people, variable days)",
      s_between > 0.15 & s_within <= 0.20 ~ "High between, low within (different people, stable days)",
      TRUE ~ "High variability (variable participants & days)"
    )
  )

# Create formatted table for ITT effects
library(kableExtra)

mde_table_itt %>%
  dplyr::select(
    `Between-Subject SD` = s_between,
    `Within-Subject SD` = s_within,
    `Variability Profile` = variability_profile,
    `MDE at 80% Power (min)` = mde_80_itt_fmt,
    `Mean Power` = mean_power_itt_fmt,
    `Max Power Achieved` = max_power_itt_fmt
  ) %>%
  kable(
    caption = "Minimum Detectable Effect Sizes at 80% Power - ITT (Intention-to-Treat) Effects",
    align = c("c", "c", "l", "c", "c", "c")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = TRUE,
    position = "left"
  ) %>%
  pack_rows("Low Between-Subject Variability (0.10-0.15)", 1, sum(mde_table_itt$s_between <= 0.15)) %>%
  pack_rows("High Between-Subject Variability (0.20-0.30)", sum(mde_table_itt$s_between <= 0.15) + 1, nrow(mde_table_itt)) %>%
  footnote(
    general = c(
      "MDE = Minimum Detectable Effect size at 80% power (minutes of sedentary time reduction)",
      "Between-Subject SD reflects variability between different participants",
      "Within-Subject SD reflects day-to-day variability within the same participant",
      "Values >120 indicate the model did not achieve 80% power at the largest tested effect size"
    ),
    general_title = "Notes:"
  )

```

### Per-Protocol Effects - Minimum Detectable Effect Sizes

```{r}
#| label: protocol-power-table
#| code-summary: "Show code (per-protocol minimum detectable effects table)"

# Function for per-protocol MDE calculation
find_mde_80_protocol <- function(power_data, power_col) {
  power_values <- power_data[[power_col]]
  # If we already have 80% power or higher at the smallest effect, return that
  if (min(power_values, na.rm = TRUE) >= 0.8) {
    return(min(power_data$effect_min, na.rm = TRUE))
  }
  
  # If we never reach 80% power, return NA
  if (max(power_values, na.rm = TRUE) < 0.8) {
    return(NA_real_)
  }
  
  # Linear interpolation to find effect size at 80% power
  approx(x = power_values, y = power_data$effect_min, xout = 0.8)$y
}

# Calculate minimum detectable effects for both per-protocol approaches
mde_table_protocol <- power_results_multi %>%
  group_by(s_between, s_within) %>%
  summarise(
    mde_80_change = find_mde_80_protocol(cur_data(), "power_protocol_change"),
    mde_80_robust = find_mde_80_protocol(cur_data(), "power_protocol_robust"),
    max_power_change = max(power_protocol_change, na.rm = TRUE),
    max_power_robust = max(power_protocol_robust, na.rm = TRUE),
    mean_power_change = mean(power_protocol_change, na.rm = TRUE),
    mean_power_robust = mean(power_protocol_robust, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(s_between, s_within) %>%
  mutate(
    # Format effect sizes for display
    mde_80_change_fmt = ifelse(is.na(mde_80_change), 
                              paste0(">", max(power_results_multi$effect_min)), 
                              sprintf("%.0f", mde_80_change)),
    mde_80_robust_fmt = ifelse(is.na(mde_80_robust), 
                              paste0(">", max(power_results_multi$effect_min)), 
                              sprintf("%.0f", mde_80_robust)),
    max_power_change_fmt = sprintf("%.1f%%", max_power_change * 100),
    max_power_robust_fmt = sprintf("%.1f%%", max_power_robust * 100),
    mean_power_change_fmt = sprintf("%.1f%%", mean_power_change * 100),
    mean_power_robust_fmt = sprintf("%.1f%%", mean_power_robust * 100),
    # Variability profile
    variability_profile = case_when(
      s_between <= 0.15 & s_within <= 0.20 ~ "Low variability",
      s_between <= 0.15 & s_within > 0.20 ~ "Low between, high within",
      s_between > 0.15 & s_within <= 0.20 ~ "High between, low within",
      TRUE ~ "High variability"
    )
  )

# Create side-by-side comparison table for per-protocol approaches
comparison_table <- mde_table_protocol %>%
  dplyr::select(
    `Between SD` = s_between,
    `Within SD` = s_within,
    `Profile` = variability_profile,
    `Change Score MDE` = mde_80_change_fmt,
    `Change Max Power` = max_power_change_fmt,
    `Robust MDE` = mde_80_robust_fmt,
    `Robust Max Power` = max_power_robust_fmt
  )

comparison_table %>%
  kable(
    caption = "Per-Protocol Minimum Detectable Effect Sizes at 80% Power - Change Score vs Robust Approach",
    align = c("c", "c", "l", "c", "c", "c", "c")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE,
    position = "left"
  ) %>%
  add_header_above(c(" " = 3, "Change Score Approach" = 2, "Robust Approach" = 2)) %>%
  pack_rows("Low Between-Subject Variability", 1, sum(mde_table_protocol$s_between <= 0.15)) %>%
  pack_rows("High Between-Subject Variability", sum(mde_table_protocol$s_between <= 0.15) + 1, nrow(mde_table_protocol)) %>%
  footnote(
    general = c(
      "Change Score Approach: Tests if reducing playtime improves outcomes (positive effect)",
      "Robust Approach: Tests if lower playtime during intervention improves outcomes (negative effect)", 
      "Both approaches test the same underlying hypothesis from different analytical perspectives",
      "MDE values in minutes represent the minimum sedentary time reduction detectable at 80% power"
    ),
    general_title = "Notes:"
  )

```
