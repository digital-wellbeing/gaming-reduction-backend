---
title: "H2 - Effect of Abstention on Sleep Quality"
output: html_document
execute-dir: project
format: 
  html:
    toc: true
    code-fold: true
author: "Tamás Földes"
date: today
---

## Introduction

This document demonstrates a complete workflow for generating synthetic data, introducing dropout, and fitting models (a GAM or MLM) to the data. The main focus is on the `sim_study` function, which orchestrates the data simulation, dropout process, and model fitting.

As an overview, we compare the following possible models for estimating the effect of gaming reduction, for both H2a (the intention-to-treat effect) and H2b (the per-protocol effect; i.e., the effect of actual gaming reduction relative to one's own baseline). For the per-protocol effect, we use a robust approach that avoids potential issues with change scores by directly modeling the relationship between current playtime and outcomes while controlling for baseline differences. The models we compare are:

| Model Name | Syntax | Target Effect | Notes |
|------------|----------------------|------------|---------------------------|
| GAM | `gam(sleep_quality ~ condition:intervention_period + age + gender + s(id, bs = "re") + s(day, by = condition, bs = "tp"), correlation = corAR1(form = ~ day | id))` | ITT |  |
| GAM with no main effect | `gam(sleep_quality ~         age + gender +         s(id, bs = "re") +         s(day, by = condition, bs = "tp"),       data = dat,       correlation = corCAR1(form = ~ day | id))` | ITT | Here we do not estimate a parameter for the effect of the intervention directly; rather, we simply fit separate curves to each condition and calculate the average marginal effect using {emmeans} |
| MLM | `lme(     fixed = sleep_quality ~ condition*intervention_period + age + gender,     random = ~ 1|id,     correlation = corCAR1(form = ~ day | id),     method = "ML"   )` | ITT | Multiple versions of this model failed when including random slopes; we therefore dropped these |
| MLM Simple | `lme(     fixed = sleep_quality ~ baseline + condition + age + gender,     random = ~ 1 + condition|id,     correlation = corCAR1(form = ~ day | id),     method = "ML",   )` | ITT | Here we do not model the baseline (pre-intervention period) itself—we model only the 14-day period when the intervention is active, using average sleep_quality during baseline as a covariate |
| GLS (generalized least squares) | `gls(     sleep_quality ~ condition * intervention_period + age + gender,     correlation = corCAR1(form = ~ day | id),   )` | ITT |  |
| GLS Simple | `gls(     sleep_quality ~ condition + baseline + age + gender,     correlation = corAR1(form = ~ day | id),   )` | ITT |  |
| GLS Splines | `gls(     sleep_quality ~ ns(day, df = 4) * intervention_period * condition,     correlation = corCAR1(form = ~ day | id),     data = dat   )` | ITT | In this version, we fit a GLS but allow non-linearity in the trajectory of sleep_quality using splines |
| MLM Reduction Robust | `lme(     fixed = sleep_quality ~ intervention_active*playtime + baseline_playtime + age + gender,     random = ~ 1 | id,     correlation = corCAR1(form = ~ day | id)   )` | PP | Robust approach without change scores - tests if intervention effect varies by actual playtime levels, controlling for baseline playtime |

: ITT = Intention-to-treat; PP = per-protocol

### Take-aways

Our simulations show that several models perform well at parameter recovery for the ITT effect, but that the GAM model has the highest power for small effects---the type of effects we believe we are most likely to observed---and for non-linear trajectories over the 14 day period (e.g., an effect that slowly accumulates over a couple of days and then plateaus, or a temporary withdrawal followed by a later improvement). The GAM has approximately 50% power for a standardized effect of .2, and 80% power for a standardized effect of .3, but this varies based the shape of that effect over time. 

The MLM Reduction Robust model provides a robust approach that avoids potential issues with change scores by directly modeling the relationship between current playtime and outcomes while controlling for baseline differences, and performs very well with >95% power for standardized effects of approximately .2 or greater. 

### Simulation Control

To force rerun simulations (bypassing the cache), set the `FORCE_RERUN_H2A` and/or `FORCE_RERUN_H2B` parameters to `TRUE` in the "Load Libraries" section below. This is useful when you want to:

- Test changes to the simulation code
- Rerun with different random seeds  
- Regenerate results after fixing issues
- Bypass potentially corrupted cache files

### Load Libraries

First we load packages with `pacman`, which is fully compatible with `renv`.

```{r}
#| label: load-libraries
#| code-summary: "Show code (load libraries)"

library(pacman)

p_load(tidyverse, qualtRics, lme4, mgcv, marginaleffects, broom, forestplot, broom.mixed, nlme, rms, emmeans, splines, furrr, extraDistr, kableExtra)

# ====================================================================
# SIMULATION CONTROL PARAMETERS
# ====================================================================
# Set these parameters to control simulation behavior:

# Force rerun simulations (bypassing cache)
FORCE_RERUN_H2A <- FALSE  # Set to TRUE to force rerun H2a simulations
FORCE_RERUN_H2B <- FALSE  # Set to TRUE to force rerun H2b simulations

# ====================================================================

# Replace your current plan() line with:
if (interactive()) {
  plan(multisession, workers = parallel::detectCores()-8)
} else {
  plan(multisession, workers = parallel::detectCores()-8)
}
# Diagnostic information about parallel setup

message("=== PARALLEL PROCESSING SETUP ===")
message("Total CPU cores detected: ", parallel::detectCores())
message("Number of workers allocated: ", nbrOfWorkers())
message("Future plan: ", paste(class(plan()), collapse = ", "))
message("===================================")

theme_set(theme_minimal())
theme_update(
  strip.background = element_rect(fill = "black"),
  strip.text = element_text(color = "white", size = 10),
  panel.grid.minor = element_blank(),
  panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
)

options(scipen = 999)

```

### Simulation, Dropout, and Fitting Functions

Here we define:

-   `sim_data`: Generates synthetic data with random intercepts/slopes and AR(1) errors.
-   `sim_dropout`: Introduces missingness and dropout in the dataset.
-   `fit_*`: Fits a statistical model to the simulated data (see table above)
-   `sim_study`: Ties everything together—generates data, applies dropout, then fits the chosen model and returns a tidy summary.

```{r}
#| label: sim-functions
#| code-summary: "Show code (sim functions)"

#' Generate Synthetic Data
#'
#' This function simulates synthetic panel data for `n` participants over `n_days` time points,
#' with an intervention effect, random intercepts and slopes, and AR(1)-correlated residuals.
#'
#' @param n Number of participants. Default is 80.
#' @param n_days Number of time points per participant.
#' @param b Fixed effect of condition (if mediated == FALSE) or a 1-hour reduction in playtime.
#' @param phi AR(1) autocorrelation coefficient.
#' @param sigma AR(1) residual standard deviation.

sim_data <- function(n = 80,
                     n_days = 28,
                     
                     # effect parameters
                     b = 0.8, # effect in unstandardized units (scaled for 5-25 range)
                     mu = 15, # grand mean of the outcome (center of 5-25 range)
                     
                     
                     # random effects parameters
                     tau_int = 2.5,   # Random intercept SD (between-person variance)
                     tau_slope = .05,  # Random slope SD 
                     within_person_sd = 2.0, # Within-person SD (scaled for 5-25 range)
                     
                     # AR(1) parameters
                     phi = 0.8,     # Autocorrelation coefficient
                     effect_shape = "grow",
                     k = .5, # affects how quickly the plateau effect plateaus
                     
                     mediated = FALSE,
                     
                     # playtime parameters
                     playtime_grand_mean = 1,   # Average baseline playtime in hours
                     playtime_grand_sd = .5,   # SD for baseline playtime in log units (log-normal distribution)
                     daily_play_sd = 0.5      # Daily noise in playtime
                     # compliance_mean = 0.7,    # Average reduction (in hours) for intervention group during intervention period
)     
{
  dat <- tibble(
    id = 1:n,
    age = sample(18:36, n, replace = TRUE),
    gender = sample(c("man","woman","non-binary"), n, prob = c(.45, .45, .1), replace = TRUE),
    condition = factor(sample(c("control", "intervention"), n, replace = TRUE)),
    experimental_condition = ifelse(condition == "intervention", 1, 0),
    intercept_sq = rnorm(n, 0, tau_int), # Renamed from intercept_wb
    slope_sq = rnorm(n, 0, tau_slope), # Renamed from slope_wb
    intercept_play = rlnorm(n, log(playtime_grand_mean), playtime_grand_sd),
  ) |> 
    # expand to 28 waves per id
    crossing(
      day = 1:n_days
    ) |> 
    mutate(
      intervention_period = as.numeric(day > 7 & day < 22),
      intervention_active = intervention_period & condition == "intervention",
      compliance = ifelse(intervention_active, rkumar(n*n_days, a = .05, b = .1), 0),
      
      # In the baseline period, play is just the subject's baseline plus some day-to-day noise
      # During the intervention, experimental subjects reduce play by their compliance amount
      playtime = (1 - compliance) * rlnorm(n, log(intercept_play), daily_play_sd),
      effect_time = case_when(
        effect_shape == "plateau" ~ if_else(intervention_period == 1, (b + slope_sq) * (1-exp(-k * (day - 7))), 0), # Renamed from slope_wb
        effect_shape == "grow" ~ if_else(intervention_period == 1, (day - 7) * ((b + slope_sq)/7), 0), # Renamed from slope_wb
        TRUE ~ NA_real_
      ),
    ) |> 
    group_by(id) |> 
    mutate(
      
      baseline_playtime = mean(playtime[day <= 7]),
      reduction = baseline_playtime - playtime, # The mediator: reduction in play relative to the baseline average
      sigma = within_person_sd * sqrt(1-phi^2),
      # Generate AR(1) errors for each participant
      e = as.numeric(arima.sim(n = n_days, 
                               model = list(ar = phi), 
                               sd = sigma)),
      # Add random effect + fixed effect + AR(1) error
      sleep_quality_raw = case_when( # Renamed from wellbeing
        mediated == TRUE ~ mu + 
                            intercept_sq + # Renamed from intercept_wb
                            effect_time * reduction + 
                            .01*(age-18) +
                            -.05*(gender %in% c("women","non-binary")) + 
                            e,
        mediated == FALSE ~ mu + 
                            intercept_sq + # Renamed from intercept_wb
                            effect_time * experimental_condition * intervention_period + 
                            .01*(age-18) +
                            -.05*(gender %in% c("women","non-binary")) + 
                            e
      ),
      # Constrain to 5-25 range and round to integers
      sleep_quality = round(pmax(5, pmin(25, sleep_quality_raw)))
    ) |> 
    ungroup() |> 
    mutate(across(where(is.numeric), ~ round(., 3)))
  
  dat

}
```

```{r}
#| label: sim-data-example
#| code-summary: "Show code (sim data example)"

sim_data(n = 10, n_days = 10, effect_shape = "grow")

#' Simulate Dropout
#'
#' Introduces missingness and dropout into a dataset by randomly assigning records as missing
#' or dropped out. Once a participant is dropped out, all subsequent records become missing.
#'
#' @param dat A tibble generated by \code{sim_data()}.
#'
#' @return A tibble of the same structure as \code{dat}, but with some \code{sleep_quality} values set to NA. # Renamed from wellbeing
#'
sim_dropout <- function(dat) {
  
  dropout <- dat |> 
    mutate(
      missing = sample(c(TRUE, FALSE), n(), replace = TRUE, prob = c(.10, .90)),
      dropout = sample(c(TRUE, FALSE), n(), replace = TRUE, prob = c(.01, .99))
    ) |>
    mutate(
      missing = ifelse(cumsum(dropout) > 0, TRUE, missing),
      .by = id
    ) |>
    arrange(as.integer(id), day) |> 
    mutate(sleep_quality = ifelse(missing, NA, sleep_quality)) # Renamed from wellbeing
  dropout
}

```

```{r}
#| label: fit-functions
#' Fit a Generalized Additive Model (GAM)
#'
#' Fits a GAM model to the provided dataset using \code{mgcv::gam}, including an AR(1)
#' correlation structure and random intercept for each ID.
#'
#' @param dat A tibble of repeated-measures data (e.g., from \code{sim_data()} and \code{sim_dropout()}).
#'
#' @return An object of class \code{gam}, which is the fitted GAM model.
#'
#' 
fit_gam <- function(dat) {
  
  gam(sleep_quality ~ # Renamed from wellbeing
        condition:intervention_period + age + gender +
        s(id, bs = "re") + 
        s(day, by = condition, bs = "tp"), 
      data = dat,
      correlation = corAR1(form = ~ day | id))
}

fit_gam_no_main <- function(dat) {
  
  gam(sleep_quality ~ # Renamed from wellbeing
        age + gender +
        s(id, bs = "re") + 
        s(day, by = condition, bs = "tp"), 
      data = dat,
      correlation = corCAR1(form = ~ day | id))
}

#' Fit a Multi-Level Model (MLM)
#'
#' Fits a linear mixed-effects model (LME) with random intercept for each ID using \code{lme4::lmer}.
#'
#' @param dat A tibble of repeated-measures data (e.g., from \code{sim_data()} and \code{sim_dropout()}).
#'
#' @return An object of class \code{lmerMod}, which is the fitted MLM model.
#'
fit_mlm <- function(dat) {
  # lmer(sleep_quality ~ condition*intervention_period + age + gender + (1|id), data = dat) # Renamed from wellbeing
  lme(
    fixed = sleep_quality ~ condition*intervention_period + age + gender, # Renamed from wellbeing
    random = ~ 1|id,  # or use a more flexible structure if needed
    correlation = corCAR1(form = ~ day | id),
    method = "ML",
    data = dat |> filter(!is.na(sleep_quality)) # Renamed from wellbeing
  )
}

fit_mlm_simple <- function(dat) {
  
  tmp <- dat |> 
    group_by(id) |> 
    # take the mean of days 1-7 
    mutate(baseline = mean(sleep_quality[day < 8], na.rm = TRUE)) |> # Renamed from wellbeing
    filter(intervention_period == 1) |> 
    filter(!is.na(sleep_quality)) # Renamed from wellbeing
  
  lme(
    fixed = sleep_quality ~ baseline + condition + age + gender, # Renamed from wellbeing
    random = ~ 1|id,  # Only random intercept since condition is between-subjects
    correlation = corCAR1(form = ~ day | id),
    method = "ML",
    data = tmp |> filter(!is.na(sleep_quality)) # Renamed from wellbeing
  )
}

fit_gls <- function(dat) {
  
  gls(
    sleep_quality ~ condition * intervention_period + age + gender, # Renamed from wellbeing
    correlation = corCAR1(form = ~ day | id),
    data = dat |> filter(!is.na(sleep_quality)) # Renamed from wellbeing
  )
}

fit_gls_simple <- function(dat) {
  
  tmp <- dat |> 
    group_by(id) |> 
    # take the mean of days 1-7 
    mutate(baseline = mean(sleep_quality[day < 8], na.rm = TRUE)) |> # Renamed from wellbeing
    filter(intervention_period == 1) |> 
    filter(!is.na(sleep_quality)) # Renamed from wellbeing
  
  gls(
    sleep_quality ~ condition + baseline + age + gender, # Renamed from wellbeing
    correlation = corAR1(form = ~ day | id),
    data = tmp
  )
}

fit_gls_spline <- function(dat) {
  gls(
    sleep_quality ~ ns(day, df = 4) * intervention_period * condition, # Renamed from wellbeing, corrected comma
    correlation = corCAR1(form = ~ day | id),
    data = dat
  )
}


fit_mlm_reduction_robust <- function(dat) {
  lme(
    fixed = sleep_quality ~ intervention_active*playtime + baseline_playtime + age + gender, # Renamed from wellbeing
    random = ~ 1 | id,  # Simplified to random intercept only
    correlation = corCAR1(form = ~ day | id),
    data = dat
  )
}

# Helper function to extract the focal effect for GLS models
extract_marginal_effect <- function(mod, dat, focal_term = "conditionintervention") {
  # Here we assume your GLS model is specified with condition*intervention_period
  # and you want the effect of condition (e.g., intervention vs. control) during intervention.
  # We create a reference grid that fixes intervention_period at 1.
  rg <- ref_grid(mod, data = dat, at = list(intervention_period = 1))
  
  # Obtain estimated marginal means for each condition.
  emm <- emmeans(rg, ~ condition)
  
  # Compute the pairwise contrast (e.g., intervention - control)
  # Adjust names as needed. The contrast below returns a one-row summary.
  contr <- emmeans::contrast(emm, method = list("intervention - control" = c(-1, 1)), adjust = "none")
  contr_sum <- summary(contr, infer = TRUE)
  
  # Construct a one-row data frame with consistent column names.
  # If you have more than one contrast, you might need to filter for the one of interest.
  df <- data.frame(
    term = focal_term,
    estimate = contr_sum$estimate,
    std.error = contr_sum$SE,
    conf.low = contr_sum$lower.CL,
    conf.high = contr_sum$upper.CL,
    row.names = NULL
  )
  
  return(df)
}
```
```{r}
#' Simulation Study Orchestrator
#'
#' A higher-level function that ties together data simulation, dropout, and model fitting,
#' returning a tidy summary of the fitted model parameters.
#'
#' @param model_function A function to fit the model. Defaults to \code{fit_gam}.
#' @param n Number of participants passed to \code{sim_data()}. Uses sim_data default (80).
#' @param n_days Number of time points per participant passed to \code{sim_data()}. Default is 28.
#' @param b Fixed effect for the intervention slope passed to \code{sim_data()}. Default is 0.01.
#' @param phi AR(1) autocorrelation coefficient passed to \code{sim_data()}. Default is 0.7.
#' @param sigma AR(1) residual standard deviation passed to \code{sim_data()}. Default is 0.6.
#'
#' @return A data frame (tibble) of model estimates from \code{broom::tidy(parametric = TRUE)}.
#'

# Updated simulation orchestrator that handles GLS models separately.
sim_study <- function(model = "fit_gam", focal_term = "intervention_activeTRUE:reduction", ...) {
  args <- list(...)
  dat <- do.call(sim_data, args)
  model_function <- get(model)
  
  mod <- model_function(dat)
  
  if (model %in% c("fit_gam_no_main","fit_gls_spline")) {
    # Extract the effect using our helper function.
    result <- suppressMessages(extract_marginal_effect(mod, 
                                                       dat, 
                                                       focal_term = focal_term))
  } else {
    # For models that work with broom, extract the focal parameter.
    # Adjust the filtering term as needed.
    result <- broom::tidy(mod, parametric = TRUE) |>
      filter(term == focal_term) |> 
      # filter(
      #   term == "conditionintervention:intervention_period" | 
      #     (model %in% c("fit_mlm_simple","fit_gls_simple") & term == "conditionintervention")
      # ) |> 
      mutate(
        conf.low = estimate - 1.96 * std.error,
        conf.high = estimate + 1.96 * std.error
      )
  }
  result
}
```

## Test and Plot One Simulated Study

Below, we create a sample dataset using `sim_data()` and examine it with a line plots by day. We can also see whether the simulated SDs for wellbeing align with the target values in the simulation---luckily, they do.

```{r}
#| label: test-plot-descriptive
#| code-summary: "Show code (descriptive plotting)"

dat <- sim_data(effect_shape = "plateau", mediated = TRUE)

sds <- dat |> 
  group_by(id) |> 
  summarise(mean_value = mean(sleep_quality, na.rm = TRUE), # Renamed from wellbeing
            sd_within  = sd(sleep_quality, na.rm = TRUE)) |> # Renamed from wellbeing
  summarise(between_sd   = sd(mean_value, na.rm = TRUE),
            avg_within_sd = mean(sd_within, na.rm = TRUE))

# plot sleep_quality by group
dat |> 
  group_by(condition, day) |> 
  summarise(sleep_quality = mean(sleep_quality)) |> # Renamed from wellbeing
  ggplot(aes(y = sleep_quality, x = day, color = condition)) + # Renamed from wellbeing
  geom_line() + 
  theme_minimal()

```

### Test Fit (h2a - intention to treat)

We fit various models to the newly simulated data to make sure each appears to be working properly, and also test the full `sim_study` pipeline.

```{r}
#| label: test-fit-h2a
#| eval: false
#| code-summary: "Show code (test fit h2a)"

dat <- sim_data(mediated = FALSE)

fit_mlm(dat) |> summary()
fit_mlm_simple(dat) |> summary()
fit_gls(dat) |> summary()
fit_gls_simple(dat) |> summary()
fit_gls_spline(dat) |> extract_marginal_effect()
fit_gam(dat) |> summary()
fit_gam_no_main(dat) |> extract_marginal_effect()

sim_study(model = "fit_gam_no_main")
sim_study(model = "fit_gls_spline")

```

Since some models (e.g., `fit_gam_no_main`) do not have a parameter that represents the average difference-in-difference between groups during the intervention period, we need to calculate this ourselves by marginalize across the 14-day intervention period.

```{r}
#| label: test-emmeans
#| eval: false
#| code-summary: "Show code (test emmeans)"

emm_day <- emmeans(
  fit_gls_spline(dat), 
  pairwise ~ condition | day, 
  at = list(day = 8:21), 
  condition = c("control", "intervention"), 
  data = dat |> mutate(condition = factor(condition, levels = c("intervention", "control")))
)

summary(emm_day$contrasts, infer = TRUE, level = .95, by = NULL, adjust = "none")

# and then integrated over the 14 day intervention period
rg <- ref_grid(fit_gls_spline(dat),
               at = list(intervention_period = 1),
               cov.reduce = list(day = mean),
               data = dat |> mutate(condition = factor(condition, levels = c("control","intervention"))))

emm <- emmeans(rg, ~ condition)
(contrast_result <- contrast(emm, method = list("intervention - control" = c(-1, 1)), adjust = "none"))


means <- summary(emm)$emmean
names(means) <- summary(emm)$condition
(diff_manual <- means["intervention"] - means["control"])
```

### Test Fit (h2b - per-protocol)

A quick test of our `fit_mlm_reduction_robust` model, to make sure the simulation whereby the effect of the intervention is mediated by a reduction in playtime is functioning properly.

```{r}
#| label: test-fit-h2b
#| eval: false
#| code-summary: "Show code (test fit h2b)"

dat <- sim_data(mediated = TRUE)

fit_mlm_reduction_robust(dat) |> summary()

```

## Simulated h2a power analysis

To assess power/sensitivity, we run multiple simulations (controlled by `n_sims`) and gather the parameter estimates for a particular term (e.g., `conditionintervention:intervention_periodTRUE`, or for our marginalized effect `conditionintervention`). Each iteration calls `sim_study`, which does the data generation, dropout, and fitting.

As this is quite slow, we both use parallel processing with `furrr` cache the results.

```{r}
#| label: sim-study-h2a
#| cache: true
#' Simulation Study Orchestrator
#'
#' A higher-level function that ties together data simulation, dropout, and model fitting,
#' returning a tidy summary of the fitted model parameters.
#'
#' @param model_function A function to fit the model. Defaults to \code{fit_gam}.
#' @param n Number of participants passed to \code{sim_data()}. Uses sim_data default (80).
#' @param n_days Number of time points per participant passed to \code{sim_data()}. Default is 28.
#' @param b Fixed effect for the intervention slope passed to \code{sim_data()}. Default is 0.01.
#' @param phi AR(1) autocorrelation coefficient passed to \code{sim_data()}. Default is 0.7.
#' @param sigma AR(1) residual standard deviation passed to \code{sim_data()}. Default is 0.6.
#'
#' @return A data frame (tibble) of model estimates from \code{broom::tidy(parametric = TRUE)}.
#'

# Updated simulation orchestrator that handles GLS models separately.
sim_study <- function(model = "fit_gam", focal_term = "intervention_activeTRUE:reduction", ...) {
  args <- list(...)
  dat <- do.call(sim_data, args)
  model_function <- get(model)
  
  mod <- model_function(dat)
  
  if (model %in% c("fit_gam_no_main","fit_gls_spline")) {
    # Extract the effect using our helper function.
    result <- suppressMessages(extract_marginal_effect(mod, 
                                                       dat, 
                                                       focal_term = focal_term))
  } else {
    # For models that work with broom, extract the focal parameter.
    # Adjust the filtering term as needed.
    result <- broom::tidy(mod, parametric = TRUE) |>
      filter(term == focal_term) |> 
      # filter(
      #   term == "conditionintervention:intervention_period" | 
      #     (model %in% c("fit_mlm_simple","fit_gls_simple") & term == "conditionintervention")
      # ) |> 
      mutate(
        conf.low = estimate - 1.96 * std.error,
        conf.high = estimate + 1.96 * std.error
      )
  }
  result
}

# Load tictoc for timing if not already loaded
if (!require(tictoc, quietly = TRUE)) {
  install.packages("tictoc")
  library(tictoc)
}

# Check if cached results exist
cache_file_h2a <- "cache/h2a_simulation_results.rds"
cache_summary_h2a <- "cache/h2a_simulation_summary.rds"
cache_params_h2a <- "cache/h2a_simulation_params.rds"

# Simulation control parameters (using global settings)
force_rerun_h2a <- FORCE_RERUN_H2A

# Define key parameters that would invalidate cache
current_params_h2a <- list(
  n_sims = 500,
  n = 80,
  n_days = 28,
  tau_int = 2.5,
  tau_slope = 0.8,
  within_person_sd = 2.5,
  phi = 0.7,
  models = c("fit_gam", "fit_gam_no_main", "fit_mlm", "fit_mlm_simple", "fit_gls", "fit_gls_simple", "fit_gls_spline"),
  effect_values = c(0.3, 0.6, 0.9, 1.2, 1.5),
  effect_shapes = c("grow", "plateau"),
  mediated = FALSE
)

# -------------------------------------------------------------------
# Prepare job specifications irrespective of cache status
# This ensures that variables such as `specs_h2a`, `all_jobs_h2a`, and
# related helpers are available even when we load results from cache,
# avoiding downstream errors (e.g., `all_jobs_h2a` not found).
n_sims <- current_params_h2a$n_sims

specs_h2a <- expand_grid(
  model = current_params_h2a$models,
  b = current_params_h2a$effect_values,
  effect_shape = current_params_h2a$effect_shapes
) |>
  mutate(
    focal_term = case_when(
      model %in% c("fit_mlm_simple","fit_gls_simple", "fit_gam_no_main") ~ "conditionintervention",
      model %in% c("fit_gam", "fit_mlm", "fit_gls", "fit_gls_spline") ~ "conditionintervention:intervention_period",
      TRUE ~ "conditionintervention:intervention_period"
    )
  ) |>
  (function(d) { d$row_id <- pmap_chr(d, ~ paste0(names(list(...)), "=", c(...), collapse = "_")); d })() |>
  mutate(i = row_number())

all_jobs_h2a <- specs_h2a |>
  crossing(sim = 1:n_sims) |>
  mutate(job_id = row_number())

# Helper objects for progress tracking & reporting
total_jobs_h2a <- nrow(all_jobs_h2a)
progress_checkpoints_h2a <- seq(from = round(total_jobs_h2a * 0.1),
                                  to   = total_jobs_h2a,
                                  by   = round(total_jobs_h2a * 0.1))

# Check if cache exists and parameters match
cache_valid_h2a <- FALSE
if (!force_rerun_h2a && file.exists(cache_file_h2a) && file.exists(cache_summary_h2a) && file.exists(cache_params_h2a)) {
  cached_params_h2a <- readRDS(cache_params_h2a)
  cache_valid_h2a <- identical(current_params_h2a, cached_params_h2a)
}

if (cache_valid_h2a) {
  message("Loading cached H2a simulation results...")
  results_h2a <- readRDS(cache_file_h2a)
  sim_summary_h2a <- readRDS(cache_summary_h2a)
  message("Cached H2a results loaded successfully!")
} else {
  if (force_rerun_h2a) {
    message("Force rerun enabled. Running H2a simulations...")
  } else {
    message("No cached H2a results found or parameters changed. Running H2a simulations...")
  }
  
  # Create cache directory if it doesn't exist
  if (!dir.exists("cache")) {
    dir.create("cache", recursive = TRUE)
  }

  # Start timing
  message("Starting H2a simulations at: ", Sys.time())
  tic("H2a simulation total time")

  # n_sims is already defined above based on `current_params_h2a`

# Create all spec-simulation combinations for better parallelization
# all_jobs_h2a <- specs_h2a |> 
#   crossing(sim = 1:n_sims) |> 
#   mutate(job_id = row_number())

message("Running ", nrow(all_jobs_h2a), " simulations across ", nbrOfWorkers(), " cores...")

# Set up progress tracking
# total_jobs_h2a <- nrow(all_jobs_h2a)
# progress_checkpoints_h2a <- seq(from = round(total_jobs_h2a * 0.1), to = total_jobs_h2a, by = round(total_jobs_h2a * 0.1))

# Run all simulations in parallel
results_h2a <- future_map_dfr(1:nrow(all_jobs_h2a), function(job_idx) {
  # Load required libraries in each worker
  library(tidyverse)
  library(lme4)
  library(mgcv)
  library(broom)
  library(nlme)
  library(emmeans)
  library(splines)
  library(broom.mixed)
  library(extraDistr)
  
  # Get job parameters
  job <- all_jobs_h2a[job_idx, ]
  
  # Progress tracking
  if (job_idx %in% progress_checkpoints_h2a) {
    message(sprintf("H2a Progress: %d/%d (%.1f%%) completed at %s", 
                    job_idx, total_jobs_h2a, (job_idx/total_jobs_h2a)*100, Sys.time()))
  }
  
  tryCatch({
    result <- sim_study(
      model = job$model,
      focal_term = job$focal_term,
      n = 80,
      n_days = 28,
      # effect parameters
      b = job$b,
      mu = 15,
      effect_shape = job$effect_shape,
      k = .5,
      # random effects parameters
      tau_int = 2.5,
      tau_slope = .8,
      within_person_sd = 2.5,
      # AR(1) parameters
      phi = 0.7
    ) |> 
      mutate(
        sim = job$sim,
        row_id = job$row_id,
        model = job$model,
        b = job$b,
        effect_shape = job$effect_shape
      )
    
    return(result)
  }, error = function(e) {
    message("Job ", job_idx, " (spec row: ", job$i, ", sim: ", job$sim, ") failed: ", e$message)
    tibble(
      term = NA_character_,
      estimate = NA_real_,
      std.error = NA_real_,
      conf.low = NA_real_,
      conf.high = NA_real_,
      sim = job$sim,
      row_id = job$row_id,
      model = job$model,
      b = job$b,
      effect_shape = job$effect_shape
    )
  })
}, .progress = TRUE,
.options = furrr_options(
  globals = c("all_jobs_h2a", "sim_study", "sim_data", 
              "fit_gam", "fit_gam_no_main",
              "fit_mlm", "fit_mlm_simple",
              "fit_gls", "fit_gls_simple",
              "fit_gls_spline", 
              "extract_marginal_effect",
              "progress_checkpoints_h2a", "total_jobs_h2a"),
  seed = TRUE
))

sim_summary_h2a <- results_h2a |> 
  group_by(row_id) |> 
  summarise(
    model = first(model),
    b = first(b),
    effect_shape = first(effect_shape),
    mean_effect = mean(estimate, na.rm = TRUE),
    mean_se = mean(std.error, na.rm = TRUE),
    mean_conf.low = mean(conf.low, na.rm = TRUE),
    mean_conf.high = mean(conf.high, na.rm = TRUE),
    power = sum(conf.low > 0, na.rm = TRUE) / sum(!is.na(conf.low))
  )

# Stop timing and display results
h2a_time <- toc()
message("H2a simulations completed at: ", Sys.time())

# Save results to cache
saveRDS(results_h2a, cache_file_h2a)
saveRDS(sim_summary_h2a, cache_summary_h2a)
saveRDS(current_params_h2a, cache_params_h2a)
message("H2a simulation results saved to cache!")

} # End of else block for cached results

# Also print some summary statistics about the simulation
message("=== SIMULATION SUMMARY ===")
message("Total number of jobs: ", nrow(all_jobs_h2a))
message("Number of successful results: ", sum(!is.na(results_h2a$estimate)))
message("Number of failed jobs: ", sum(is.na(results_h2a$estimate)))
message("Success rate: ", round(100 * sum(!is.na(results_h2a$estimate)) / nrow(results_h2a), 1), "%")
message("============================")


```

```{r}
#| label: visualize-power-h2a
#| code-summary: "Show code (visualize power h2a)"

# Estimated effect vs. true effect (b)
ggplot(sim_summary_h2a, aes(x = b, y = mean_effect, color = model, alpha = model == "fit_gam")) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean_conf.low, ymax = mean_conf.high), width = 0.1) +
  facet_wrap(~ effect_shape) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  scale_alpha_manual(values = c("TRUE" = 1.0, "FALSE" = 0.2), guide = "none") +
  labs(x = "True Effect (unstandardized b)", y = "Estimated Effect",
       title = "Estimated vs. True Effects by Model and Effect Shape") +
  scale_x_continuous(breaks = c(0.3, 0.6, 0.9, 1.2, 1.5),
                     sec.axis = sec_axis(~ . / 3, name = "Standardized Effect (b/3)"))

# ggsave("figures/estimated_vs_true_effects.png", width = 10, height = 6, dpi = 300)

# Power vs. true effect (b)
ggplot(sim_summary_h2a, aes(x = b, y = power, color = model, alpha = model == "fit_gam")) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  facet_wrap(~ effect_shape) +
  scale_alpha_manual(values = c("TRUE" = 1.0, "FALSE" = 0.2), guide = "none") +
  labs(x = "True Effect (unstandardized b)", y = "Power",
       title = "Power by Model and Effect Shape") +
  scale_x_continuous(breaks = c(0.3, 0.6, 0.9, 1.2, 1.5),
                     sec.axis = sec_axis(~ . / 3, name = "Standardized Effect (b/3)")) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, 1), breaks = seq(0, 1, .1))

# ggsave("figures/power_by_model_and_effect_shape.png", width = 10, height = 6, dpi = 300)

# results |>
#   forestplot(mean = estimate,
#              lower = conf.low,
#              upper = conf.high,
#              labeltext = term)

### Minimum Detectable Effect Sizes at 80% Power
```

```{r}
#| label: h2a-power-table
#| code-summary: "Show code (minimum detectable effects table)"

# Function to interpolate minimum detectable effect at 80% power
find_mde_80 <- function(power_data) {
  # If we already have 80% power or higher at the smallest effect, return that
  if (min(power_data$power, na.rm = TRUE) >= 0.8) {
    return(min(power_data$b, na.rm = TRUE))
  }
  
  # If we never reach 80% power, return NA
  if (max(power_data$power, na.rm = TRUE) < 0.8) {
    return(NA_real_)
  }
  
  # Linear interpolation to find effect size at 80% power
  approx(x = power_data$power, y = power_data$b, xout = 0.8)$y
}

# Calculate minimum detectable effects for each model and effect shape
mde_table <- sim_summary_h2a |>
  group_by(model, effect_shape) |>
  summarise(
    mde_80_unstandardized = find_mde_80(cur_data()),
    mde_80_standardized = mde_80_unstandardized / 3,  # Convert to standardized units (sleep quality range is ~20, so /3 approximates standardization)
    max_power = max(power, na.rm = TRUE),
    .groups = "drop"
  ) |>
  arrange(effect_shape, mde_80_standardized) |>
  mutate(
    # Clean up model names for presentation
    model_clean = case_when(
      model == "fit_gam" ~ "GAM",
      model == "fit_gam_no_main" ~ "GAM (no main effect)",
      model == "fit_mlm" ~ "MLM",
      model == "fit_mlm_simple" ~ "MLM Simple",
      model == "fit_gls" ~ "GLS",
      model == "fit_gls_simple" ~ "GLS Simple", 
      model == "fit_gls_spline" ~ "GLS Splines",
      TRUE ~ model
    ),
    # Format effect sizes for display
    mde_80_unstandardized_fmt = ifelse(is.na(mde_80_unstandardized), 
                                     ">1.5", 
                                     sprintf("%.1f", mde_80_unstandardized)),
    mde_80_standardized_fmt = ifelse(is.na(mde_80_standardized), 
                                   ">0.50", 
                                   sprintf("%.2f", mde_80_standardized)),
    max_power_fmt = sprintf("%.1f%%", max_power * 100)
  )

# Create formatted table
mde_table |>
  select(
    `Effect Shape` = effect_shape,
    `Model` = model_clean,
    `MDE (Unstandardized)` = mde_80_unstandardized_fmt,
    `MDE (Standardized)` = mde_80_standardized_fmt,
    `Max Power Achieved` = max_power_fmt
  ) |>
  kable(
    caption = "Minimum Detectable Effect Sizes at 80% Power by Model and Effect Shape",
    align = c("l", "l", "r", "r", "r")
  ) |>
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE,
    position = "left"
  ) |>
  pack_rows("Growing Effect", 1, sum(mde_table$effect_shape == "grow")) |>
  pack_rows("Plateau Effect", sum(mde_table$effect_shape == "grow") + 1, nrow(mde_table)) |>
  footnote(
    general = c(
      "MDE = Minimum Detectable Effect size at 80% power",
      "Standardized effects calculated as unstandardized effect / 3",
      "Models ordered by sensitivity (smallest MDE first) within each effect shape",
      "'>1.5' and '>0.50' indicate models that did not achieve 80% power at largest tested effect size"
    ),
    general_title = "Notes:"
  )


```

## Simulated h2b power analysis

Here we look at power for our robust per-protocol model that avoids change scores by directly modeling the relationship between current playtime and outcomes while controlling for baseline differences.

```{r}
#| label: sim-study-h2b
#| cache: true
#' Simulation Study Orchestrator
#'
#' A higher-level function that ties together data simulation, dropout, and model fitting,
#' returning a tidy summary of the fitted model parameters.
#'
#' @param model_function A function to fit the model. Defaults to \code{fit_gam}.
#' @param n Number of participants passed to \code{sim_data()}. Uses sim_data default (80).
#' @param n_days Number of time points per parameter passed to \code{sim_data()}. Default is 28.
#' @param b Fixed effect for the intervention slope passed to \code{sim_data()}. Default is 0.01.
#' @param phi AR(1) autocorrelation coefficient passed to \code{sim_data()}. Default is 0.7.
#' @param sigma AR(1) residual standard deviation passed to \code{sim_data()}. Default is 0.6.
#'
#' @return A data frame (tibble) of model estimates from \code{broom.mixed::tidy()} for lme models.
#'

# Updated simulation orchestrator that handles GLS models separately.
sim_study <- function(model = "fit_gam", focal_term = "intervention_activeTRUE:reduction", ...) {
  args <- list(...)
  dat <- do.call(sim_data, args)
  model_function <- get(model)
  
  mod <- model_function(dat)
  
  if (model %in% c("fit_gam_no_main","fit_gls_spline")) {
    # Extract the effect using our helper function.
    result <- suppressMessages(extract_marginal_effect(mod, 
                                                       dat, 
                                                       focal_term = focal_term))
  } else {
    # For models that work with broom, extract the focal parameter.
    # Use broom.mixed for lme models, regular broom for others
    if (model == "fit_mlm_reduction_robust") {
      result <- broom.mixed::tidy(mod) |>
        filter(term == focal_term) |> 
        mutate(
          conf.low = estimate - 1.96 * std.error,
          conf.high = estimate + 1.96 * std.error
        )
    } else {
      result <- broom::tidy(mod, parametric = TRUE) |>
        filter(term == focal_term) |> 
        # filter(
        #   term == "conditionintervention:intervention_period" | 
        #     (model %in% c("fit_mlm_simple","fit_gls_simple") & term == "conditionintervention")
        # ) |> 
        mutate(
          conf.low = estimate - 1.96 * std.error,
          conf.high = estimate + 1.96 * std.error
        )
    }
  }
  result
}

# Load tictoc for timing if not already loaded
if (!require(tictoc, quietly = TRUE)) {
  install.packages("tictoc")
  library(tictoc)
}

# Check if cached results exist
cache_file_h2b <- "cache/h2b_simulation_results.rds"
cache_summary_h2b <- "cache/h2b_simulation_summary.rds"
cache_params_h2b <- "cache/h2b_simulation_params.rds"

# Simulation control parameters (using global settings)
force_rerun_h2b <- FORCE_RERUN_H2B


# Define key parameters that would invalidate cache
current_params_h2b <- list(
  n_sims = 500,
  n = 80,
  n_days = 28,
  tau_int = 2.5,
  tau_slope = 0.8,
  within_person_sd = 2.0,
  phi = 0.7,
  models = c("fit_mlm_reduction_robust"),
  effect_values = c(0.5, 1.0, 1.5, 2.0, 2.5),
  effect_shapes = c("grow", "plateau"),
  mediated = TRUE
)

# -------------------------------------------------------------------
# Prepare job specifications irrespective of cache status
# This guarantees objects like `specs_h2b`, `all_jobs_h2b`, etc. exist
# even when cached results are loaded, preventing missing-object errors.

  n_sims <- current_params_h2b$n_sims

  specs_h2b <- expand_grid(
    model = current_params_h2b$models,
    b = current_params_h2b$effect_values,
    effect_shape = current_params_h2b$effect_shapes
  ) |> 
    mutate(
      # focal term for robust model
      focal_term = "intervention_activeTRUE:playtime",
      # expected mediated effect size (approximate)
      expected_effect = b * .1*beta(1 + 1/.05, .1)
    ) |> 
    (function(d) { d$row_id <- pmap_chr(d, ~ paste0(names(list(...)), "=", c(...), collapse = "_")); d })() |> 
    mutate(i = row_number())

  all_jobs_h2b <- specs_h2b |>
    crossing(sim = 1:n_sims) |>
    mutate(job_id = row_number())

  # Helper objects for progress
  total_jobs_h2b <- nrow(all_jobs_h2b)
  progress_checkpoints_h2b <- seq(from = round(total_jobs_h2b * 0.1),
                                  to   = total_jobs_h2b,
                                  by   = round(total_jobs_h2b * 0.1))

# Check if cache exists and parameters match
cache_valid_h2b <- TRUE
if (!force_rerun_h2b && file.exists(cache_file_h2b) && file.exists(cache_summary_h2b) && file.exists(cache_params_h2b)) {
  cached_params_h2b <- readRDS(cache_params_h2b)
  cache_valid_h2b <- identical(current_params_h2b, cached_params_h2b)
}

if (cache_valid_h2b) {
  message("Loading cached H2b simulation results...")
  results_h2b <- readRDS(cache_file_h2b)
  sim_summary_h2b <- readRDS(cache_summary_h2b)
  message("Cached H2b results loaded successfully!")
} else {
  if (force_rerun_h2b) {
    message("Force rerun enabled. Running H2b simulations...")
  } else {
    message("No cached H2b results found or parameters changed. Running H2b simulations...")
  }
  
  # Create cache directory if it doesn't exist
  if (!dir.exists("cache")) {
    dir.create("cache", recursive = TRUE)
  }

  # Start timing
  message("Starting H2b simulations at: ", Sys.time())
  tic("H2b simulation total time")

  # n_sims is already defined above based on `current_params_h2b`

# Create all spec-simulation combinations for better parallelization
# all_jobs_h2b <- specs_h2b |> 
#   crossing(sim = 1:n_sims) |> 
#   mutate(job_id = row_number())

message("Running ", nrow(all_jobs_h2b), " simulations across ", nbrOfWorkers(), " cores...")

# Set up progress tracking
# total_jobs_h2b <- nrow(all_jobs_h2b)
# progress_checkpoints_h2b <- seq(from = round(total_jobs_h2b * 0.1), to = total_jobs_h2b, by = round(total_jobs_h2b * 0.1))

# Run all simulations in parallel
results_h2b <- future_map_dfr(1:nrow(all_jobs_h2b), function(job_idx) {
  # Load required libraries in each worker
  library(tidyverse)
  library(nlme)
  library(broom.mixed)
  library(extraDistr)
  library(rms)
  library(broom)
  
  # Define sim_data function in each worker to ensure availability
  sim_data <- function(n = 80, n_days = 28, b = 0.8, mu = 15, tau_int = 2.5, tau_slope = .05, within_person_sd = 2.0, phi = 0.8, effect_shape = "grow", k = .5, mediated = FALSE, playtime_grand_mean = 1, playtime_grand_sd = .5, daily_play_sd = 0.5) {
    dat <- tibble(
      id = 1:n,
      age = sample(18:36, n, replace = TRUE),
      gender = sample(c("man","woman","non-binary"), n, prob = c(.45, .45, .1), replace = TRUE),
      condition = factor(sample(c("control", "intervention"), n, replace = TRUE)),
      experimental_condition = ifelse(condition == "intervention", 1, 0),
      intercept_sq = rnorm(n, 0, tau_int),
      slope_sq = rnorm(n, 0, tau_slope),
      intercept_play = rlnorm(n, log(playtime_grand_mean), playtime_grand_sd),
    ) |> 
      crossing(day = 1:n_days) |> 
      mutate(
        intervention_period = as.numeric(day > 7 & day < 22),
        intervention_active = intervention_period & condition == "intervention",
        compliance = ifelse(intervention_active, rkumar(n*n_days, a = .05, b = .1), 0),
        playtime = (1 - compliance) * rlnorm(n, log(intercept_play), daily_play_sd),
        effect_time = case_when(
          effect_shape == "plateau" ~ if_else(intervention_period == 1, (b + slope_sq) * (1-exp(-k * (day - 7))), 0),
          effect_shape == "grow" ~ if_else(intervention_period == 1, (day - 7) * ((b + slope_sq)/7), 0),
          TRUE ~ NA_real_
        ),
      ) |> 
      group_by(id) |> 
      mutate(
        baseline_playtime = mean(playtime[day <= 7]),
        reduction = baseline_playtime - playtime,
        sigma = within_person_sd * sqrt(1-phi^2),
        e = as.numeric(arima.sim(n = n_days, model = list(ar = phi), sd = sigma)),
        sleep_quality_raw = case_when(
          mediated == TRUE ~ mu + intercept_sq + effect_time * reduction + .01*(age-18) + -.05*(gender %in% c("women","non-binary")) + e,
          mediated == FALSE ~ mu + intercept_sq + effect_time * experimental_condition * intervention_period + .01*(age-18) + -.05*(gender %in% c("women","non-binary")) + e
        ),
        sleep_quality = round(pmax(5, pmin(25, sleep_quality_raw)))
      ) |> 
      ungroup() |> 
      mutate(across(where(is.numeric), ~ round(., 3)))
    return(dat)
  }
  
  # Define other required functions
  fit_mlm_reduction_robust <- function(dat) {
    lme(
      fixed = sleep_quality ~ intervention_active*playtime + baseline_playtime + age + gender,
      random = ~ 1 | id,
      correlation = corCAR1(form = ~ day | id),
      data = dat
    )
  }
  
  # Define sim_study function
  sim_study <- function(model = "fit_gam", focal_term = "intervention_activeTRUE:reduction", ...) {
    args <- list(...)
    dat <- do.call(sim_data, args)
    model_function <- get(model)
    
    mod <- model_function(dat)
    
    if (model %in% c("fit_gam_no_main","fit_gls_spline")) {
      result <- suppressMessages(extract_marginal_effect(mod, dat, focal_term = focal_term))
    } else {
      if (model == "fit_mlm_reduction_robust") {
        result <- broom.mixed::tidy(mod) |>
          filter(term == focal_term) |> 
          mutate(
            conf.low = estimate - 1.96 * std.error,
            conf.high = estimate + 1.96 * std.error
          )
      } else {
        result <- broom::tidy(mod, parametric = TRUE) |>
          filter(term == focal_term) |> 
          mutate(
            conf.low = estimate - 1.96 * std.error,
            conf.high = estimate + 1.96 * std.error
          )
      }
    }
    result
  }
  
  # Get job parameters
  job <- all_jobs_h2b[job_idx, ]
  
  # Progress tracking
  if (job_idx %in% progress_checkpoints_h2b) {
    message(sprintf("H2b Progress: %d/%d (%.1f%%) completed at %s", 
                    job_idx, total_jobs_h2b, (job_idx/total_jobs_h2b)*100, Sys.time()))
  }
  
  tryCatch({
    result <- sim_study(
      model = job$model,
      focal_term = job$focal_term,
      n = 80,
      n_days = 28,
      # effect parameters
      b = job$b,
      mu = 15,
      effect_shape = job$effect_shape,
      k = .5,
      # random effects parameters
      tau_int = 2.5,
      tau_slope = .8,
      within_person_sd = 2.0,
      # AR(1) parameters
      phi = 0.7,
      mediated = TRUE
    ) |> 
      mutate(
        sim = job$sim,
        row_id = job$row_id,
        model = job$model,
        b = job$b,
        expected_effect = job$expected_effect,
        effect_shape = job$effect_shape
      )
    
    return(result)
  }, error = function(e) {
    message("Job ", job_idx, " (spec row: ", job$i, ", sim: ", job$sim, ") failed: ", e$message)
    tibble(
      term = NA_character_,
      estimate = NA_real_,
      std.error = NA_real_,
      conf.low = NA_real_,
      conf.high = NA_real_,
      sim = job$sim,
      row_id = job$row_id,
      model = job$model,
      b = job$b,
      expected_effect = job$expected_effect,
      effect_shape = job$effect_shape
    )
  })
}, .progress = TRUE,
.options = furrr_options(
  globals = c("all_jobs_h2b", "sim_study", "sim_data", "fit_mlm_reduction_robust",
              "progress_checkpoints_h2b", "total_jobs_h2b", "sim_dropout",
              "fit_gam", "fit_gam_no_main", "fit_mlm", "fit_mlm_simple", 
              "fit_gls", "fit_gls_simple", "fit_gls_spline", "extract_marginal_effect"),
  seed = TRUE
))

sim_summary_h2b <- results_h2b |> 
  group_by(row_id) |> 
  summarise(
    model = first(model),
    b = first(b),
    expected_effect = first(expected_effect),
    effect_shape = first(effect_shape),
    mean_effect = mean(estimate, na.rm = TRUE),
    mean_se = mean(std.error, na.rm = TRUE),
    mean_conf.low = mean(conf.low, na.rm = TRUE),
    mean_conf.high = mean(conf.high, na.rm = TRUE),
    # Power calculation for robust approach (expects negative coefficient)
    power = sum(conf.high < 0, na.rm = TRUE) / sum(!is.na(conf.high))
  )

# Stop timing and display results
h2b_time <- toc()
message("H2b simulations completed at: ", Sys.time())

# Save results to cache
saveRDS(results_h2b, cache_file_h2b)
saveRDS(sim_summary_h2b, cache_summary_h2b)
saveRDS(current_params_h2b, cache_params_h2b)
message("H2b simulation results saved to cache!")

} # End of else block for cached results

# Also print some summary statistics about the simulation
message("=== H2B SIMULATION SUMMARY ===")
message("Total number of jobs: ", nrow(all_jobs_h2b))
message("Number of successful results: ", sum(!is.na(results_h2b$estimate)))
message("Number of failed jobs: ", sum(is.na(results_h2b$estimate)))
message("Success rate: ", round(100 * sum(!is.na(results_h2b$estimate)) / nrow(results_h2b), 1), "%")
message("===============================")

```

```{r}
#| label: recalculate-h2b-summary
#| code-summary: "Recalculate H2b summary with power calculation"

# Recalculate summary with power calculation
if (exists("results_h2b")) {
  sim_summary_h2b <- results_h2b |> 
    group_by(row_id) |> 
    summarise(
      model = first(model),
      b = first(b),
      expected_effect = first(expected_effect),
      effect_shape = first(effect_shape),
      mean_effect = mean(estimate, na.rm = TRUE),
      mean_se = mean(std.error, na.rm = TRUE),
      mean_conf.low = mean(conf.low, na.rm = TRUE),
      mean_conf.high = mean(conf.high, na.rm = TRUE),
      # Power calculation for robust approach (expects negative coefficient)
      power = sum(conf.high < 0, na.rm = TRUE) / sum(!is.na(conf.high))
    )
  
  message("Recalculated H2b summary with power calculation")
  message("Robust Approach - Mean Power: ", 
          round(mean(sim_summary_h2b$power[sim_summary_h2b$model == "fit_mlm_reduction_robust"], na.rm = TRUE), 3))
}
```

```{r}
#| label: visualize-power-h2b
#| code-summary: "Show code (visualize power h2b)"

# Estimated effect vs. true effect (b) - Robust Approach Only
ggplot(sim_summary_h2b, aes(x = expected_effect, y = -mean_effect)) +
  geom_point(size = 3, color = "#FF6347") +
  geom_errorbar(aes(ymin = -mean_conf.high, ymax = -mean_conf.low), 
                width = 0.01, color = "#FF6347") +
  facet_wrap(~ effect_shape) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  labs(x = "True Effect Magnitude (expected effect)", 
       y = "Estimated Effect Magnitude",
       title = "Estimated vs. True Effects for Robust Per-Protocol Model",
       subtitle = "Effects shown as absolute magnitudes") +
  scale_x_continuous(breaks = c(0.05, 0.1, 0.15, 0.2, 0.25),
                     sec.axis = sec_axis(~ . / 3, name = "Standardized Effect"))

# ggsave("figures/estimated_vs_true_effects_h2b.png", width = 10, height = 6, dpi = 300)

# Power vs. true effect (b) - Robust Approach Only
ggplot(sim_summary_h2b, aes(x = expected_effect, y = power)) +
  geom_line(size = 1.2, color = "#FF6347") +
  geom_point(size = 3, color = "#FF6347") +
  facet_wrap(~ effect_shape) +
  labs(x = "True Effect (expected effect)", y = "Power",
       title = "Power for Robust Per-Protocol Model by Effect Shape") +
  scale_x_continuous(breaks = c(0.05, 0.1, 0.15, 0.2, 0.25),
                     sec.axis = sec_axis(~ . / 3, name = "Standardized Effect")) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, 1), breaks = seq(0, 1, .1))

# ggsave("figures/power_by_model_and_effect_shape_h2b.png", width = 10, height = 6, dpi = 300)  

# Summary statistics for the robust per-protocol approach
protocol_summary <- sim_summary_h2b |>
  group_by(effect_shape) |>
  summarise(
    mean_power = mean(power, na.rm = TRUE),
    max_power = max(power, na.rm = TRUE),
    min_power = min(power, na.rm = TRUE),
    mean_bias = mean(abs(mean_effect - expected_effect), na.rm = TRUE),
    .groups = "drop"
  ) |>
  mutate(
    model_name = "Robust Approach"
  )

# Display summary table
knitr::kable(
  protocol_summary,
  caption = "Performance Summary for Robust Per-Protocol Approach",
  col.names = c("Effect Shape", "Mean Power", "Max Power", "Min Power", "Mean Bias", "Model Name"),
  digits = 3
)

```

### Minimum Detectable Effect Sizes for H2b

```{r}
#| label: h2b-power-table
#| code-summary: "Show code (minimum detectable effects table for H2b)"

# Function to interpolate minimum detectable effect at 80% power (adapted for H2b)
find_mde_80_h2b <- function(power_data) {
  # If we already have 80% power or higher at the smallest effect, return that
  if (min(power_data$power, na.rm = TRUE) >= 0.8) {
    return(min(power_data$expected_effect, na.rm = TRUE))
  }
  
  # If we never reach 80% power, return NA
  if (max(power_data$power, na.rm = TRUE) < 0.8) {
    return(NA_real_)
  }
  
  # Linear interpolation to find effect size at 80% power
  approx(x = power_data$power, y = power_data$expected_effect, xout = 0.8)$y
}

# Calculate minimum detectable effects for H2b (robust per-protocol approach)
mde_table_h2b <- sim_summary_h2b |>
  group_by(effect_shape) |>
  summarise(
    mde_80_unstandardized = find_mde_80_h2b(cur_data()),
    mde_80_standardized = mde_80_unstandardized / 3,  # Convert to standardized units
    max_power = max(power, na.rm = TRUE),
    min_power = min(power, na.rm = TRUE),
    mean_power = mean(power, na.rm = TRUE),
    .groups = "drop"
  ) |>
  arrange(mde_80_standardized) |>
  mutate(
    # Format effect sizes for display
    mde_80_unstandardized_fmt = ifelse(is.na(mde_80_unstandardized), 
                                     ">0.25", 
                                     sprintf("%.3f", mde_80_unstandardized)),
    mde_80_standardized_fmt = ifelse(is.na(mde_80_standardized), 
                                   ">0.083", 
                                   sprintf("%.3f", mde_80_standardized)),
    max_power_fmt = sprintf("%.1f%%", max_power * 100),
    min_power_fmt = sprintf("%.1f%%", min_power * 100),
    mean_power_fmt = sprintf("%.1f%%", mean_power * 100)
  )

# Create formatted table for H2b
mde_table_h2b |>
  select(
    `Effect Shape` = effect_shape,
    `MDE (Unstandardized)` = mde_80_unstandardized_fmt,
    `MDE (Standardized)` = mde_80_standardized_fmt,
    `Mean Power` = mean_power_fmt,
    `Min Power` = min_power_fmt,
    `Max Power Achieved` = max_power_fmt
  ) |>
  kable(
    caption = "Minimum Detectable Effect Sizes at 80% Power - H2b Robust Per-Protocol Model",
    align = c("l", "r", "r", "r", "r", "r")
  ) |>
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE,
    position = "left"
  ) |>
  footnote(
    general = c(
      "MDE = Minimum Detectable Effect size at 80% power for the robust per-protocol approach",
      "Standardized effects calculated as unstandardized effect / 3",
      "This model tests the interaction between intervention status and actual playtime levels",
      "Effects are for the intervention_activeTRUE:playtime interaction term",
      "Power calculation expects negative coefficients (reduction in sleep quality with higher playtime during intervention)"
    ),
    general_title = "Notes:"
  )

```


